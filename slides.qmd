---
title: "torch-workshop"
subtitle: "RMACC 2023 - Arizona State University"
author: "Nathan Stephens"
format: revealjs
smaller: true
logo: nvidia.png
scrollable: true
slide-number: true
eval: false
echo: true
---

# Welcome

## About Me

* Currently Senior Manager for Developer Relations at NVIDIA. 
* Background in analytic solutions and consulting.
* Experience building data science teams, architecting analytic infrastructure, and operationalizing data products. 
* Worked at RStudio, oversaw the solutions engineering team.
* Longtime advocate for operationalizing data science using open-source software.
* Relocated to Peoria, AZ in 2020.


## Deep Learning and Scientific Computing With R Torch

This workshop presents a conceptual overview of the [R interface for PyTorch](https://torch.mlverse.org/). Emphasis will be on basic structures and applications for deep learning. Materials are based on the new book, [Deep Learning and Scientific Computing with R torch](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/) by [Sigrid Keydana](https://divergences.xyz/) (2023).

<center>![](Keydana.jpg)</center>

## Three Goals

This is a book about `torch`, the R interface to PyTorch. PyTorch, as of this writing, is one of the major deep-learning and scientific-computing frameworks, widely used across industries and areas of research. With `torch`, you get to access its rich functionality directly from R, with no need to install, let alone learn, Python. Though still "young" as a project, `torch` already has a vibrant community of users and developers; the latter not just extending the core framework, but also, building on it in their own packages.

1. The first is a thorough introduction to core `torch`: the basic structures without whom nothing would work.
2. In the second section, basics explained, we proceed to explore various applications of deep learning, ranging from image recognition over time series and tabular data to audio classification. 
3. The third section is special in that it highlights some of the non-deep-learning things you can do with `torch`: matrix computations, calculating the Discrete Fourier Transform, and wavelet analysis.


## Workshop Contents

**Covering first two sections:**

* Getting familiar with torch
  * Torch mechanics: `3`, 4, 9
  * What can we do? 5, `6`, 10
  * Abstraction: 7, 8, `11`
* Deep learning with torch
  * Basic image classification: 13, `14`, `15`
  * Opitmized image classification: 16, 17, 18

**Reproducing Chapters:**

* Tensors (chapter 3)
* A neural network from scratch (chapter 6)
* Modularizing the neural network (chapter 11)
* Training with luz (chapter 14)
* A first go at image classification - using ImageNet (242MB) (chapter 15)

**Remaining chapters are left to the reader**

# Setup

## CPU Setup (Windows and Mac)

1. Install RStudio desktop
2. Install `torch`

```{r}
install.packages("torch")
```

3. Clone the [torch-workshop](https://github.com/nwstephens/torch-workshop.git) repos and make sure the working directory is `torch-workshop`
4. Download `Tiny ImageNet` data

```{r}
tiny_imagenet_dataset(".", download = TRUE)
```

5. Install the `modeldata` package from the `2021-06-08` snapshot

```{r}
install.packages("modeldata", repos = "https://packagemanager.rstudio.com/cran/__linux__/bionic/2021-06-08")
```

## GPU Accelerated R Setup

* Obtaining an NVIDIA GPU
  * [Rig](https://boxx.com/)
  * [Cloud](https://rapids.ai/#quick-start)
  * [Data Center](https://www.nvidia.com/en-us/data-center/)
* Installing and configuring an NVIDIA GPU
  * [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads)
  * [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
* Installing and configuring R
  * [The Rocker Project](https://rocker-project.org/)
  * [Posit Package Manager](https://packagemanager.rstudio.com/client/#/)
  * [R Binaries](https://github.com/rstudio/r-builds)

## GPU Setup (Linux)

1. Have a CUDA compatible NVIDIA GPU with [compute capability](https://developer.nvidia.com/cuda-gpus#compute) 6.0 or higher
2. Install the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
3. Pull and run the RStudio Rocker container

```{bash}
docker pull rocker/rstudio
docker run --gpus=all -d -t -e PASSWORD=rstudio -p 8787:8787 --name rstudio rocker/rstudio
```

4. Open RStudio Server from your browser by opening `http://<your.ip.address>:8787/`. Make sure port 8787 is open. Your username is `rstudio` and your password is `rstudio`.
5. Configure RStudio Server to download package binaries from the [Posit Package Manager](https://packagemanager.rstudio.com/client/#/)

```
# For Red Hat 8 use:
https://packagemanager.rstudio.com/cran/__linux__/centos8/latest

# For Ubuntu 20.04 use:
https://packagemanager.rstudio.com/cran/__linux__/focal/latest
```
6. Install `torch` from the pre-built binaries (Warning! This download is 2Gb)

```{r}
options(timeout = 600)
install.packages("torch", repos = "https://storage.googleapis.com/torch-lantern-builds/packages/cu117/0.10.0/")
```

7. Make sure your CUDA device is available (this should return `TRUE`)

```{r}
library(torch)
cuda_is_available()
```

8. Clone this repos and make sure the working directory is `torch-workshop`
9. Download `Tiny ImageNet` data

```{bash}
wget http://cs231n.stanford.edu/tiny-imagenet-200.zip
unzip tiny-imagenet-200.zip
```

`10.` Install the `modeldata` package from the `2021-06-08` snapshot

```{r}
install.packages("modeldata", repos = "https://packagemanager.rstudio.com/cran/__linux__/bionic/2021-06-08")
```

# Getting Familiar with torch

## 2. What is torch?

`torch` is an R port of PyTorch, one of the two (as of this writing) most-employed deep learning frameworks in industry and research. By its design, it is also an excellent tool to use in various types of scientific computation tasks (a subset of which you'll encounter in the book's final part). It is written entirely in R and C++ (including a bit of C). No Python installation is required to use it.

There's a vibrant community of developers, of diverse origin and with diverse goals, working to further develop and extend `torch`, so it can help more and more people accomplish their various tasks. 

There are three packages used in the book: `torchvision` , `torchaudio`, and `luz`.

## 3. Tensors [Exercise]

**What's in a tensor?**

To do anything useful with `torch`, you need to know about tensors. Not tensors in the math/physics sense. In deep learning frameworks such as TensorFlow and (Py-)Torch, *tensors* are "just" multi-dimensional arrays optimized for fast computation -- not on the CPU only but also, on specialized devices such as GPUs and TPUs.

In fact, a `torch` `tensor` is like an R `array`, in that it can be of arbitrary dimensionality. But unlike `array`, it is designed for fast and scalable execution of mathematical calculations, and you can move it to the GPU. (It also has an extra capability of enormous practical impact -- automatic differentiation -- but we reserve that for the next chapter.)

**Exercises:**

1. Creating tensors
2. Operations on tensors\index{tensors!operations on}
3. Accessing parts of a tensor\index{tensors!index into} \index{tensors!slice}
4. Reshaping tensors\index{tensors!reshape}
5. Broadcasting\index{tensors!broadcasting}




## 4. Autograd

Frameworks like `torch` are so popular because of what you can do with them: deep learning, machine learning, optimization, large-scale scientific computation in general. Most of these application areas involve minimizing some *loss function*. This, in turn, entails computing function *derivatives*.

`torch` implements what is called *automatic differentiation*\index{automatic differentiation}. In automatic differentiation, and more specifically, its often-used *reverse-mode* variant, derivatives are computed and combined on a *backward pass* through the graph of tensor operations.

In torch, the AD engine is usually referred to as autograd.

**Automatic differentiation example**

Quadratic function of two variables: $f(x_1, x_2) = 0.2 {x_1}^2 + 0.2 {x_2}^2 - 5$. It has its minimum at `(0,0)`.

![](images/autograd-paraboloid.png)

![](images/autograd-compgraph.png)

-   At `x7`, we calculate partial derivatives with respect to `x5` and `x6`. Basically, the equation to differentiate looks like this: $f(x_5, x_6) = x_5 + x_6 - 5$. Thus, both partial derivatives are 1.

-   From `x5`, we move to the left to see how it depends on `x3`. We find that $\frac{\partial x_5}{\partial x_3} = 0.2$. At this point, applying the chain rule of calculus, we already know how the output depends on `x3`: $\frac{\partial f}{\partial x_3} = 0.2 * 1 = 0.2$.

-   From `x3`, we take the final step to `x`. We learn that $\frac{\partial x_3}{\partial x_1} = 2 x_1$. Now, we again apply the chain rule, and are able to formulate how the function depends on its first input: $\frac{\partial f}{\partial x_1} = 2 x_1 * 0.2 * 1 = 0.4 x_1$.

-   Analogously, we determine the second partial derivative, and thus, already have the gradient available: $\nabla f = \frac{\partial f}{\partial x_1} + \frac{\partial f}{\partial x_2} = 0.4 x_1 + 0.4 x_2$.

**Automatic differentiation with `torch` autograd**

```{r}
library(torch)

x1 <- torch_tensor(2, requires_grad = TRUE)
x2 <- torch_tensor(2, requires_grad = TRUE)

x3 <- x1$square()
x5 <- x3 * 0.2

x4 <- x2$square()
x6 <- x4 * 0.2

x7 <- x5 + x6 - 5
x7
```
    torch_tensor
    -3.4000
    [ CPUFloatType{1} ][ grad_fn = <SubBackward1> ]

```{r}
x7$backward()

x1$grad
x2$grad
```

     0.8000
    [ CPUFloatType{1} ]
    torch_tensor
     0.8000
    [ CPUFloatType{1} ]

These are the partial derivatives of `x7` with respect to `x1` and `x2`, respectively. Conforming to our manual calculations above, both amount to 0.8, that is, 0.4 times the tensor values 2 and 2.



## 5. Function minimization with *autograd* {#sec:optim-1}

In optimization\index{optimization} research, the *Rosenbrock function* is a classic. It is a function of two variables; its minimum is at `(1,1)`. If you take a look at its contours, you see that the minimum lies inside a stretched-out, narrow valley.

![](images/optim-rosenbrock.png)

```{r}
a <- 1
b <- 5

rosenbrock <- function(x) {
  x1 <- x[1]
  x2 <- x[2]
  (a - x1)^2 + b * (x2 - x1^2)^2
}
```

* `lr`, for learning rate, is the fraction of the gradient to subtract on every step
* `num_iterations` is the number of steps to take.

```{r}
library(torch)

num_iterations <- 1000

lr <- 0.01

x <- torch_tensor(c(-1, 1), requires_grad = TRUE)

for (i in 1:num_iterations) {
  if (i %% 100 == 0) cat("Iteration: ", i, "\n")

  value <- rosenbrock(x)
  if (i %% 100 == 0) {
    cat("Value is: ", as.numeric(value), "\n")
  }

  value$backward()
  if (i %% 100 == 0) {
    cat("Gradient is: ", as.matrix(x$grad), "\n")
  }

  with_no_grad({
    x$sub_(lr * x$grad)
    x$grad$zero_()
  })
}
```

    Iteration:  100 
    Value is:  0.3502924 
    Gradient is:  -0.667685 -0.5771312 

    Iteration:  200 
    Value is:  0.07398106 
    Gradient is:  -0.1603189 -0.2532476 

    Iteration:  300 
    Value is:  0.02483024 
    Gradient is:  -0.07679074 -0.1373911 

    Iteration:  400 
    Value is:  0.009619333 
    Gradient is:  -0.04347242 -0.08254051 

    Iteration:  500 
    Value is:  0.003990697 
    Gradient is:  -0.02652063 -0.05206227 

    Iteration:  600 
    Value is:  0.001719962 
    Gradient is:  -0.01683905 -0.03373682 

    Iteration:  700 
    Value is:  0.0007584976 
    Gradient is:  -0.01095017 -0.02221584 

    Iteration:  800 
    Value is:  0.0003393509 
    Gradient is:  -0.007221781 -0.01477957

    Iteration:  900 
    Value is:  0.0001532408 
    Gradient is:  -0.004811743 -0.009894371 

    Iteration:  1000 
    Value is:  6.962555e-05 
    Gradient is:  -0.003222887 -0.006653666 

After thousand iterations, we have reached a function value lower than 0.0001. What is the corresponding `(x1,x2)`-position?

```{r}
x
```

    torch_tensor
     0.9918
     0.9830
    [ CPUFloatType{2} ]

This is rather close to the true minimum of `(1,1)`.

![](images/optim-rosenbrock.png)


## 6. A neural network from scratch [Exercise]

Understanding the basics will be an efficient antidote against the surprisingly common temptation to think of deep learning as some kind of "magic". It's all just matrix computations; one has to learn how to orchestrate them though.

**Multiple Linear Regression**

$$
f(\mathbf{X}) = \mathbf{X}\mathbf{W} + \mathbf{b}
$$

```{r}
library(torch)

x <- torch_randn(100, 3)

w <- torch_randn(3, 1, requires_grad = TRUE)

b <- torch_zeros(1, 1, requires_grad = TRUE)

y <- x$matmul(w) + b

print(y, n = 10)
```

    torch_tensor
    -2.1600
    -3.3244
     0.6046
     0.4472
    -0.4971
    -0.0530
     5.1259
    -1.1595
    -0.5960
    -1.4584
    ... [the output was truncated (use n=-1 to disable)]
    [ CPUFloatType{100,1} ][ grad_fn = <AddBackward0> ]

**Layers**

$$
g(f(\mathbf{X}))
$$
$$
g \circ f
$$

One of the defining features of neural networks is their ability to chain an unlimited (in theory) number of layers. 

All but the output layer may be referred to as "hidden" layers, although from the point of view of someone who uses a deep learning framework such as `torch`, they are not that *hidden* after all.

```{r}
# Hidden Layer
w1 <- torch_randn(3, 8, requires_grad = TRUE)
b1 <- torch_zeros(1, 8, requires_grad = TRUE)

# Output Layer
w2 <- torch_randn(8, 1, requires_grad = TRUE)
b2 <- torch_randn(1, 1, requires_grad = TRUE)
```

**Activation functions**

$$
\begin{align}
f(\mathbf{X}) & = ((\mathbf{X} \mathbf{W}_1)\mathbf{W}_2)\mathbf{W}_3 \\
& = \mathbf{X} (\mathbf{W}_1\mathbf{W}_2\mathbf{W}_3) \\
& = \mathbf{X} \mathbf{W}_4
\end{align}
$$

We have lost all advantages associated with deep neural networks. This is where activation functions, sometimes called "nonlinearities", come in. They introduce non-linear operations that cannot be modeled by matrix multiplication. Historically, the prototypical activation function has been the *sigmoid*\index{activation!sigmoid}, and it's still extremely important today. Its constitutive action is to squish its input between zero and one, yielding a value that can be interpreted as a probability. But in regression, this is not usually what we want, and neither would it be for most hidden layers.

Instead, the most-used activation function inside a network is the so-called *ReLU*\index{activation!ReLU}, or Rectified Linear Unit. This is a long name for something rather straightforward: All negative values are set to zero. In `torch`, this can be accomplished using the `relu()` function:

```{r}
t <- torch_tensor(c(-2, 1, 5, -7))
t$relu()
```

    torch_tensor
     0
     1
     5
     0
    [ CPUFloatType{4} ]

**Loss functions**

Loss is a measure of how far away we are from our goal. For regression-type tasks, this often will be mean squared error (MSE).

```{r}
y <- torch_randn(5)
y_pred <- y + 0.01

loss <- (y_pred - y)$pow(2)$mean()

loss
```

    torch_tensor
    9.99999e-05
    [ CPUFloatType{} ]


**Example**

-   do a forward pass\index{forward pass}, yielding the network's predictions (if you dislike the one-liner, feel free to split it up);

-   compute the loss (this, too, being a one-liner -- we merely added some logging);

-   have *autograd* calculate the gradient of the loss with respect to the parameters; and

-   update the parameters accordingly (again, taking care to wrap the whole action in `with_no_grad()`, and zeroing the `grad` fields on every iteration).


**Generate random data**

```{r}
library(torch)

## input dimensionality (number of input features)
d_in <- 3
## number of observations in training set
n <- 100

x <- torch_randn(n, d_in)
coefs <- c(0.2, -1.3, -0.5)
y <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)

```

**Build the network**

```{r}
# dimensionality of hidden layer
d_hidden <- 32
# output dimensionality (number of predicted features)
d_out <- 1

# weights connecting input to hidden layer
w1 <- torch_randn(d_in, d_hidden, requires_grad = TRUE)
# weights connecting hidden to output layer
w2 <- torch_randn(d_hidden, d_out, requires_grad = TRUE)

# hidden layer bias
b1 <- torch_zeros(1, d_hidden, requires_grad = TRUE)
# output layer bias
b2 <- torch_zeros(1, d_out, requires_grad = TRUE)
```

**Train the network**

```{r}

learning_rate <- 1e-4

### training loop ----------------------------------------

for (t in 1:200) {
  
  ### -------- Forward pass --------
  
  y_pred <- x$mm(w1)$add(b1)$relu()$mm(w2)$add(b2)
  
  ### -------- Compute loss -------- 
  loss <- (y_pred - y)$pow(2)$mean()
  if (t %% 10 == 0)
    cat("Epoch: ", t, "   Loss: ", loss$item(), "\n")
  
  ### -------- Backpropagation --------
  
  # compute gradient of loss w.r.t. all tensors with
  # requires_grad = TRUE
  loss$backward()
  
  ### -------- Update weights -------- 
  
  # Wrap in with_no_grad() because this is a part we don't 
  # want to record for automatic gradient computation
   with_no_grad({
     w1 <- w1$sub_(learning_rate * w1$grad)
     w2 <- w2$sub_(learning_rate * w2$grad)
     b1 <- b1$sub_(learning_rate * b1$grad)
     b2 <- b2$sub_(learning_rate * b2$grad)  
     
     # Zero gradients after every pass, as they'd
     # accumulate otherwise
     w1$grad$zero_()
     w2$grad$zero_()
     b1$grad$zero_()
     b2$grad$zero_()  
   })

}
```

    Epoch: 10 Loss: 24.92771
    Epoch: 20 Loss: 23.56143
    Epoch: 30 Loss: 22.3069
    Epoch: 40 Loss: 21.14102
    Epoch: 50 Loss: 20.05027
    Epoch: 60 Loss: 19.02925
    Epoch: 70 Loss: 18.07328
    Epoch: 80 Loss: 17.16819
    Epoch: 90 Loss: 16.31367
    Epoch: 100 Loss: 15.51261
    Epoch: 110 Loss: 14.76012
    Epoch: 120 Loss: 14.05348
    Epoch: 130 Loss: 13.38944
    Epoch: 140 Loss: 12.77219
    Epoch: 150 Loss: 12.19302
    Epoch: 160 Loss: 11.64823
    Epoch: 170 Loss: 11.13535
    Epoch: 180 Loss: 10.65219
    Epoch: 190 Loss: 10.19666
    Epoch: 200 Loss: 9.766989


## 7. Modules {#sec:modules}

In `torch`, a module can be of any complexity, ranging from basic *layers* -- like the `nn_linear()` we are going to introduce in a minute -- to complete *models* consisting of many such layers. Code-wise, there is no difference between "layers"\index{layer (terminology)} and "models"\index{model (terminology)}. This book, will mostly stay with the common terminology of layers and models.

**Built-in `nn_module()`**

In `torch`, a linear layer is created using `nn_linear()`. `nn_linear()` expects (at least) two arguments: `in_features` and `out_features`.

```{r}
library(torch)
l <- nn_linear(in_features = 5, out_features = 16)
l
```

    An `nn_module` containing 96 parameters.
    Parameters
     weight: Float [1:16, 1:5]
     bias: Float [1:16]

```{r}
l$weight
```

    torch_tensor
    -0.2079 -0.1920  0.2926  0.0036 -0.0897
     0.3658  0.0076 -0.0671  0.3981 -0.4215
     0.2568  0.3648 -0.0374 -0.2778 -0.1662
     0.4444  0.3851 -0.1225  0.1678 -0.3443
    -0.3998  0.0207 -0.0767  0.4323  0.1653
     0.3997  0.0647 -0.2823 -0.1639 -0.0225
     0.0479  0.0207 -0.3426 -0.1567  0.2830
     0.0925 -0.4324  0.0448 -0.0039  0.1531
    -0.2924 -0.0009 -0.1841  0.2028  0.1586
    -0.3064 -0.4006 -0.0553 -0.0067  0.2575
    -0.0472  0.1238 -0.3583  0.4426 -0.0269
    -0.0275 -0.0295 -0.2687  0.2236  0.3787
    -0.2617 -0.2221  0.1503 -0.0627  0.1094
     0.0122  0.2041  0.4466  0.4112  0.4168
    -0.4362 -0.3390  0.3679 -0.3045  0.1358
     0.2979  0.0023  0.0695 -0.1906 -0.1526
    [ CPUFloatType{16,5} ]

```{r}
l$bias
```

    torch_tensor
    -0.2314
     0.2942
     0.0567
    -0.1728
    -0.3220
    -0.1553
    -0.4149
    -0.2103
    -0.1769
     0.4219
    -0.3368
     0.0689
     0.3625
    -0.1391
    -0.1411
    -0.2014
    [ CPUFloatType{16} ]

```{r}
x <- torch_randn(50, 5)
output <- l(x)
output$size() # foward pass
```

    [1] 50 16


**Models as sequences of layers: `nn_sequential()`**

If all our model should do is propagate straight through the layers, we can use `nn_sequential()` to build it. Models consisting of all linear layers are known as *Multi-Layer Perceptrons* (MLPs). Here is one:

```{r}
mlp <- nn_sequential(
  nn_linear(10, 32),
  nn_relu(),
  nn_linear(32, 64),
  nn_relu(),
  nn_linear(64, 1)
)
```

**Other modules**

-   `nn_conv1d()`, `nn_conv2d(), and nn_conv3d()`, the so-called *convolutional* layers that apply filters to input data of varying dimensionality,

-   `nn_lstm()` and `nn_gru()` , the *recurrent* layers that carry through a state,

-   `nn_embedding()` that is used to embed categorical data in high-dimensional space,

-   and more.

## 8. Optimizers {#sec:optimizers}

Why optimizers? 

**1. Optimizers help in updating weights**

```{r}
library(torch)

# compute gradient of loss w.r.t. all tensors with
# requires_grad = TRUE
loss$backward()
  
### -------- Update weights -------- 
  
# Wrap in with_no_grad() because this is a part we don't 
# want to record for automatic gradient computation
with_no_grad({
  w1 <- w1$sub_(learning_rate * w1$grad)
  w2 <- w2$sub_(learning_rate * w2$grad)
  b1 <- b1$sub_(learning_rate * b1$grad)
  b2 <- b2$sub_(learning_rate * b2$grad)  
     
  # Zero gradients after every pass, as they'd accumulate
  # otherwise
  w1$grad$zero_()
  w2$grad$zero_()
  b1$grad$zero_()
  b2$grad$zero_()  
})
```

Now, update weights with an optimizer.

```{r}
# compute gradient of loss w.r.t. all tensors with
# requires_grad = TRUE
# no change here
loss$backward()

# Still need to zero out gradients before the backward pass,
# only this time, on the optimizer object
optimizer$zero_grad()

# use the optimizer to update model parameters
optimizer$step()
```

**2. Optimizers help minimize the loss function**

**Things that matter:**

1. Keep momentum. instead of starting in a completely new direction every time we re-compute the gradient, we might want to keep a bit of the old direction.
2. Update individual learning rates. Looking back at just that example of minimizing a non-symmetric function ... Why, really, should we be constrained to using the same learning rate for all variables? When it's evident that all variables don't vary to the same degree.
3. Update parameters as learning progresses. This is a fix for problems that only arise once you've taken actions to reduce the learning rate for overly-impactful features -- you also want to make sure that as learning still progresses, that parameters still get updated.

**Update strategies:**

* Stochastic gradient descent (baseline)
* Gradient descent with momentum
* Adagrad
* RMSProp
* Adam

Adam combines momentum and parameter-dependent updates, to avoid excessive dependence on fast-changing parameters.

![Adam (white), compared with vanilla SGD (gray).](images/optimizers-adam.png)
![Adam (white), compared with vanilla SGD (gray).](images/optimizers-adam.png)



## 9. Loss functions

**Mean squared error**

```{r}
library(torch)

loss <- (y_pred - y)$pow(2)$sum()
```

**`torch` loss functions**

In `torch`, loss functions start with `nn_` or `nnf_`.

```{r}
nnf_mse_loss(torch_ones(2, 2), torch_zeros(2, 2) + 0.1)
```

    torch_tensor
    0.81
    [ CPUFloatType{} ]


**What loss function should I choose?**

* Maximum likelihood
* Regression
* Classification


|        |          |             |            |               |           |
|--------|----------|-------------|------------|---------------|-----------|
|        | **Data** |             | **Input**  |               |           |
|        | binary   | multi-class | raw scores | probabilities | log probs |
| *BCeL* | Y        |             | Y          |               |           |
| *Ce*   |          | Y           | Y          |               |           |
| *BCe*  | Y        |             |            | Y             |           |
| *Nll*  |          | Y           |            |               | Y         |

: Loss functions, by type of data they work on (binary vs. multi-class) and expected input (raw scores, probabilities, or log probabilities). {#tbl-loss-funcs-features}

|        |                                          |
|--------|------------------------------------------|
| *BCeL* | `nnf_binary_cross_entropy_with_logits()` |
| *Ce*   | `nnf_cross_entropy()`                    |
| *BCe*  | `nnf_binary_cross_entropy()`             |
| *Nll*  | `nnf_nll_loss()`                         |



## 10. Function minimization with L-BFGS

So far, we've only talked about the kinds of optimizers often used in deep learning -- stochastic gradient descent (SGD), SGD with momentum, and a few classics from the *adaptive* *learning rate* family: RMSProp, Adadelta, Adagrad, Adam. All these have in common one thing: They only make use of the *gradient*, that is, the vector of first derivatives. Accordingly, they are all *first-order* algorithms. This means, however, that they are missing out on helpful information provided by the *Hessian*, the matrix of second derivatives.

**Approximate Newton: BFGS and L-BFGS**

Among approximate Newton methods, probably the most-used is the Broyden-Goldfarb-Fletcher-Shanno algorithm, or BFGS. Instead of continually computing the exact inverse of the Hessian, it keeps an iteratively-updated approximation of that inverse. BFGS is often implemented in a more memory-friendly version, referred to as Limited-Memory BFGS (L-BFGS). This is the one provided as part of the core torch optimizers.

**Line search**

With line search, we spend some time evaluating how far to follow the descent direction. There are two principal ways of doing this.

1. Exact. Take the current point, compute the descent direction, and hard-code them as givens in a *second* function that depends on the learning rate only. Then, differentiate this function to find *its* minimum. The solution will be the learning rate that optimizes the step length taken.
2. Approximate search. Essentially, we look for something that is *just* *good enough*. Among the most established heuristics are the *Strong Wolfe conditions*, and this is the strategy implemented in `torch`'s `optim_lbfgs()`.

```{r}
num_iterations <- 2

x <- torch_tensor(c(-1, 1), requires_grad = TRUE)

optimizer <- optim_lbfgs(x, line_search_fn = "strong_wolfe")

calc_loss <- function() {
  optimizer$zero_grad()

  value <- rosenbrock(x)
  cat("Value is: ", as.numeric(value), "\n")

  value$backward()
  value
}

for (i in 1:num_iterations) {
  cat("\nIteration: ", i, "\n")
  optimizer$step(calc_loss)
}
```

## 11. Modularizing the neural network [Exercise]

Conceptually, we distinguish four phases. We make the code more modular and more readable.

-   The forward pass, instead of calling functions on tensors, will call the model.

-   In computing the loss, we now make use of `torch`'s `nnf_mse_loss()`.

-   Backpropagation of gradients is, in fact, the only operation that remains unchanged.

-   Weight updating is taken care of by the optimizer.

**Data**

```{r}
library(torch)

# input dimensionality (number of input features)
d_in <- 3
# number of observations in training set
n <- 100

x <- torch_randn(n, d_in)
coefs <- c(0.2, -1.3, -0.5)
y <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)
```

**Network**

```{r}
# dimensionality of hidden layer
d_hidden <- 32
# output dimensionality (number of predicted features)
d_out <- 1

net <- nn_sequential(
  nn_linear(d_in, d_hidden),
  nn_relu(),
  nn_linear(d_hidden, d_out)
)
```

**Training**

```{r}

opt <- optim_adam(net$parameters)

#### training loop --------------------------------------

for (t in 1:200) {
  
  ### -------- Forward pass --------
  y_pred <- net(x)
  
  ### -------- Compute loss -------- 
  loss <- nnf_mse_loss(y_pred, y)
  if (t %% 10 == 0)
    cat("Epoch: ", t, "   Loss: ", loss$item(), "\n")
  
  ### -------- Backpropagation --------
  opt$zero_grad()
  loss$backward()
  
  ### -------- Update weights -------- 
  opt$step()

}
```




# Deep learning with torch

## 12. Overview

The upcoming two chapters will introduce you to workflow-related techniques that are indispensable in practice. You'll encounter another package, `luz`, that endows `torch` with an important layer of abstraction, and significantly streamlines the workflow. Once you know how to use it, we're all set to look at a first application: image classification.

Regarding workflow, we'll see how to:

-   prepare the input data in a form the model can work with;
-   effectively and efficiently train a model, monitoring progress and adjusting hyper-parameters on the fly;
-   save and load models;
-   making models generalize beyond the training data;
-   speed up training;
-   and more.



## 13. Loading data

**Using `dataset()`**

1. `initialize(...)`
2. `.getitem(i)`
3. `.length()`

```{r}
ds <- dataset()(
  initialize = function(...) {
    ...
  },
  .getitem = function(index) {
    ...
  },
  .length = function() {
    ...
  }
)
```

```{r}
library(torch)
library(palmerpenguins)
library(dplyr)

penguins_dataset <- dataset(
  name = "penguins_dataset()",
  initialize = function(df) {
    df <- na.omit(df)
    self$x <- as.matrix(df[, 3:6]) %>% torch_tensor()
    self$y <- torch_tensor(
      as.numeric(df$species)
    )$to(torch_long())
  },
  .getitem = function(i) {
    list(x = self$x[i, ], y = self$y[i])
  },
  .length = function() {
    dim(self$x)[1]
  }
)

ds[1]
```

    $x
    torch_tensor
       39.1000
       18.7000
      181.0000
     3750.0000
    [ CPUFloatType{4} ]

    $y
    torch_tensor
    1
    [ CPULongType{} ]

**`tensor_dataset()`**

```{r}
penguins <- na.omit(penguins)
ds <- tensor_dataset(
  torch_tensor(as.matrix(penguins[, 3:6])),
  torch_tensor(
    as.numeric(penguins$species)
  )$to(torch_long())
)
```


**Using `dataloader()`**

```{r}
dl <- dataloader(ds, batch_size = 32, shuffle = TRUE)

first_batch <- dl %>%
  # obtain an iterator for this dataloader
  dataloader_make_iter() %>% 
  dataloader_next()

dim(first_batch$x)
dim(first_batch$y)
```

    [1] 32  1 28 28
    [1] 32



## 14. Training with luz [Exercise]

At this point in the book, you know how to train a neural network. Truth be told, though, there's some cognitive effort involved in having to remember the right execution order of steps like `optimizer$zero_grad()`, `loss$backward()`, and `optimizer$step()`. Also, in more complex scenarios than our running example, the list of things to actively remember gets longer.

One thing we haven't talked about yet, for example, is how to handle the usual three stages of machine learning: training, validation, and testing. Another is the question of data flow between *devices* (CPU and GPU, if you have one). Both topics necessitate additional code to be introduced to the training loop. Writing this code can be tedious, and creates a potential for mistakes.

You can see exactly what I'm referring to in the appendix at the end of this chapter. But now, I want to focus on the remedy: a high-level, easy-to-use, concise way of organizing and instrumenting the training process, contributed by a package built on top of `torch`: `luz`.

### Que haya luz - Que haja luz - Let there be light

A *torch* already brings some light, but sometimes in life, there is no *too bright*. `luz` was designed to make deep learning with `torch` as effortless as possible, while at the same time allowing for easy customization. In this chapter, we focus on the overall process; examples of customization will appear in later chapters.

For ease of comparison, we take our running example, and add a third version, now using `luz`. First, we "just" directly port the example; then, we adapt it to a more realistic scenario. In that scenario, we

-   make use of separate training, validation, and test sets;

-   have `luz` compute *metrics* during training/validation;

-   illustrate the use of *callbacks* to perform custom actions or dynamically change hyper-parameters during training; and

-   explain what is going on with the aforementioned *devices*.

### Porting the toy example

#### Data

`luz` does not just substantially transform the code required to train a neural network; it also adds flexibility on the data side of things. In addition to a reference to a `dataloader()`, its `fit()` method accepts `dataset()`s, tensors, and even R objects, as we'll be able to verify soon.

We start by generating an R matrix and a vector, as before. This time though, we also wrap them in a `tensor_dataset()`, and instantiate a `dataloader()`. Instead of just 100, we now generate 1000 observations.

```{r}
library(torch)
library(luz)

# input dimensionality (number of input features)
d_in <- 3
# number of observations in training set
n <- 1000

x <- torch_randn(n, d_in)
coefs <- c(0.2, -1.3, -0.5)
y <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)

ds <- tensor_dataset(x, y)

dl <- dataloader(ds, batch_size = 100, shuffle = TRUE)
```

#### Model

To use `luz`, no changes are needed to the model definition. Note, though, that we just *define* the model architecture; we never actually *instantiate* a model object ourselves.

```{r}
# dimensionality of hidden layer
d_hidden <- 32
# output dimensionality (number of predicted features)
d_out <- 1

net <- nn_module(
  initialize = function(d_in, d_hidden, d_out) {
    self$net <- nn_sequential(
      nn_linear(d_in, d_hidden),
      nn_relu(),
      nn_linear(d_hidden, d_out)
    )
  },
  forward = function(x) {
    self$net(x)
  }
)
```

#### Training

To train the model, we don't write loops anymore. `luz` replaces the familiar *iterative* style by a *declarative* one: You tell `luz` what you want to happen, and like a docile sorcerer's apprentice, it sets in motion the machinery.

Concretely, instruction happens in two -- required -- calls.

1.  In `setup()`\index{\texttt{setup()} (luz)}, you specify the loss function and the optimizer to use.
2.  In `fit()`\index{\texttt{fit()} (luz)}, you pass reference(s) to the training (and optionally, validation) data, as well as the number of epochs to train for.

If the model is configurable -- meaning, it accepts arguments to `initialize()` -- a third method comes into play: `set_hparams()`\index{\texttt{set{\textunderscore}hparams()} (luz)}, to be called in-between the other two. (That's `hparams` for hyper-parameters.) Using this mechanism, you can easily experiment with, for example, different layer sizes, or other factors suspected to affect performance.

```{r}
fitted <- net %>%
  setup(loss = nn_mse_loss(), optimizer = optim_adam) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(dl, epochs = 20)
```

Running this code, you should see output approximately like this:

    Epoch 1/200
    Train metrics: Loss: 3.0343                                                                               
    Epoch 2/200
    Train metrics: Loss: 2.5387                                                                               
    Epoch 3/200
    Train metrics: Loss: 2.2758                                                                               
    ...
    ...
    Epoch 198/200
    Train metrics: Loss: 0.891                                                                                
    Epoch 199/200
    Train metrics: Loss: 0.8879                                                                               
    Epoch 200/200
    Train metrics: Loss: 0.9036 

Above, what we passed to `fit()` was the `dataloader()`. Let's check that referencing the `dataset()` would have been just as fine:

```{r}
fitted <- net %>%
  setup(loss = nn_mse_loss(), optimizer = optim_adam) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(ds, epochs = 20)
```

Or even, `torch` tensors:

```{r}
fitted <- net %>%
  setup(loss = nn_mse_loss(), optimizer = optim_adam) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(list(x, y), epochs = 20)
```

And finally, R objects, which can be convenient when we aren't already working with tensors.

```{r}
fitted <- net %>%
  setup(loss = nn_mse_loss(), optimizer = optim_adam) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(list(as.matrix(x), as.matrix(y)), epochs = 20)
```

In the following sections, we'll always be working with `dataloader()`s; but in some cases those "shortcuts" may come in handy.

Next, we extend the toy example, illustrating how to address more complex requirements.

### A more realistic scenario

#### Integrating training, validation, and test

In deep learning, training and validation phases are interleaved. Every epoch of training is followed by an epoch of validation. Importantly, the data used in both phases have to be strictly disjoint.

In each training phase, gradients are computed and weights are changed; during validation, none of that happens. Why have a validation set, then? If, for each epoch, we compute task-relevant metrics for both partitions, we can see if we are *overfitting* to the training data: that is, drawing conclusions based on training sample specifics not descriptive of the overall population we want to model. All we have to do is two things: instruct `luz` to compute a suitable metric, and pass it an additional `dataloader` pointing to the validation data.

The former is done in `setup()`, and for a regression task, common choices are mean squared or mean absolute error (MSE or MAE, resp.). As we're already using MSE as our loss, let's choose MAE for a metric:

```
fitted <- net %>%
  setup(
    loss = nn_mse_loss(),
    optimizer = optim_adam,
    metrics = list(luz_metric_mae())
  ) %>%
  fit(...)
```

The validation `dataloader` is passed in `fit()` -- but to be able to reference it, we need to construct it first! So now (anticipating we'll want to have a test set, too), we split up the original 1000 observations into three partitions, creating a `dataset` and a `dataloader` for each of them.

```{r}
train_ids <- sample(1:length(ds), size = 0.6 * length(ds))
valid_ids <- sample(
  setdiff(1:length(ds), train_ids),
  size = 0.2 * length(ds)
)
test_ids <- setdiff(
  1:length(ds),
  union(train_ids, valid_ids)
)

train_ds <- dataset_subset(ds, indices = train_ids)
valid_ds <- dataset_subset(ds, indices = valid_ids)
test_ds <- dataset_subset(ds, indices = test_ids)

train_dl <- dataloader(train_ds,
  batch_size = 100, shuffle = TRUE
)
valid_dl <- dataloader(valid_ds, batch_size = 100)
test_dl <- dataloader(test_ds, batch_size = 100)
```

Now, we are ready to start the enhanced workflow:

```{r}
fitted <- net %>%
  setup(
    loss = nn_mse_loss(),
    optimizer = optim_adam,
    metrics = list(luz_metric_mae())
  ) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(train_dl, epochs = 20, valid_data = valid_dl)
```

    Epoch 1/200
    Train metrics: Loss: 2.5863 - MAE: 1.2832                                       
    Valid metrics: Loss: 2.487 - MAE: 1.2365
    Epoch 2/200
    Train metrics: Loss: 2.4943 - MAE: 1.26                                          
    Valid metrics: Loss: 2.4049 - MAE: 1.2161
    Epoch 3/200
    Train metrics: Loss: 2.4036 - MAE: 1.236                                         
    Valid metrics: Loss: 2.3261 - MAE: 1.1962
    ...
    ...
    Epoch 198/200
    Train metrics: Loss: 0.8947 - MAE: 0.7504
    Valid metrics: Loss: 1.0572 - MAE: 0.8287
    Epoch 199/200
    Train metrics: Loss: 0.8948 - MAE: 0.7503
    Valid metrics: Loss: 1.0569 - MAE: 0.8286
    Epoch 200/200
    Train metrics: Loss: 0.8944 - MAE: 0.75
    Valid metrics: Loss: 1.0579 - MAE: 0.8292

Even though both training and validation sets come from the exact same distribution, we do see a bit of overfitting. This is a topic we'll talk about more in the next chapter.

Once training has finished, the `fitted` object above holds a history of epoch-wise metrics, as well as references to a number of important objects involved in the training process. Among the latter is the fitted model itself -- which enables an easy way to obtain predictions on the test set:\index{\texttt{predict()} (luz)}

```{r}
fitted %>% predict(test_dl)
```

    torch_tensor
     0.7799
     1.7839
    -1.1294
    -1.3002
    -1.8169
    -1.6762
    -0.7548
    -1.2041
     2.9613
    -0.9551
     0.7714
    -0.8265
     1.1334
    -2.8406
    -1.1679
     0.8350
     2.0134
     2.1083
     1.4093
     0.6962
    -0.3669
    -0.5292
     2.0310
    -0.5814
     2.7494
     0.7855
    -0.5263
    -1.1257
    -3.3117
     0.6157
    ... [the output was truncated (use n=-1 to disable)]
    [ CPUFloatType{200,1} ]

We also want to evaluate performance on the test set:\index{\texttt{evaluate()} (luz)}

```{r}
fitted %>% evaluate(test_dl)
```

    A `luz_module_evaluation`
    ── Results 
    loss: 0.9271
    mae: 0.7348

This workflow of: training and validation in lock-step, then checking and extracting predictions on the test set is something we'll encounter times and again in this book.

#### Using callbacks to "hook" into the training process\index{callbacks (luz)}

At this point, you may feel that what we've gained in code efficiency, we may have lost in flexibility. Coding the training loop yourself, you can arrange for all kinds of things to happen: save model weights, adjust the learning rate ... whatever you need.

In reality, no flexibility is lost. Instead, `luz` offers a standardized way to achieve the same goals: callbacks. Callbacks are objects that can execute arbitrary R code, at any of the following points in time:

-   when the overall training process starts or ends (`on_fit_begin()` / `on_fit_end()`);

-   when an epoch (comprising training and validation) starts or ends (`on_epoch_begin()` / `on_epoch_end()`);

-   when during an epoch, the training (validation, resp.) phase starts or ends (`on_train_begin()` / `on_train_end()`; `on_valid_begin()` / `on_valid_end()`);

-   when during training (validation, resp.), a new batch is either about to be or has been processed (`on_train_batch_begin()` / `on_train_batch_end()`; `on_valid_batch_begin()` / `on_valid_batch_end()`);

-   and even at specific landmarks inside the "innermost" training / validation logic, such as "after loss computation", "after `backward()`" or "after `step()`".

While you can implement any logic you wish using callbacks (and we'll see how to do this in a later chapter), `luz` already comes equipped with a very useful set. For example:

-   `luz_callback_model_checkpoint()` saves model weights after every epoch (or just in case of improvements, if so instructed).

-   `luz_callback_lr_scheduler()` activates one of `torch`'s *learning rate schedulers*. Different scheduler objects exist, each following their own logic in dynamically updating the learning rate.

-   `luz_callback_early_stopping()` terminates training once model performance stops to improve. What exactly "stops to improve" should mean is configurable by the user.

Callbacks are passed to the `fit()` method in a list. For example, augmenting our most recent workflow:

```{r}
fitted <- net %>%
  setup(
    loss = nn_mse_loss(),
    optimizer = optim_adam,
    metrics = list(luz_metric_mae())
  ) %>%
  set_hparams(d_in = d_in,
              d_hidden = d_hidden,
              d_out = d_out) %>%
  fit(
    train_dl,
    epochs = 20,
    valid_data = valid_dl,
    callbacks = list(
      luz_callback_model_checkpoint(path = "./models/",
                                    save_best_only = TRUE),
      luz_callback_early_stopping(patience = 10)
    )
  )

```

With this configuration, weights will be saved, but only if validation loss decreases. Training will halt if there is no improvement (again, in validation loss) for ten epochs. With both callbacks, you can pick any other metric to base the decision on, and the metric in question may also refer to the training set.

Here, we see early stopping happening after 111 epochs:

    Epoch 1/200
    Train metrics: Loss: 2.5803 - MAE: 1.2547
    Valid metrics: Loss: 3.3763 - MAE: 1.4232
    Epoch 2/200
    Train metrics: Loss: 2.4767 - MAE: 1.229
    Valid metrics: Loss: 3.2334 - MAE: 1.3909
    ...
    ...
    Epoch 110/200
    Train metrics: Loss: 1.011 - MAE: 0.8034
    Valid metrics: Loss: 1.1673 - MAE: 0.8578
    Epoch 111/200
    Train metrics: Loss: 1.0108 - MAE: 0.8032
    Valid metrics: Loss: 1.167 - MAE: 0.8578
    Early stopping at epoch 111 of 200

#### How `luz` helps with devices\index{device handling (luz)}

Finally, let's quickly mention how `luz` helps with device placement. Devices, in a usual environment, are the CPU and perhaps, if available, a GPU. For training, data and model weights need to be located on the same device. This can introduce complexities, and -- at the very least -- necessitates additional code to keep all pieces in sync.

With `luz`, related actions happen transparently to the user. Let's take the prediction step from above:

```{r}
fitted %>% predict(test_dl)
```

In case this code was executed on a machine that has a GPU, `luz` will have detected that, and the model's weight tensors will already have been moved there. Now, for the above call to `predict()`, what happened "under the hood" was the following:

-   `luz` put the model in evaluation mode, making sure that weights are not updated.
-   `luz` moved the test data to the GPU, batch by batch, and obtained model predictions.
-   These predictions were then moved back to the CPU, in anticipation of the caller wanting to process them further with R. (Conversion functions like `as.numeric()`, `as.matrix()` etc. can only act on CPU-resident tensors.)

In the below appendix, you find a complete walk-through of how to implement the train-validate-test workflow by hand. You'll likely find this a lot more complex than what we did above -- and it does not even bring into play metrics, or any of the functionality afforded by `luz` callbacks.

In the next chapter, we discuss essential ingredients of modern deep learning we haven't yet touched upon; and following that, we look at specific architectures destined to specifically handle different tasks and domains.

### Appendix: A train-validate-test workflow implemented by hand

For clarity, we repeat here the two things that do *not* depend on whether you're using `luz` or not: `dataloader()` preparation and model definition.

```

# input dimensionality (number of input features)
d_in <- 3
# number of observations in training set
n <- 1000

x <- torch_randn(n, d_in)
coefs <- c(0.2, -1.3, -0.5)
y <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)

ds <- tensor_dataset(x, y)

dl <- dataloader(ds, batch_size = 100, shuffle = TRUE)

train_ids <- sample(1:length(ds), size = 0.6 * length(ds))
valid_ids <- sample(setdiff(
  1:length(ds),
  train_ids
), size = 0.2 * length(ds))
test_ids <- setdiff(1:length(ds), union(train_ids, valid_ids))

train_ds <- dataset_subset(ds, indices = train_ids)
valid_ds <- dataset_subset(ds, indices = valid_ids)
test_ds <- dataset_subset(ds, indices = test_ids)

train_dl <- dataloader(train_ds,
  batch_size = 100,
  shuffle = TRUE
)
valid_dl <- dataloader(valid_ds, batch_size = 100)
test_dl <- dataloader(test_ds, batch_size = 100)

# dimensionality of hidden layer
d_hidden <- 32
# output dimensionality (number of predicted features)
d_out <- 1

net <- nn_module(
  initialize = function(d_in, d_hidden, d_out) {
    self$net <- nn_sequential(
      nn_linear(d_in, d_hidden),
      nn_relu(),
      nn_linear(d_hidden, d_out)
    )
  },
  forward = function(x) {
    self$net(x)
  }
)
```

Recall that with `luz`, now all that separates you from watching how training and validation losses evolve is a snippet like this:

```
fitted <- net %>%
  setup(
    loss = nn_mse_loss(),
    optimizer = optim_adam
  ) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(train_dl, epochs = 200, valid_data = valid_dl)
```

Without `luz`, however, things to be taken care of fall into three distinct categories.

First, instantiate the network, and, if CUDA is installed, move its weights to the GPU.

```
device <- torch_device(if
(cuda_is_available()) {
  "cuda"
} else {
  "cpu"
})

model <- net(d_in = d_in, d_hidden = d_hidden, d_out = d_out)
model <- model$to(device = device)

```

Second, create an optimizer.

```
optimizer <- optim_adam(model$parameters)
```

And third, the biggest chunk: In each epoch, iterate over training batches as well as validation batches, performing backpropagation when working on the former, while just passively reporting losses when processing the latter.

For clarity, we pack training logic and validation logic each into their own functions. `train_batch()` and `valid_batch()` will be called from inside loops over the respective batches. Those loops, in turn, will be executed for every epoch.

While `train_batch()` and `valid_batch()`, per se, trigger the usual actions in the usual order, note the device placement calls: For the model to be able to take in the data, they have to live on the same device. Then, for mean-squared-error computation to be possible, the target tensors need to live there as well.

```

train_batch <- function(b) {
  optimizer$zero_grad()
  output <- model(b[[1]]$to(device = device))
  target <- b[[2]]$to(device = device)
  loss <- nnf_mse_loss(output, target)
  loss$backward()
  optimizer$step()
  loss$item()
}
valid_batch <- function(b) {
  output <- model(b[[1]]$to(device = device))
  target <- b[[2]]$to(device = device)
  loss <- nnf_mse_loss(output, target)
  loss$item()
}

valid_batch <- function(b) {
  output <- model(b[[1]]$to(device = device))
  target <- b[[2]]$to(device = device)

  loss <- nn_mse_loss(output, target)
  loss$item()
}

```

The loop over epochs contains two lines that deserve special attention: `model$train()` and `model$eval()`. The former instructs `torch` to put the model in training mode; the latter does the opposite. With the simple model we're using here, it wouldn't be a problem if you forgot those calls; however, when later we'll be using regularization layers like `nn_dropout()` and `nn_batch_norm2d()`, calling these methods in the correct places is essential. This is because these layers behave differently during evaluation and training.

```
num_epochs <- 200

for (epoch in 1:num_epochs) {
  model$train()
  train_loss <- c()

  # use coro::loop() for stability and performance
  coro::loop(for (b in train_dl) {
    loss <- train_batch(b)
    train_loss <- c(train_loss, loss)
  })

  cat(sprintf(
    "\nEpoch %d, training: loss: %3.5f \n",
    epoch, mean(train_loss)
  ))

  model$eval()
  valid_loss <- c()

  # disable gradient tracking to reduce memory usage
  with_no_grad({ 
    coro::loop(for (b in valid_dl) {
      loss <- valid_batch(b)
      valid_loss <- c(valid_loss, loss)
    })  
  })
  
  cat(sprintf(
    "\nEpoch %d, validation: loss: %3.5f \n",
    epoch, mean(valid_loss)
  ))
}
```

This completes our walk-through of manual training, and should have made more concrete my assertion that using `luz` significantly reduces the potential for casual (e.g., copy-paste) errors.




## 15. A first go at image classification [Exercise]

### What does it take to classify an image?

Think about how we, as human beings, can say "that's a cat", or: "this is a dog". No conscious processing is required. (Usually, that is.)

Why? The neuroscience, and cognitive psychology, involved are definitely out of scope for this book; but on a high level, we can assume that there are at least two prerequisites: First, that our visual system be able to build up complex representations out of lower-level ones, and second, that we have a set of concepts available we can map those high-level representations to. Presumably, then, an algorithm expected to do the same thing needs to be endowed with these same capabilities.

In the context of this chapter, dedicated to image classification, the second prerequisite is satisfied gratuitously. Classification being a variant of supervised machine learning, the concepts are given by means of the targets. The first, however, is all-important. We can again distinguish two components: the capability to detect low-level features, and that to successively compose them into higher-level ones.

Take a simple example. What would be required to identify a rectangle? A rectangle consists of edges: straight-ish borders of sort where something in the visual impression (color, for example) changes. To start with, then, the algorithm would have to be able to identify a single edge. That "edge extractor", as we might call it, is going to mark all four edges in the image. In this case, no further composition of features is needed; we can directly infer the concept.

On the other hand, assume the image were showing a house built of bricks. Then, there would be many rectangles, together forming a wall of the house; another rectangle, the door; and a few further ones, the windows. Maybe there'd be a different arrangement of edges, triangle-shaped, the roof. Meaning, an edge detector is not enough: We also need a "rectangle detector", a "triangle detector", a "wall detector", a "roof detector" ... and so on. Evidently, these detectors can't all be programmed up front. They'll have to be emergent properties of the algorithm: the neural network, in our case.

### Neural networks for feature detection\index{feature detection} and feature emergence

The way we've spelled out the requirements, a neural network for image classification needs to (1) be able to detect features, and (2) build up a hierarchy of such. Networks being networks, we can safely assume that (1) will be taken care of by a specialized layer (module), while (2) will be made possible by chaining several layers.

#### Detecting low-level features with cross-correlation\index{cross-correlation}

This chapter is about "convolutional" neural networks; the specialized module in question is the "convolutional" one. Why, then, am I talking about cross-correlation? It's because what neural-network people refer to as *convolution*\index{convolution}, technically is *cross-correlation*. (Don't worry -- I'll be making the distinction just here, in the conceptual introduction; afterwards I'll be saying "convolution", just like everyone else.)

So why am I insisting? It is for two reasons. First, this book actually *has* a chapter on convolution -- the "real one"; it figures in part three right between matrix operations and the Discrete Fourier Transform. Second, while in a formal sense the difference may be small, semantically as well as in terms of mathematical status, convolution and cross-correlation are decidedly distinct. In broad strokes:

Convolution may well be the most fundamental operation in all of signal processing, fundamental in the way addition and multiplication are. It can act as a *filter*, a signal-space transformation intended to achieve a desired result. For example, a moving average filter can be programmed as a convolution. So can, however, something quite the opposite: a filter that emphasizes differences. (An edge enhancer would be an example of the latter.)

Cross-correlation, in contrast, is more specialized. It *finds* things, or put differently: It spots similarities. This is what is needed in image recognition. To demonstrate how it works, we start in a single dimension.

##### Cross-correlation in one dimension

Assume we have a signal -- a univariate time series -- that looks like this: `0,1,1,1,-1,0,-1,-1,1,1,1,-1`. We want to find locations where a *one* occurs three times in a row. To that end, we make use of a filter that, too, has three ones in a row: `1,1,1`.

That filter, also called a *kernel*, is going to slide over the input sequence, producing an output value at every location. To be precise: The output value in question will be mapped to the *input value co-located with the kernel's central value*. How, then, can we obtain an output for the very first input value, which has no way of being mapped to the center of the kernel? In order for this to work, the input sequence is padded with zeroes: one in front, and one at the end. The new signal looks like this: `0,0,1,1,1,-1,0,-1,-1,1,1,1,-1,0` .

Now, we have the kernel sliding over the signal. Like so:

```         
0,0,1,1,1,-1,0,-1,-1,1,1,1,-1,0
1,1,1

0,0,1,1,1,-1,0,-1,-1,1,1,1,-1,0
   1,1,1

0,0,1,1,1,-1,0,-1,-1,1,1,1,-1,0
      1,1, 1
```

And so on.

At every position, products are computed between the mapped input and kernel values, and then, those products are added up, to yield the output value at the central position. For example, this is what gets computed at the very first matching: `0*1 + 0*1 + 1*1 = 1`. Appending the outputs, we get a new sequence: `1,2,3,1,0,-2,-2,-1,1,3,1,0` .

How does this help in finding three consecutive ones? Well, a three can only result when the kernel has found such a location. Thus, with that choice of kernel, we take every occurrence of `3` in the output as the center of the target sequence we're looking for.

##### Cross-correlation in two dimensions

This chapter is about images; how does that logic carry over to two dimensions?

Everything works just the same; it's just that now, the input signal extends over two dimensions, and the kernel is two-dimensional, as well. Again, the input is padded; with a kernel of size 3 x 3, for example, one row is added on top and bottom each, and one column, on the left and the right. Again, the kernel slides over the image, row by row and column by column. At each point it computes an aggregate that is the sum of point-wise products. Mathematically, that's a *dot product*.

To get a feel for how this works, we look at a bare-bones example: a white square on black background (@fig-images-square).

![White square on black background.](images/images-square.png){#fig-images-square fig-alt="A white square on a black background."}

Nicely, the open-source graphics program Gimp has a feature that allows one to experiment with custom filters ("Filters" -\> "Custom" -\> "Convolution matrix"). We can construct kernels and directly examine their effects.

Say we want to find the left edge of the square. We are looking for locations where the color changes, horizontally, from black to white. This can be achieved with a 3x3 kernel that looks like this (@fig-images-square-left):

```         
 0 0 0
-1 1 0
 0 0 0
```

This kernel is *similar* to the edge type we're interested in in that it has, in the second row, a horizontal transition from -1 to 1.

Analogously, kernels can be constructed that extract the right (@fig-images-square-right), top (@fig-images-square-top), and bottom (@fig-images-square-bottom) edges.

![Gimp convolution matrix that detects the left edge.](images/images-square-left.png){#fig-images-square-left fig-alt="A gimp convolution matrix of size 5x5. Individual values read: Row 1: all zero. Row 2: all zero. Row 3: 0, -1, 1, 0, 0. Row 4: all zero. Row 5: all zero."}

![Gimp convolution matrix that detects the right edge.](images/images-square-right.png){#fig-images-square-right fig-alt="A gimp convolution matrix of size 5x5. Individual values read: Row 1: all zero. Row 2: all zero. Row 3: 0, 1, -1, 0, 0. Row 4: all zero. Row 5: all zero."}

![Gimp convolution matrix that detects the top edge.](images/images-square-top.png){#fig-images-square-top fig-alt="A gimp convolution matrix of size 5x5. Individual values read: Row 1: all zero. Row 2: 0, 0, -1, 0, 0. Row 3: 0, 0, 1, 0, 0. Row 4: all zero. Row 5: all zero."}

![Gimp convolution matrix that detects the bottom edge.](images/images-square-bottom.png){#fig-images-square-bottom fig-alt="A gimp convolution matrix of size 5x5. Individual values read: Row 1: all zero. Row 2: all zero. Row 3: 0, 0, 1, 0, 0. Row 4: 0, 0, -1, 0, 0. Row 5: all zero."}

To understand this numerically, we can simulate a tiny image (@fig-images-cross-correlation, left). The numbers represent a grayscale image with values ranging from 0 to 255. To its right, we have the kernel; this is the one we used to detect the left edge. As a result of having that kernel slide over the image, we obtain the "image" on the right. `0` being the lowest possible value, negative pixels end up black, and we obtain a white edge on black background, just like we saw with Gimp.

![Input image, filter, and result as pixel values. Negative pixel values being impossible, -255 will end up as 0.](images/images-cross-correlation.png){#fig-images-cross-correlation fig-alt="Convolution by example. On the left, the white square on black background, with black pixels mapped to value 0 and white pixels, to 255. In the middle, the 3 x 3 kernel that detects a left edge, with the central row holding values -1, 1, 0, and both other rows being all zero. On the right, the result. Rows 1 and 2 as well as 7 and 8 are all zero; same with columns 1, 2, 4, 5, 6, and 8. Column 3 reads: 0, 0, 1, 1, 1, 1, 0, 0. Column 7 reads: 0, 0, -255, -255, -255, -255, 0, 0."}

Now, we've talked a lot about constructing kernels. Neural networks are all about *learning* feature detectors, not having them programmed up-front. Naturally, then, learning a filter means having a layer type whose weights embody this logic.

##### Convolutional layers in `torch`

So far, the only layer type we've seen that learns weights is `nn_linear()`. `nn_linear()` performs an affine operation: It takes an input tensor, matrix-multiplies it by its weight matrix $\mathbf{W}$, and adds the bias vector $\mathbf{b}$. While there is just a single bias per layer, independently of the number of neurons it has, this is not the case for the weights: There is a unique connection between each feature in the input tensor and each of the layer's neurons.

This is not true for `nn_conv2d()`, `torch`'s (two-dimensional) convolution[^image_classification_1-1] layer.

[^image_classification_1-1]: Like I said above, I'll be using the established term "convolution" from now on. Actually -- given that weights are *learned --* it does not matter that much anyway.

Back to how convolutional layers differ from linear ones. We've already seen what the layer's effect is supposed to be: A *kernel* should slide over its input, generating an output value at each location. Now the kernel, for a convolutional layer, is exactly its weight matrix. The kernel sliding over an input image means that weights are re-used every time it shifts its position. Thus, the number of weights is determined by the size of the kernel, not the size of the input. As a consequence, a convolutional layer is way more economical than a linear one.

Another way to express this is the following.

Conceptually, we are looking for the same thing, wherever it appears in the image. Take the most standard of standard image classification benchmarks, MNIST. It is about classifying images of the Arabic numerals 0-9. Say we want to learn the shape for a 2. The 2 could be right in the middle of the image, or it could be shifted to the left (say). An algorithm should be able to recognize it no matter where. Additional requirements depend on the task. If all we need to be able to do is say "that's a 2", we're good to use an algorithm that is *translation-invariant*\index{invariance (translational)}: It outputs the same thing independently of any shifts that may have occurred. For classification, that's just fine: A 2 is a 2 is a 2.

Another important task, though, is image segmentation (something we'll look at in an upcoming chapter). In segmentation, we want to mark all pixels in an image according to whether they are part of some object or not. Think tumor cells, for example. The 2 is still a 2, but we do need the information where in the image it is located. The algorithm to use now has to be *translation-equivariant*\index{equivariance (translational)}: If a shift has occurred, the target is still detected, but at a new location. And thinking about the convolution algorithm, translation-equivariant is exactly what it is.

So now, we have an idea how `torch` lets us detect individual features in an image. This gives us the first in our list of desiderates. The second is about combining feature detectors, that is, building up a hierarchy, in order to discern more and more specialized types of objects. This means that from a single layer, we move on to a network of layers.

#### Build up feature hierarchies\index{feature hierarchies}

A prototypical convolutional neural network for image classification will chain blocks composed of three types of layers: convolutional ones (`nn_conv1d()`, `nn_conv2d()`, or `nn_conv3d()`, depending on the dimension we're in), activation layers (e.g., `nn_relu()`), and pooling layers (e.g., `nn_maxpool1d()`, `nn_maxpool2d()`, `nn_maxpool3d()`).

The only type we haven't talked about yet are the pooling\index{pooling} layers. Just like activation layers, these don't have any parameters; what they do is aggregate neighboring tensor values. The size of the region to summarize is specified in the layer constructor's parameters. Various types of aggregation are available: `nn_maxpool<n>d()` picks the highest value, while `nn_avg_pool<n>d()` computes the average.

Why would one want to perform these kinds of aggregation? Practically speaking, one *has to* if one wants to arrive at a per-image (as opposed to per-pixel) output. But we can't just choose *any* way of aggregating spatially-arranged values. Picture, for example, an average where the interior pixels of an image patch were weighted higher than the exterior ones. Then, it would make a difference where in the patch some object was located. But for classification, this should not be the case. For classification, as opposed to segmentation, we want translation *invariance* -- not just *equivariance*, the property we just said convolution has. And translation-invariant is just what layers like `nn_maxpool2d()`, `nn_avgpool2d()`, etc. are.

##### A prototypical convnet

A template for a convolutional network, called "convnet" from now on, could thus look as below. To preempt any possible confusion: Even though, above, I was talking about three types of *layers*, there really is just one type in the code: the convolutional one. For brevity, both ReLU activation and max pooling are realized as functions instead.

Here is a possible template. It is not intended as a recommendation (as to number of filters, kernel size, or other hyperparameters, for example) -- just to illustrate the mechanics. More detailed comments follow.

```{r}
library(torch)

convnet <- nn_module(
  "convnet",
  
  initialize = function() {
    
    # nn_conv2d(in_channels, out_channels, kernel_size)
    self$conv1 <- nn_conv2d(1, 16, 3)
    self$conv2 <- nn_conv2d(16, 32, 3)
    self$conv3 <- nn_conv2d(32, 64, 3)
    
    self$output <- nn_linear(2304, 3)

  },
  
  forward = function(x) {
    
    x %>% 
      self$conv1() %>% 
      nnf_relu() %>%
      nnf_max_pool2d(2) %>%
      self$conv2() %>% 
      nnf_relu() %>%
      nnf_max_pool2d(2) %>%
      self$conv3() %>% 
      nnf_relu() %>%
      nnf_max_pool2d(2) %>%
      torch_flatten(start_dim = 2) %>%
      self$output()
      
  }
)

model <- convnet()
```

To understand what is going on, we need to know how images are represented in `torch`. By itself, an image is represented as a three-dimensional tensor, with one dimension indexing into available channels\index{channels (image)} (package luz)} (one for gray-scale images, three for RGB, possibly more for different kinds of imaging outputs), and the other two, corresponding to the two spatial axes, height (rows) and width (columns). In deep learning, we work with batches; thus, there is an additional dimension\index{batch dimension} -- the very first one -- that refers to batch number.

Let's look at an example image that may be used with the above template:

```{r}
img <- torch_randn(1, 1, 64, 64)

```

What we have here is an image, or more precisely, a batch containing a single image, that has a single channel, and is of size 64 x 64.

That said, the above template assumes the following:

1.  The input image has one channel. That's why the first argument to `nn_conv2d()` is `1` when we construct the first of the conv layers. (No assumptions are made, on the other hand, about the size of the input image.)
2.  We want to distinguish between three different target classes. This means that the output layer, a linear module, needs to have three output channels.

To test the code, we can call the un-trained model on our example image:

```{r}
model(img)
```

```         
torch_tensor
0.01 *
 6.4821  3.4166 -5.6050
[ CPUFloatType{1,3} ][ grad_fn = <AddmmBackward0> ]
```

One final note about that template. When you were reading the code above, one line that might have stood out is the following:

```         
self$output <- nn_linear(2304, 3)
```

How did that 2304, the number of input connections to `nn_linear()`, come about? It is the result of (1) a number of operations that each reduce spatial resolution, plus (2) a flattening operation that removes all dimensional information besides the batch dimension. This will make more sense once we've discussed the arguments to the layers in question. But one thing needs to be said upfront: *If this sounds like magic, there is a simple means to make the magic go away.* Namely, a simple way to find out about tensor shapes at any stage in a network is to comment all subsequent actions in `forward()`, and call the modified model. Naturally, this should not replace understanding, but it's a great way not to lose one's nerves when encountering shape errors.

Now, about layer arguments.

##### Arguments to `nn_conv2d()`

Above, we passed three arguments to `nn_conv2d()`: `in_channels`, `out_channels`, and `kernel_size`. This is not an exhaustive list of parameters, though. The remaining ones all have default values, but it is important to know about their existence. We're going to elaborate on three of them, all of whom you're likely to play with applying the template to some concrete task. All of them affect output size. So do two of the three mandatory arguments, `out_channels` and `kernel_size`:

-   `out_channels` refers to the number of kernels\index{kernel} (often called *filters*\index{filter}, in this context) learned. Its value affects the second of the four dimensions of the output tensor; it does not affect spatial resolution, though. Learning more filters adds capacity to the network, as it increases the number of weights.

-   `kernel_size`, on the other hand, *does* alter spatial resolution -- unless its value is 1, in which case the kernel never exceeds image boundaries. Like `out_channels`, it is a candidate for experimentation. In general, though, it is advisable to keep kernel size rather small, and chain a larger number of convolutional layers, instead of enlarging kernel size in a "shallow" network.

Now for the three non-mandatory arguments to explore.

1.  `padding`\index{padding} is something we've encountered before. Any kernel that extends over more than a single pixel will move outside the valid region when sliding over an image; the more, the bigger the kernel. General options are to (1) either pad the image (with zeroes, for example), or (2) compute the dot product only where possible. In the latter case, spatial resolution will decrease. That need not in itself be a problem; like so many things, it's a matter of experimentation. By default, `torch` does not pad images; however by passing a value greater than `0` for `padding`, you can ensure that spatial resolution is preserved, whatever the kernel size. Compare @fig-images-conv-arithmetic-padding, reproduced from a nice compilation by @2016arXiv160307285D, to see the effect of padding.

2.  `stride`\index{stride} refers to the way a kernel moves over the image. With a `stride` greater than `1`, it takes "leaps" of sorts -- see @fig-images-conv-arithmetic-strides. This results in fewer "snapshots" being taken. As a result, spatial resolution decreases.

3.  A setting of `dilation`\index{dilation} greater than `1,` too, results in fewer snapshots, but for a different reason. Now, it's not that the kernel moves faster. Instead, the pixels it is applied to are not adjacent anymore. They're spread out -- how much, depends on the argument's value. See @fig-images-conv-arithmetic-dilation.

![Convolution, and the effect of padding. Copyright @2016arXiv160307285D, reproduced under [MIT license](https://github.com/vdumoulin/conv_arithmetic/blob/master/LICENSE).](images/images-conv-arithmetic-padding.png){#fig-images-conv-arithmetic-padding fig-alt="Comparing convolution with and without padding. A 3 x 3 filter slides over a 4 x 4 image. Top row: The resulting image is of size 2 x 2. Kernel never transcends the image. Bottom row: Padding by two on all sides. On the image's edges, kernel is aligned such that it stands out by two pixels."}

![Convolution, and the effect of `strides`. Copyright @2016arXiv160307285D, reproduced under [MIT license](https://github.com/vdumoulin/conv_arithmetic/blob/master/LICENSE).](images/images-conv-arithmetic-strides.png){#fig-images-conv-arithmetic-strides fig-alt="Convolution done with a strides setting of 2 x 2. A 3 x 3 filter slides over a 5 x 5 image. At each step, it jumps over one pixel, both when sliding horizontally and when sliding vertically."}

![Convolution, and the effect of `dilation`. Copyright @2016arXiv160307285D, reproduced under [MIT license](https://github.com/vdumoulin/conv_arithmetic/blob/master/LICENSE).](images/images-conv-arithmetic-dilation.png){#fig-images-conv-arithmetic-dilation fig-alt="Convolution done with a dilation factor of 2. A 3 x 3 filter slides over a 7 x 7 image. The filter is mapped in a way that there are 1-pixel holes in the image that are not covered by the filter."}

For non-mandatory arguments `padding`, `stride`, and `dilation`, @tbl-images-convargs has a summary of defaults and effects.

| Argument   | Default | Action (if non-default)                                             |
|------------|------------|-----------------------------------------------|
| `padding`  | 0       | virtual rows/columns added around the image                         |
| `stride`   | 1       | kernel moves across image at bigger step size ("jumps" over pixels) |
| `dilation` | 1       | kernel is applied to spread-out image pixels ("holes" in kernel)    |

: Arguments to `nn_conv_2d()` you may want to experiment with -- default values and non-default actions. {#tbl-images-convargs}

##### Arguments to pooling layers

Pooling layers compute aggregates over neighboring pixels. The number of pixels of aggregate over in every dimension is specified in the layer constructor's first argument (alternatively, the corresponding function's second argument). Slightly misleadingly, that argument is called `kernel_size`, although there are no weights involved: For example, in the above template, we were unconditionally taking the maximum pixel value over regions of size 2 x 2.

In analogy to convolution layers, pooling layers also accept arguments `padding` and `stride`. However, they are seldom used.

##### Zooming out

We've talked a lot about layers and their arguments. Let's zoom out and think back about the general template, and what it is supposed to achieve.

We are chaining blocks that, each, perform a convolution, apply a non-linearity, and spatially aggregate the result. Each block's weights act as feature detectors, and every block but the first receives as input something that already is the result of applying one or more feature detectors. The magical thing that happens, and the reason behind the success of convnets, is that by chaining layers, a *hierarchy* of features is built. Early layers detect edges and textures, later ones, patterns of various complexity, and the final ones, objects and parts of objects (see @fig-images-feature-visualization, a beautiful visualization reproduced from @olah2017feature).

![Feature visualization on a subset of layers of GoogleNet. Figure from @olah2017feature, reproduced under [Creative Commons Attribution CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) without modification.](images/images-feature-visualization.png){#fig-images-feature-visualization fig-alt="What layers at different levels of the hierarchy respond to, by example. From left to right: Various species of edges, textures, patterns, parts, objects."}

We now know enough about coding convnets and how they work to explore a real example.

### Classification on Tiny Imagenet

Before we start coding, let me anchor your expectations. In this chapter, we design and train a basic convnet from scratch. As to data pre-processing, we do what is needed, not more. In the next two chapters, we'll learn about common techniques used to improve model training, in terms of quality as well as speed. Once we've covered those, we'll pick up right where this chapter ended, and apply a few of those techniques to the present task. Therefore, what we're doing here is build a baseline, to be used in comparison with more sophisticated approaches. This is "just" a beginning.

#### Data pre-processing

In addition to `torch` and `luz`, we load a third package from the `torch` ecosystem: `torchvision`. `torchvision` provides operations on images, as well as a set of pre-trained models and common benchmark datasets.

```{r}
library(torch)
library(torchvision)
library(luz)
```

The dataset we use is "Tiny Imagenet". Tiny Imagenet is a subset of [ImageNet](https://image-net.org/index.php), a gigantic collection of more than fourteen million images, initially made popular through the "ImageNet Large Scale Visual Recognition Challenge" that was run between 2012 and 2017. In the challenge, the most popular task was multi-class classification, with one thousand different classes to choose from.

One thousand classes is a lot; and with images typically being processed at a resolution of 256 x 256, training a model takes a lot of time, even on luxurious hardware. For that reason, a more manageable version was created as part of a popular Stanford class on deep learning for images, [Convolutional Neural Networks for Visual Recognition (CS231n)](http://cs231n.stanford.edu/). The condensed dataset has two hundred classes, with five hundred training images per class. Two hundred classes, that's still a lot! (Most introductory examples will do "cats vs. dogs", or some other binary problem.) Thus, it's not an easy task.

We start by downloading the data.

Linux:

```
wget http://cs231n.stanford.edu/tiny-imagenet-200.zip
unzip tiny-imagenet-200.zip
```

Windows:

```
tiny_imagenet_dataset(".", download = TRUE)
```

```{r}
set.seed(777)
torch_manual_seed(777)

train_ds <- tiny_imagenet_dataset(
  root=".",
  download = FALSE,
  transform = function(x) {
    x %>%
      transform_to_tensor() 
  }
)

valid_ds <- tiny_imagenet_dataset(
  root=".",
  split = "val",
  transform = function(x) {
    x %>%
      transform_to_tensor()
  }
)

```

Notice how `tiny_imagenet_dataset()` takes an argument called `transform`\index{\texttt{transform} (torchvision)}. This is used to specify operations to be performed as part of the input pipeline. Here, not much is happening: We just convert images to something we can work with, tensors. However, very soon we'll see this argument used to specify sequences of transformations such as resizing, cropping, rotation, and more.

What remains to be done is create the data loaders.

```{r}
train_dl <- dataloader(train_ds,
  batch_size = 128,
  shuffle = TRUE
)
valid_dl <- dataloader(valid_ds, batch_size = 128)

```

Images are RGB, and of size 64 x 64:

```{r}
batch <- train_dl %>%
  dataloader_make_iter() %>%
  dataloader_next()

dim(batch$x)
```

```         
[1] 128   3  64  64
```

Classes are integers between 1 to 200:

```{r}
batch$y
```

```         
torch_tensor
 172
  17
  76
  78
 111
  57
   8
 166
 146
 114
  41
  28
 138
  98
  57
  98
  25
 148
 166
 135
  31
 182
  48
 184
 160
 166
  40
 115
 161
  21
... [the output was truncated (use n=-1 to disable)]
[ CPULongType{128} ]
```

Now we define a convnet, and train it with `luz`.

#### Image classification from scratch

Here is a prototypical convnet, modeled after our template, but more powerful.

In addition to what we've seen already, the code illustrates a way of modularizing the code, arranging layers into three groups:

-   a (large) feature detector that, as a whole, is shift-equivariant;

-   a shift-invariant pooling layer (`nn_adaptive_avg_pool2d()`) that allows us to specify a desired output resolution; and

-   a feed-forward neural network that takes the computed features and uses them to produce final scores: two hundred values, corresponding to two hundred classes, for each item in the batch.

```{r}
convnet <- nn_module(
  "convnet",
  initialize = function() {
    self$features <- nn_sequential(
      nn_conv2d(3, 64, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_conv2d(64, 128, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_conv2d(128, 256, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_conv2d(256, 512, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_conv2d(512, 1024, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_adaptive_avg_pool2d(c(1, 1))
    )
    self$classifier <- nn_sequential(
      nn_linear(1024, 1024),
      nn_relu(),
      nn_linear(1024, 1024),
      nn_relu(),
      nn_linear(1024, 200)
    )
  },
  forward = function(x) {
    x <- self$features(x)$squeeze()
    x <- self$classifier(x)
    x
  }
)
```

Now, we train the network. The classifier outputs raw logits, not probabilities; this means we need to make use of `nn_cross_entropy_loss()`. We train for fifty epochs:

```{r}
fitted <- luz_load("fitted.rds")
```

```         
fitted <- convnet %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam,
    metrics = list(
      luz_metric_accuracy()
    )
  ) %>%
  fit(train_dl,
      epochs = 50,
      valid_data = valid_dl,
      verbose = TRUE
      )
```

After fifty epochs, this resulted in accuracy values of 0.92 and 0.22, on the training and test sets, respectively. This is quite a difference! On the training set, this model is near-perfect; on the test set, it only gets up to every fourth image correct.

```         
Epoch 1/50
Train metrics: Loss: 5.0822 - Acc: 0.0146                                     
Valid metrics: Loss: 4.8564 - Acc: 0.0269
Epoch 2/50
Train metrics: Loss: 4.5545 - Acc: 0.0571                                     
Valid metrics: Loss: 4.2592 - Acc: 0.0904
Epoch 3/50
Train metrics: Loss: 4.0727 - Acc: 0.1122                                     
Valid metrics: Loss: 3.9097 - Acc: 0.1381
...
...
Epoch 48/50
Train metrics: Loss: 0.3033 - Acc: 0.9064                                     
Valid metrics: Loss: 10.2999 - Acc: 0.2188
Epoch 49/50
Train metrics: Loss: 0.2932 - Acc: 0.9098                                     
Valid metrics: Loss: 10.7348 - Acc: 0.222
Epoch 50/50
Train metrics: Loss: 0.2733 - Acc: 0.9152                                     
Valid metrics: Loss: 10.641 - Acc: 0.2204
```

With two hundred options to choose from, "every fourth" does not even seem so bad; however, looking at the enormous difference between both metrics, something is not quite right. The model has severely *overfitted* to the training set -- memorized the training samples, in other words. Overfitting is not specific to deep learning; it is the nemesis of all of machine learning. We'll consecrate the whole next chapter to this topic.

Before we end, though, let's see how we would use `luz` to obtain predictions:

```{r}
preds <- fitted %>% predict(valid_dl)
```

`predict()` directly returns what is output by the model: two hundred non-normalized scores for each item. That's because the model's last layer is a linear module, with no activation applied. (Remember how the loss function, `nn_cross_entropy_loss()`, applies a *softmax* operation before calculating cross-entropy.)

Now, we could certainly call `nnf_softmax()` ourselves, converting outputs from `predict()` to probabilities:

```{r}
preds <- nnf_softmax(preds, dim = 2)
```

However, if we're just interested in determining the most likely class, we can as well skip the normalization step, and directly pick the highest value for each batch item:

```{r}
torch_argmax(preds, dim = 2)
```

```         
torch_tensor
 144
   5
  22
  84
 190
 103
 186
  12
  39
  43
  61
 108
 140
  21
  43
  44
  24
  81
  79
 158
 169
 171
  50
 130
  67
  46
 103
  55
 180
  35
... [the output was truncated (use n=-1 to disable)]
[ CUDALongType{10000} ]
```

We could now go on to compare predictions with actual classes, looking for inspiration on what could be done better. But at this stage, there is still a *lot* that can be done better! We will return to this application in due time, but first, we need to learn about overfitting, and ways to speed up model training.


# Appendix

## Chapters left to the reader

* 19. Image segmentation
* 20. Tabular data
* 21. Time series
* 22. Audio classification
* 23. Overview
* 24. Matrix computations: Least-squares problems
* 25. Matrix computations: Convolution
* 26. Exploring the Discrete Fourier Transform (DFT)
* 27. The Fast Fourier Transform (FFT)
* 28. Wavelets

## Thank you

* Nathan Stephens
* nstephens@nvidia.com
* [torch-workshop](https://github.com/nwstephens/torch-workshop.git)