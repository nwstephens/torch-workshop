---
title: "torch-workshop"
subtitle: "RMACC 2023 - Arizona State University"
author: "Nathan Stephens"
format: revealjs
smaller: true
logo: nvidia.png
scrollable: true
slide-number: true
eval: false
echo: true
---

# Welcome

## About Me

* Currently Senior Manager for Developer Relations at NVIDIA. 
* Background in analytic solutions and consulting.
* Experience building data science teams, architecting analytic infrastructure, and operationalizing data products. 
* Worked at RStudio, oversaw the solutions engineering team.
* Longtime advocate for operationalizing data science using open-source software.
* Recently relocated to Peoria, AZ.


## Deep Learning and Scientific Computing With R Torch

This workshop presents a conceptual overview of the [R interface for PyTorch](https://torch.mlverse.org/). Emphasis will be on basic structures and applications for deep learning. Materials are based on the new book, [Deep Learning and Scientific Computing with R torch](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/) by [Sigrid Keydana](https://divergences.xyz/) (2023).

<center>![](Keydana.jpg)</center>

## Three Goals

This is a book about `torch`, the R interface to PyTorch. PyTorch, as of this writing, is one of the major deep-learning and scientific-computing frameworks, widely used across industries and areas of research. With `torch`, you get to access its rich functionality directly from R, with no need to install, let alone learn, Python. Though still "young" as a project, `torch` already has a vibrant community of users and developers; the latter not just extending the core framework, but also, building on it in their own packages.

1. The first is a thorough introduction to core `torch`: the basic structures without whom nothing would work.
2. In the second section, basics explained, we proceed to explore various applications of deep learning, ranging from image recognition over time series and tabular data to audio classification. 
3. The third section is special in that it highlights some of the non-deep-learning things you can do with `torch`: matrix computations, calculating the Discrete Fourier Transform, and wavelet analysis.


## Workshop Contents

**Covering first two sections:**

* Getting familiar with torch (chapters 1-11)
* Deep learning with torch (up to chapter 18)

**Reproducing Chapters:**

* Tensors (chapter 3)
* A neural network from scratch (chapter 6)
* Modularizing the neural network (chapter 11)
* Training with luz (chapter 14)
* A first go at image classification - using ImageNet (242MB) (chapter 15)

**Remaining chapters are left to the reader**

# Setup

## CPU Setup (Windows and Mac)

1. Install RStudio desktop
2. Install `torch`

```{r}
install.packages("torch")
```

3. Clone the [torch-workshop](https://github.com/nwstephens/torch-workshop.git) repos and make sure the working directory is `torch-workshop`
4. Download `Tiny ImageNet` data

```{r}
tiny_imagenet_dataset(".", download = TRUE)
```

5. Install the `modeldata` package from the `2021-06-08` snapshot

```{r}
install.packages("modeldata", repos = "https://packagemanager.rstudio.com/cran/__linux__/bionic/2021-06-08")
```

## GPU Accelerated R Setup

* Obtaining an NVIDIA GPU
  * [Rig](https://boxx.com/)
  * [Cloud](https://rapids.ai/#quick-start)
  * [Data Center](https://www.nvidia.com/en-us/data-center/)
* Installing and configuring an NVIDIA GPU
  * [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads)
  * [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
* Installing and configuring R
  * [The Rocker Project](https://rocker-project.org/)
  * [Posit Package Manager](https://packagemanager.rstudio.com/client/#/)
  * [R Binaries](https://github.com/rstudio/r-builds)

## GPU Setup (Linux)

1. Have a CUDA compatible NVIDIA GPU with [compute capability](https://developer.nvidia.com/cuda-gpus#compute) 6.0 or higher
2. Install the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
3. Pull and run the RStudio Rocker container

```{bash}
docker pull rocker/rstudio
docker run --gpus=all -d -t -e PASSWORD=rstudio -p 8787:8787 --name rstudio rocker/rstudio
```

4. Open RStudio Server from your browser by opening `http://<your.ip.address>:8787/`. Make sure port 8787 is open. Your username is `rstudio` and your password is `rstudio`.
5. Configure RStudio Server to download package binaries from the [Posit Package Manager](https://packagemanager.rstudio.com/client/#/)

```
# For Red Hat 8 use:
https://packagemanager.rstudio.com/cran/__linux__/centos8/latest

# For Ubuntu 20.04 use:
https://packagemanager.rstudio.com/cran/__linux__/focal/latest
```
6. Install `torch` from the pre-built binaries (Warning! This download is 2Gb)

```{r}
options(timeout = 600)
install.packages("torch", repos = "https://storage.googleapis.com/torch-lantern-builds/packages/cu117/0.10.0/")
```

7. Make sure your CUDA device is available (this should return `TRUE`)

```{r}
library(torch)
cuda_is_available()
```

8. Clone this repos and make sure the working directory is `torch-workshop`
9. Download `Tiny ImageNet` data

```{bash}
wget http://cs231n.stanford.edu/tiny-imagenet-200.zip
unzip tiny-imagenet-200.zip
```

10. Install the `modeldata` package from the `2021-06-08` snapshot

```{r}
install.packages("modeldata", repos = "https://packagemanager.rstudio.com/cran/__linux__/bionic/2021-06-08")
```

# Getting Familiar with torch

## 2. What is torch?

`torch` is an R port of PyTorch, one of the two (as of this writing) most-employed deep learning frameworks in industry and research. By its design, it is also an excellent tool to use in various types of scientific computation tasks (a subset of which you'll encounter in the book's final part). It is written entirely in R and C++ (including a bit of C). No Python installation is required to use it.

On the Python (PyTorch) side, the ecosystem appears as a set of concentric cycles. In the middle, there's PyTorch\index{PyTorch} itself, the core library without which nothing could work. Surrounding it, we have the inner circle of what could be called framework libraries, dedicated to special types of data (images, sound, text ...), or centered on workflow tasks, like deployment. Then, there is the broader ecosystem of add-ons, specializations, and libraries for whom PyTorch is a building block, or a tool.

On the R side, we have the same "heart" -- all depends on core `torch` -- and we do have the same types of libraries; but the categories, the "circles", appear less clearly set off from each other. There are no strict boundaries. There's just a vibrant community of developers, of diverse origin and with diverse goals, working to further develop and extend `torch`, so it can help more and more people accomplish their various tasks. The ecosystem growing so quickly, I'll refrain from naming individual packages -- at any time, visit [the `torch` website](https://torch.mlverse.org/packages/) to see a featured subset.

There are three packages, though, that I *will* name here, since they are used in the book: `torchvision` , `torchaudio`, and `luz`. The former two bundle domain-specific transformations, deep learning models, datasets, and utilities for images (incl. video) and audio data, respectively. The third is a high-level, intuitive, nice-to-use interface to `torch`, allowing to define, train, and evaluate a neural network in just a few lines of code. Like `torch` itself, all three packages can be installed from CRAN.

## 3. Tensors

### What's in a tensor?

To do anything useful with `torch`, you need to know about tensors. Not tensors in the math/physics sense. In deep learning frameworks such as TensorFlow and (Py-)Torch, *tensors* are "just" multi-dimensional arrays optimized for fast computation -- not on the CPU only but also, on specialized devices such as GPUs and TPUs.

In fact, a `torch` `tensor` is like an R `array`, in that it can be of arbitrary dimensionality. But unlike `array`, it is designed for fast and scalable execution of mathematical calculations, and you can move it to the GPU. (It also has an extra capability of enormous practical impact -- automatic differentiation -- but we reserve that for the next chapter.)

**Exercises:**

1. Creating tensors
2. Operations on tensors\index{tensors!operations on}
3. Accessing parts of a tensor\index{tensors!index into} \index{tensors!slice}
4. Reshaping tensors\index{tensors!reshape}
5. Broadcasting\index{tensors!broadcasting}




## 4. Autograd

Frameworks like `torch` are so popular because of what you can do with them: deep learning, machine learning, optimization, large-scale scientific computation in general. Most of these application areas involve minimizing some *loss function*. This, in turn, entails computing function *derivatives*.

`torch` implements what is called *automatic differentiation*\index{automatic differentiation}. In automatic differentiation, and more specifically, its often-used *reverse-mode* variant, derivatives are computed and combined on a *backward pass* through the graph of tensor operations.

In torch, the AD engine is usually referred to as autograd.

### Automatic differentiation example

Quadratic function of two variables: $f(x_1, x_2) = 0.2 {x_1}^2 + 0.2 {x_2}^2 - 5$. It has its minimum at `(0,0)`.

![](images/autograd-paraboloid.png)

![](images/autograd-compgraph.png)

-   At `x7`, we calculate partial derivatives with respect to `x5` and `x6`. Basically, the equation to differentiate looks like this: $f(x_5, x_6) = x_5 + x_6 - 5$. Thus, both partial derivatives are 1.

-   From `x5`, we move to the left to see how it depends on `x3`. We find that $\frac{\partial x_5}{\partial x_3} = 0.2$. At this point, applying the chain rule of calculus, we already know how the output depends on `x3`: $\frac{\partial f}{\partial x_3} = 0.2 * 1 = 0.2$.

-   From `x3`, we take the final step to `x`. We learn that $\frac{\partial x_3}{\partial x_1} = 2 x_1$. Now, we again apply the chain rule, and are able to formulate how the function depends on its first input: $\frac{\partial f}{\partial x_1} = 2 x_1 * 0.2 * 1 = 0.4 x_1$.

-   Analogously, we determine the second partial derivative, and thus, already have the gradient available: $\nabla f = \frac{\partial f}{\partial x_1} + \frac{\partial f}{\partial x_2} = 0.4 x_1 + 0.4 x_2$.

### Automatic differentiation with `torch` *autograd*

```{r}
library(torch)

x1 <- torch_tensor(2, requires_grad = TRUE)
x2 <- torch_tensor(2, requires_grad = TRUE)

x3 <- x1$square()
x5 <- x3 * 0.2

x4 <- x2$square()
x6 <- x4 * 0.2

x7 <- x5 + x6 - 5
x7
```
    torch_tensor
    -3.4000
    [ CPUFloatType{1} ][ grad_fn = <SubBackward1> ]

```{r}
x7$backward()

x1$grad
x2$grad
```

     0.8000
    [ CPUFloatType{1} ]
    torch_tensor
     0.8000
    [ CPUFloatType{1} ]

These are the partial derivatives of `x7` with respect to `x1` and `x2`, respectively. Conforming to our manual calculations above, both amount to 0.8, that is, 0.4 times the tensor values 2 and 2.



## 5. Function minimization with *autograd* {#sec:optim-1}

In optimization\index{optimization} research, the *Rosenbrock function* is a classic. It is a function of two variables; its minimum is at `(1,1)`. If you take a look at its contours, you see that the minimum lies inside a stretched-out, narrow valley.

![](images/optim-rosenbrock.png)

```{r}
a <- 1
b <- 5

rosenbrock <- function(x) {
  x1 <- x[1]
  x2 <- x[2]
  (a - x1)^2 + b * (x2 - x1^2)^2
}
```

* `lr`, for learning rate, is the fraction of the gradient to subtract on every step
* `num_iterations` is the number of steps to take.

```{r}
library(torch)

num_iterations <- 1000

lr <- 0.01

x <- torch_tensor(c(-1, 1), requires_grad = TRUE)

for (i in 1:num_iterations) {
  if (i %% 100 == 0) cat("Iteration: ", i, "\n")

  value <- rosenbrock(x)
  if (i %% 100 == 0) {
    cat("Value is: ", as.numeric(value), "\n")
  }

  value$backward()
  if (i %% 100 == 0) {
    cat("Gradient is: ", as.matrix(x$grad), "\n")
  }

  with_no_grad({
    x$sub_(lr * x$grad)
    x$grad$zero_()
  })
}
```

    Iteration:  100 
    Value is:  0.3502924 
    Gradient is:  -0.667685 -0.5771312 

    Iteration:  200 
    Value is:  0.07398106 
    Gradient is:  -0.1603189 -0.2532476 

    Iteration:  300 
    Value is:  0.02483024 
    Gradient is:  -0.07679074 -0.1373911 

    Iteration:  400 
    Value is:  0.009619333 
    Gradient is:  -0.04347242 -0.08254051 

    Iteration:  500 
    Value is:  0.003990697 
    Gradient is:  -0.02652063 -0.05206227 

    Iteration:  600 
    Value is:  0.001719962 
    Gradient is:  -0.01683905 -0.03373682 

    Iteration:  700 
    Value is:  0.0007584976 
    Gradient is:  -0.01095017 -0.02221584 

    Iteration:  800 
    Value is:  0.0003393509 
    Gradient is:  -0.007221781 -0.01477957

    Iteration:  900 
    Value is:  0.0001532408 
    Gradient is:  -0.004811743 -0.009894371 

    Iteration:  1000 
    Value is:  6.962555e-05 
    Gradient is:  -0.003222887 -0.006653666 

After thousand iterations, we have reached a function value lower than 0.0001. What is the corresponding `(x1,x2)`-position?

```{r}
x
```

    torch_tensor
     0.9918
     0.9830
    [ CPUFloatType{2} ]

This is rather close to the true minimum of `(1,1)`.

![](images/optim-rosenbrock.png)


## 6. A neural network from scratch {#sec:network-1}

In this chapter, we are going to solve a regression\index{regression} task. But wait -- not the `lm` way. We'll be building a real neural network, making use of tensors only (`autograd`-enabled ones, it goes without saying). Of course, this is not how you'll be using `torch`, later; but this does not make it a useless endeavor. On the contrary. Having seen the raw mechanics, you'll be able to appreciate even more the hard work that `torch` saves you. What's more, understanding the basics will be an efficient antidote against the surprisingly common temptation to think of deep learning as some kind of "magic". It's all just matrix computations; one has to learn how to orchestrate them though.

Let's start with what we need for a network that can perform regression.

### Idea

In a nutshell, a network is a *function* from inputs to outputs. A suitable function, thus, is what we're looking for.

To find it, let's first think of regression as *linear* regression. What linear regression does is multiply and add. For each independent variable, there is a *coefficient* that multiplies it. On top of that, there is a so-called *bias* term that gets added at the end. (In two dimensions, regression coefficient and bias correspond to slope and x-intercept of the regression line.)

Thinking about it, multiplication and addition are things we can do with tensors -- one could even say they are made for exactly that. Let's take an example where the input data consist of a hundred observations, with three features each. For example:

```{r}
library(torch)

x <- torch_randn(100, 3)
x$size()
```

    [1] 100   3

To store the per-feature coefficients that should multiply `x`, we need a column vector of length 3, the number of features. Alternatively, preparing for a modification we're going to make very soon, this can be a matrix whose columns are of length three, that is, a matrix with three rows. How many columns should it have? Let's say we want to predict a single output feature. In that case, the matrix should be of size 3 x 1.

Here comes a suitable candidate, initialized randomly. Note how the tensor is created with `requires_grad = TRUE`, as it represents a parameter we'll want the network to *learn*.

```{r}
w <- torch_randn(3, 1, requires_grad = TRUE)
```

The bias tensor then has to be of size 1 x 1:

```{r}
b <- torch_zeros(1, 1, requires_grad = TRUE)
```

Now, we can get a "prediction" by multiplying the data with the weight\index{weight} matrix `w` and adding the bias\index{bias} `b`:

```{r}
y <- x$matmul(w) + b
print(y, n = 10)
```

    torch_tensor
    -2.1600
    -3.3244
     0.6046
     0.4472
    -0.4971
    -0.0530
     5.1259
    -1.1595
    -0.5960
    -1.4584
    ... [the output was truncated (use n=-1 to disable)]
    [ CPUFloatType{100,1} ][ grad_fn = <AddBackward0> ]

In math notation, what we've done here is implement the function:

$$
f(\mathbf{X}) = \mathbf{X}\mathbf{W} + \mathbf{b}
$$

How does this relate to neural networks?

### Layers

Circling back to neural-network terminology, what we've done here is prototype the action of a network that has a *single* *layer*\index{layer}: the output layer. However, a single-layer network is hardly the type you'd be interested in building -- why would you, when you could simply do linear regression instead? In fact, one of the defining features of neural networks is their ability to chain an unlimited (in theory) number of layers. Of these, all but the output\index{layer!output} layer may be referred to as "hidden" layers, although from the point of view of someone who uses a deep learning framework such as `torch`, they are not that *hidden* after all.

Let's say we want our network to have one hidden layer\index{layer!hidden}. Its size, meaning, the number of *units* it has, will be an important factor in determining the network's power. This number is reflected in the weight matrix we create: A layer with eight units will need a weight matrix with eight columns.

```{r}
w1 <- torch_randn(3, 8, requires_grad = TRUE)
```

Each unit has its own value for bias, too.

```{r}
b1 <- torch_zeros(1, 8, requires_grad = TRUE)
```

Just like we saw before, the hidden layer will multiply the input it receives by the weights and add the bias. That is, it applies the function $f$ displayed above. Then, another function is applied. This function receives its input from the hidden layer and produces the final output. In a nutshell, what is happening here is function composition: Calling the second function $g$, the overall transformation is $g(f(\mathbf{X})$, or $g \circ f$.

For $g$ to yield an output analogous to the single-layer architecture above, its weight matrix has to take the eight-column hidden layer to a single column. That is, `w2` looks like this:

```{r}
w2 <- torch_randn(8, 1, requires_grad = TRUE)
```

The bias, `b2`, is a single value, like `b1`:

```{r}
b2 <- torch_randn(1, 1, requires_grad = TRUE)
```

Of course, there is no reason to stop at *one* hidden layer, and once we've built up the complete apparatus, please feel invited to experiment with the code. But first, we need to add in a few other types of components. For one, with our most recent architecture, what we're doing is chain, or compose, functions -- which is good. But all these functions are doing is add and multiply, implying that they are linear. The power of neural networks, however, is usually associated with *nonlinearity*\index{nonlinearity}. Why?

### Activation functions\index{activation function}

Imagine, for a moment, that we had a network with three layers, and all each layer did was multiply its input by its weight matrix. (Having a bias term doesn't really change anything. But it makes the example more complex, so we're "abstracting it out".)

This gives us a chain of matrix multiplications: $f(\mathbf{X}) = ((\mathbf{X} \mathbf{W}_1)\mathbf{W}_2)\mathbf{W}_3$. Now, this can be rearranged so that all the weight matrices are multiplied together before application to $\mathbf{X}$: $f(\mathbf{X}) = \mathbf{X} (\mathbf{W}_1\mathbf{W}_2\mathbf{W}_3)$. Thus, this three-layer network can be simplified to a single-layer one, where $f(\mathbf{X}) = \mathbf{X} \mathbf{W}_4$. And now, we have lost all advantages associated with deep neural networks.

This is where activation functions, sometimes called "nonlinearities", come in. They introduce non-linear operations that cannot be modeled by matrix multiplication. Historically, the prototypical activation function has been the *sigmoid*\index{activation!sigmoid}, and it's still extremely important today. Its constitutive action is to squish its input between zero and one, yielding a value that can be interpreted as a probability. But in regression, this is not usually what we want, and neither would it be for most hidden layers.

Instead, the most-used activation function inside a network is the so-called *ReLU*\index{activation!ReLU}, or Rectified Linear Unit. This is a long name for something rather straightforward: All negative values are set to zero. In `torch`, this can be accomplished using the `relu()` function:

```{r}
t <- torch_tensor(c(-2, 1, 5, -7))
t$relu()
```

    torch_tensor
     0
     1
     5
     0
    [ CPUFloatType{4} ]

Why would this be nonlinear? One criterion for a linear function is that when you have two inputs, it doesn't matter if you first add them and then, apply the transformation, or if you start by applying the transformation independently to both inputs and then, go ahead and add them. But with ReLU, this does not work:

```{r}
t1 <- torch_tensor(c(1, 2, 3))
t2 <- torch_tensor(c(1, -2, 3))

t1$add(t2)$relu()
```

    torch_tensor
     2
     0
     6
    [ CPUFloatType{3} ]

```{r}
t1_clamped <- t1$relu()
t2_clamped <- t2$relu()

t1_clamped$add(t2_clamped)
```

    torch_tensor
     2
     2
     6
    [ CPUFloatType{3} ]

The results are not the same.

Wrapping up so far, we've talked about how to code layers and activation functions. There is just one further concept to discuss before we can build the complete network. This is the loss function.

### Loss functions

Put abstractly, the loss is a measure of how far away we are from our goal. When minimizing a function, like we did in the previous chapter, this is the difference between the current function value and the smallest value it can take. With neural networks, we are free to choose a suitable loss function as we like, provided it matches our task. For regression-type tasks, this often will be mean squared error (MSE), although it doesn't have to be. For example, there could be reasons to use mean absolute error instead.

In `torch`, computation of mean squared error is a one-liner:

```{r}
y <- torch_randn(5)
y_pred <- y + 0.01

loss <- (y_pred - y)$pow(2)$mean()

loss
```

    torch_tensor
    9.99999e-05
    [ CPUFloatType{} ]

As soon as we have the loss, we'll be able to update the weights, subtracting a fraction of its gradient. We've already seen how to do this in the last chapter, and will see it again shortly.

We now take the pieces discussed and put them together.

### Implementation

We split this into three parts. This way, when later we refactor individual components to make use of higher-level `torch` functionality, it will be easier to see the areas where encapsulation and modularization are occurring.

#### Generate random data

Our example data consist of one hundred observations. The input, `x`, has three features; the target, `y`, just one. `y` is generated from `x`, but with some noise added.

```{r}
library(torch)

## input dimensionality (number of input features)
d_in <- 3
## number of observations in training set
n <- 100

x <- torch_randn(n, d_in)
coefs <- c(0.2, -1.3, -0.5)
y <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)

```

Next, the network.

#### Build the network\index{network}

The network has two layers: a hidden layer and the output layer. This means that we need two weight matrices and two bias tensors. For no special reason, the hidden layer here has thirty-two units:

```{r}
# dimensionality of hidden layer
d_hidden <- 32
# output dimensionality (number of predicted features)
d_out <- 1

# weights connecting input to hidden layer
w1 <- torch_randn(d_in, d_hidden, requires_grad = TRUE)
# weights connecting hidden to output layer
w2 <- torch_randn(d_hidden, d_out, requires_grad = TRUE)

# hidden layer bias
b1 <- torch_zeros(1, d_hidden, requires_grad = TRUE)
# output layer bias
b2 <- torch_zeros(1, d_out, requires_grad = TRUE)
```

With their current values -- results of random initialization -- those weights and biases won't be of much use. Time to train the network.

#### Train the network

Training the network means passing the input through its layers, calculating the loss, and adjusting the parameters\index{parameters} (weights and biases) in a way that predictions improve. These activities we keep repeating until performance seems sufficient (which, in real-life applications, would have to be defined very carefully). Technically, each repeated application of these steps is called an *epoch*\index{epoch}.

Just like with function minimization, deciding on a suitable learning rate (the fraction of the gradient to subtract) needs some experimentation.

Looking at the below training loop, you see that, logically, it consists of four parts:

-   do a forward pass\index{forward pass}, yielding the network's predictions (if you dislike the one-liner, feel free to split it up);

-   compute the loss (this, too, being a one-liner -- we merely added some logging);

-   have *autograd* calculate the gradient of the loss with respect to the parameters; and

-   update the parameters accordingly (again, taking care to wrap the whole action in `with_no_grad()`, and zeroing the `grad` fields on every iteration).

```{r}

learning_rate <- 1e-4

### training loop ----------------------------------------

for (t in 1:200) {
  
  ### -------- Forward pass --------
  
  y_pred <- x$mm(w1)$add(b1)$relu()$mm(w2)$add(b2)
  
  ### -------- Compute loss -------- 
  loss <- (y_pred - y)$pow(2)$mean()
  if (t %% 10 == 0)
    cat("Epoch: ", t, "   Loss: ", loss$item(), "\n")
  
  ### -------- Backpropagation --------
  
  # compute gradient of loss w.r.t. all tensors with
  # requires_grad = TRUE
  loss$backward()
  
  ### -------- Update weights -------- 
  
  # Wrap in with_no_grad() because this is a part we don't 
  # want to record for automatic gradient computation
   with_no_grad({
     w1 <- w1$sub_(learning_rate * w1$grad)
     w2 <- w2$sub_(learning_rate * w2$grad)
     b1 <- b1$sub_(learning_rate * b1$grad)
     b2 <- b2$sub_(learning_rate * b2$grad)  
     
     # Zero gradients after every pass, as they'd
     # accumulate otherwise
     w1$grad$zero_()
     w2$grad$zero_()
     b1$grad$zero_()
     b2$grad$zero_()  
   })

}
```

    Epoch: 10 Loss: 24.92771
    Epoch: 20 Loss: 23.56143
    Epoch: 30 Loss: 22.3069
    Epoch: 40 Loss: 21.14102
    Epoch: 50 Loss: 20.05027
    Epoch: 60 Loss: 19.02925
    Epoch: 70 Loss: 18.07328
    Epoch: 80 Loss: 17.16819
    Epoch: 90 Loss: 16.31367
    Epoch: 100 Loss: 15.51261
    Epoch: 110 Loss: 14.76012
    Epoch: 120 Loss: 14.05348
    Epoch: 130 Loss: 13.38944
    Epoch: 140 Loss: 12.77219
    Epoch: 150 Loss: 12.19302
    Epoch: 160 Loss: 11.64823
    Epoch: 170 Loss: 11.13535
    Epoch: 180 Loss: 10.65219
    Epoch: 190 Loss: 10.19666
    Epoch: 200 Loss: 9.766989

The loss decreases quickly at first, and then, not so rapidly anymore. But this example was not created to exhibit magnificent performance; the idea was to show how few lines of code are needed to build a "real" neural network.

Now, the layers, the loss, the parameter updates -- all that is still pretty "raw": It's (literally) *just tensors*. For such a small network this works fine, but it would get cumbersome pretty fast for more complex designs. The following two chapters, thus, will show how to abstract away weights and biases into neural network *modules*, swap self-made loss functions with built-in ones, and get rid of the verbose parameter update routine.



## 7. Modules {#sec:modules}

In the last chapter, we built a neural network for a regression task. There were two distinct types of operations: linear and non-linear.

In the non-linear category, we had ReLU activation, expressed as a straightforward function call: `nnf_relu()`. Activation functions are *functions*: Given input $\mathbf{x}$, they return output $\mathbf{y}$ every time. In other words, they are deterministic. It's different with the linear part, though.

The linear part in the regression network was implemented as multiplication by a matrix -- the weight matrix -- and addition of a vector (the bias vector). With operations like that, results inevitably depend on the actual values stored in the respective tensors. Put differently, the operation is *stateful*.

Whenever there is state involved, it helps to encapsulate it in an object, freeing the user from manual management. This is what `torch`'s *modules* do.

Note that term, *modules*\index{module (terminology)}. In `torch`, a module can be of any complexity, ranging from basic *layers* -- like the `nn_linear()` we are going to introduce in a minute -- to complete *models* consisting of many such layers. Code-wise, there is no difference between "layers"\index{layer (terminology)} and "models"\index{model (terminology)}. This is why in some texts, you'll see "module" used throughout. In this book, I'll mostly stay with the common terminology of layers and models, as it maps more closely to how things appear conceptually.

Back to the *why* of modules. In addition to encapsulation, there is another reason for providing layer objects: Not all often-used layers are as light-weight as `nn_linear()` is. We'll quickly mention a few others at the end of the next section, reserving a complete introduction to later chapters of this book.

### Built-in `nn_module()`s

In `torch`, a linear layer is created using `nn_linear()`. `nn_linear()` expects (at least) two arguments: `in_features` and `out_features`. Let's say your input data has fifty observations with five features each; that is, it is of size 50 x 5. You want to build a hidden layer with sixteen units. Then `in_features` is 5, and `out_features` is 16. (The same 5 and 16 would constitute the number of rows/columns in the weight matrix if you built one yourself.)

```{r}
library(torch)
l <- nn_linear(in_features = 5, out_features = 16)
```

Once created, the module readily informs you about its parameters:

```{r}
l
```

    An `nn_module` containing 96 parameters.
    Parameters
     weight: Float [1:16, 1:5]
     bias: Float [1:16]

Encapsulation doesn't keep us from inspecting the weight and bias tensors:

```{r}
l$weight
```

    torch_tensor
    -0.2079 -0.1920  0.2926  0.0036 -0.0897
     0.3658  0.0076 -0.0671  0.3981 -0.4215
     0.2568  0.3648 -0.0374 -0.2778 -0.1662
     0.4444  0.3851 -0.1225  0.1678 -0.3443
    -0.3998  0.0207 -0.0767  0.4323  0.1653
     0.3997  0.0647 -0.2823 -0.1639 -0.0225
     0.0479  0.0207 -0.3426 -0.1567  0.2830
     0.0925 -0.4324  0.0448 -0.0039  0.1531
    -0.2924 -0.0009 -0.1841  0.2028  0.1586
    -0.3064 -0.4006 -0.0553 -0.0067  0.2575
    -0.0472  0.1238 -0.3583  0.4426 -0.0269
    -0.0275 -0.0295 -0.2687  0.2236  0.3787
    -0.2617 -0.2221  0.1503 -0.0627  0.1094
     0.0122  0.2041  0.4466  0.4112  0.4168
    -0.4362 -0.3390  0.3679 -0.3045  0.1358
     0.2979  0.0023  0.0695 -0.1906 -0.1526
    [ CPUFloatType{16,5} ]

```{r}
l$bias
```

    torch_tensor
    -0.2314
     0.2942
     0.0567
    -0.1728
    -0.3220
    -0.1553
    -0.4149
    -0.2103
    -0.1769
     0.4219
    -0.3368
     0.0689
     0.3625
    -0.1391
    -0.1411
    -0.2014
    [ CPUFloatType{16} ]

At this point, I need to ask for your indulgence. You've probably noticed that `torch` reports the weight matrix as being of size 16 x 5, not 5 x 16, like we said you'd create it when coding from scratch. This is due to an implementation detail inherited from the underlying C++ implementation, `libtorch`. For performance reasons, `libtorch`'s linear module stores the weight and bias tensors in *transposed* form. On the R side, all we can do is explicitly point you to it and thereby, hopefully, alleviate the confusion.

Let's go on. To apply this module to input data, just "call" it like a function:

```{r}
x <- torch_randn(50, 5)
output <- l(x)
output$size()
```

    [1] 50 16

So that's the forward pass. How about gradient computation? Previously, when creating a tensor we wanted to figure as a "source" in gradient computation, we had to let `torch` know explicitly, passing `requires_grad = TRUE`. No such thing is required for built-in `nn_module()`s. We can immediately check that `output` knows what to do on `backward()`:

```{r}
output$grad_fn
```

    AddmmBackward0

To be sure though, let's calculate some "dummy" loss based on `output`, and call `backward()`. We see that now, the linear module's `weight` tensor has its `grad` field populated:

```{r}
loss <- output$mean()
loss$backward()
l$weight$grad
```

    torch_tensor
    0.01 *
    -0.3064  2.4118 -0.6095  0.3419 -1.6131
     -0.3064  2.4118 -0.6095  0.3419 -1.6131
     -0.3064  2.4118 -0.6095  0.3419 -1.6131
     -0.3064  2.4118 -0.6095  0.3419 -1.6131
     -0.3064  2.4118 -0.6095  0.3419 -1.6131
     -0.3064  2.4118 -0.6095  0.3419 -1.6131
     -0.3064  2.4118 -0.6095  0.3419 -1.6131
     -0.3064  2.4118 -0.6095  0.3419 -1.6131
     -0.3064  2.4118 -0.6095  0.3419 -1.6131
     -0.3064  2.4118 -0.6095  0.3419 -1.6131
     -0.3064  2.4118 -0.6095  0.3419 -1.6131
     -0.3064  2.4118 -0.6095  0.3419 -1.6131
     -0.3064  2.4118 -0.6095  0.3419 -1.6131
     -0.3064  2.4118 -0.6095  0.3419 -1.6131
     -0.3064  2.4118 -0.6095  0.3419 -1.6131
     -0.3064  2.4118 -0.6095  0.3419 -1.6131
    [ CPUFloatType{16,5} ]

Thus, once you work with `nn_module`s, `torch` automatically assumes that you'll want gradients computed.

`nn_linear()`, straightforward though it may be, is an essential building block encountered in most every model architecture. Others include:

-   `nn_conv1d()`, `nn_conv2d(), and nn_conv3d()`, the so-called *convolutional* layers that apply filters to input data of varying dimensionality,

-   `nn_lstm()` and `nn_gru()` , the *recurrent* layers that carry through a state,

-   `nn_embedding()` that is used to embed categorical data in high-dimensional space,

-   and more.

### Building up a model

The built-in `nn_module()`s give us *layers*, in usual speak. How do we combine those into *models*? Using the "factory function" `nn_module()`, we can define models of arbitrary complexity. But we may not always need to go that way.

#### Models as sequences of layers: `nn_sequential()`index{`nn_sequential()`}

If all our model should do is propagate straight through the layers, we can use `nn_sequential()` to build it. Models consisting of all linear layers are known as *Multi-Layer Perceptrons*index{Multi-Layer Perceptron (MLP)} (MLPs). Here is one:

```{r}
mlp <- nn_sequential(
  nn_linear(10, 32),
  nn_relu(),
  nn_linear(32, 64),
  nn_relu(),
  nn_linear(64, 1)
)
```

Take a close look at the layers involved. We've already seen `nnf_relu()`, the *function* that implements ReLU activation. (The `f` in `nnf_` stands for functional.) Below, `nn_relu`, like `nn_linear()`, is a module, that is, an object. This is because `nn_sequential()` expects all its arguments to be modules.

Just like the built-in modules, you can apply this model to data by just *calling* it:

```{r}
mlp(torch_randn(5, 10))
```

    torch_tensor
    0.01 *
    -7.8097
     -9.0363
     -38.3282
      5.3959
     -16.4837
    [ CPUFloatType{5,1} ][ grad_fn = <AddmmBackward0> ]

The single call triggered a complete forward pass through the network. Analogously, calling `backward()` will back-propagate\index{backpropagation} through all the layers.

What if you need the model to chain execution steps in a non-sequential way?

#### Models with custom logic

As already hinted at, this is where you use `nn_module()`.

`nn_module()` creates constructors for custom-made R6 objects. Below, `my_linear()` is such a constructor. When called, it will return a linear module similar to the built-in `nn_linear()`.

Two methods should be implemented in defining a constructor: `initialize()` and `forward()`. `initialize()` creates the module object's fields, that is, the objects or values it "owns" and can access from inside any of its methods. `forward()` defines what should happen when the module is called on the input:

```{r}
my_linear <- nn_module(
  initialize = function(in_features, out_features) {
    self$w <- nn_parameter(torch_randn(
      in_features, out_features
    ))
    self$b <- nn_parameter(torch_zeros(out_features))
  },
  forward = function(input) {
    input$mm(self$w) + self$b
  }
)
```

Note the use of `nn_parameter()`. `nn_parameter()` makes sure that the passed-in tensor is registered as a module *parameter*, and thus, is subject to backpropagation by default.

To instantiate the newly-defined module, call its constructor:

```{r}
l <- my_linear(7, 1)
l
```

    An `nn_module` containing 8 parameters.

    Parameters ────────────────────────────────────────────────────────────────────────────────────────────
    ● w: Float [1:7, 1:1]
    ● b: Float [1:1]

Granted, in this example, there really is no *custom logic* we needed to define our own module for. But here, you have a template applicable to any use case. Later, we'll see definitions of `initialize()` and `forward()` that are more complex, and we'll encounter additional methods defined on modules. But the basic mechanism will remain the same.

At this point, you may feel like you'd like to rewrite last chapter's neural network using modules. Feel free to do so! Or maybe wait until, in the next chapter, we'll have learned about *optimizer*s, and built-in loss functions. Once we're done, we'll return to our two examples, function minimization and the regression network. Then, we'll be removing all do-it-yourself pieces rendered superfluous by `torch`.



## 8. Optimizers {#sec:optimizers}

By now, we've gone into quite some detail on tensors, automatic differentiation, and modules. In this chapter, we look into the final major concept present in core `torch`: *optimizers*\index{optimizers}. Where modules encapsulate layer and model logic, optimizers do the same for optimization strategies.

Let's start by pondering why having optimizer objects is so useful.

### Why optimizers?

To this question, there are two main types of answer. First, the technical one.

If you look back at how we coded our first neural network, you'll see that we proceeded like this:

-   compute predictions (forward pass),

-   calculate the loss,

-   have *autograd* compute partial derivatives (calling `loss$backward()`), and

-   update the parameters, subtracting from each some fraction of the gradient.

Here is how that last part looked:

```{r}
library(torch)
```

```
# compute gradient of loss w.r.t. all tensors with
# requires_grad = TRUE
loss$backward()
  
### -------- Update weights -------- 
  
# Wrap in with_no_grad() because this is a part we don't 
# want to record for automatic gradient computation
with_no_grad({
  w1 <- w1$sub_(learning_rate * w1$grad)
  w2 <- w2$sub_(learning_rate * w2$grad)
  b1 <- b1$sub_(learning_rate * b1$grad)
  b2 <- b2$sub_(learning_rate * b2$grad)  
     
  # Zero gradients after every pass, as they'd accumulate
  # otherwise
  w1$grad$zero_()
  w2$grad$zero_()
  b1$grad$zero_()
  b2$grad$zero_()  
})
```

Now this was a small network -- imagine having to code such logic for architectures with tens or hundreds of layers! Surely this can't be what developers of a deep learning framework want their users to do. Accordingly, weight updates are taken care of by specialized objects -- the optimizers in question.

Thus, the technical type of answer concerns usability and convenience. But more is involved. With the above approach, there's hardly a way to find a good learning rate other than by trial and error. And most probably, there is not even an optimal learning rate that would be constant over the whole training process. Fortunately, a rich tradition of research has turned up at set of proven update strategies. These strategies commonly involve a *state* kept between operations. This is another reason why, just like modules, optimizers are objects in `torch`.

Before we look deeper at these strategies, let's see how we'd replace the above manual weight-updating process with a version that uses an optimizer.

### Using built-in `torch` optimizers

An optimizer needs to know what it's supposed to optimize. In the context of a neural network model, this will be the network's parameters. With no real difference between "model modules" and "layer modules", however, we can demonstrate how it works using a single built-in module such as `nn_linear()`.

Here we instantiate a gradient descent optimizer\index{gradient descent (optimizer)} designed to work on some linear module's parameters:

```{r}
l <- nn_linear(10, 2)

opt <- optim_sgd(l$parameters, lr = 0.1)
```

In addition to the always-required reference to what tensors should be optimized, `optim_sgd()` has just a single non-optional parameter: `lr`, the learning rate.

Once we have an optimizer object, parameter updates are triggered by calling its `step()` method. One thing remains unchanged, though. We still need to make sure gradients are not accumulated over training iterations. This means we still call `zero_grad()` -- but this time, on the optimizer object.

This is the complete code replacing the above manual procedure:

```
# compute gradient of loss w.r.t. all tensors with
# requires_grad = TRUE
# no change here
loss$backward()

# Still need to zero out gradients before the backward pass,
# only this time, on the optimizer object
optimizer$zero_grad()

# use the optimizer to update model parameters
optimizer$step()
```

I'm sure you'll agree that usability-wise, this is an enormous improvement. Now, let's get back to our original question -- why optimizers? -- and talk more about the second, strategic part of the answer.

### Parameter update strategies

Searching for a good learning rate by trial and error is costly. And the learning rate isn't even the only thing we're uncertain about. All it does is specify how big of a step to take. However, that's not the only unresolved question.

So far, we've always assumed that the direction of steepest descent, as given by the gradient, is the best way to go. This is not always the case, though. So we are left with uncertainties regarding both magnitude and direction of parameter updates.

Fortunately, over the last decade, there has been significant progress in research related to weight updating in neural networks. Here, we take a look at major considerations involved, and situate in context some of the most popular optimizers provided by `torch`.

The baseline to compare against is *gradient descent*, or *steepest descent*, the algorithm we've been using in our manual implementations of function minimization and neural-network training. Let's quickly recall the guiding principle behind it.

#### Gradient descent (a.k.a. steepest descent, a.k.a. stochastic gradient descent (SGD))\index{stochastic gradient descent (optimizer)}

The gradient -- the vector of partial derivatives, one for each input feature -- indicates the direction in which a function increases most. Going in the opposite direction means we descend the fastest way possible. Or does it?

Unfortunately, it is not that simple. It depends on the landscape that surrounds us, or put more technically, the contours of the function we want to minimize. To illustrate, compare two situations.

The first is the one we encountered when first learning about automatic differentiation. The example there was a quadratic function in two dimensions. We didn't make a great deal out of it at the time, but an important point about this specific function was that the slope was the same in both dimensions. Under such conditions, steepest descent is optimal.

Let's verify that. The function was : $f(x_1, x_2) = 0.2 {x_1}^2 + 0.2 {x_2}^2 - 5$, and its gradient, $\begin{bmatrix}0.4\\0.4 \end{bmatrix}$. Now say we're at point $(x1, x2) = (6,6)$. For each coordinate, we subtract 0.4 times its current value. Or rather, that would be if we had to use a learning rate of 1. But we don't have to. If we pick a learning rate of 2.5, we can arrive at the minimum in a single step: $(x_1, x_2) = (6 - 2.5*0.4*6, 6 - 2.5*0.4*6) = (0,0)$. See below for an illustration of what happens in each case (@fig-optimizers-steepest-descent-symmetric).

![Steepest descent on an isotropic paraboloid, using different learning rates.](images/optimizers-steepest-descent-symmetric.png){#fig-optimizers-steepest-descent-symmetric fig-alt="An isotropic paraboloid (one that has the same curvature in all dimensions), and two optimization paths. Both use the steepest-descent algorithm, but differ in learning rate. One needs many steps to arrive at the function's minimum, while the other gets there in a single step."}

In a nutshell, thus, with a isotropic function like this -- the variance being the same in both directions -- it is "just" a matter of getting the learning rate right.

Now compare this to what happens if slopes in both directions are decidedly distinct.

This time, the coefficient for $x_2$ is ten times as big as that for $x_1$: We have $f(x_1, x_2) = 0.2 {x_1}^2 + 2 {x_2}^2 - 5$. This means that as we progress in the $x_2$ direction, the function value increases sharply, while in the $x_1$ direction, it rises much more slowly. Thus, during gradient descent, we make far greater progress in one direction than the other.

Again, we investigate what happens for different learning rates\index{learning rate}. Below, we contrast three different settings. With the lowest learning rate, the process eventually reaches the minimum, but a lot more slowly than in the symmetric case. With a learning rate just slightly higher, descent gets lost in endless zig-zagging, oscillating between positive and negative values of the more influential variable, $x_2$. Finally, a learning rate that, again, is just minimally higher, has a catastrophic effect: The function value explodes, zig-zagging up right to infinity (@fig-optimizers-steepest-descent-elliptic).

![Steepest descent on a non-isotropic paraboloid, using (minimally!) different learning rates.](images/optimizers-steepest-descent-elliptic.png){#fig-optimizers-steepest-descent-elliptic fig-alt="A non-isotropic paraboloid, stretched-out widely along the x-axis, but with y-values centered sharply around y = 0. Displayed are three optimization paths, all using steepest descent, but varying in learning rate. One of them reaches the minimum after a high number of steps; the second zig-zags along the y-axis, making just minimal progress along the x-axis; the third zig-zags off to infinity."}

This should be pretty convincing -- even with a pretty conventional function of just two variables, steepest descent is far from being a panacea! And in deep learning, loss functions will be a *lot* less well-behaved. This is where the need for more sophisticated algorithms arises: Enter -- again -- optimizers.

#### Things that matter

Viewed conceptually, major modifications to steepest descent can be categorized by the considerations that drive them, or equivalently, by the problems they're trying to solve. Here, we focus on three such considerations.

First, instead of starting in a completely new direction every time we re-compute the gradient, we might want to keep a bit of the old direction -- keep momentum, to use the technical term. This should help avoiding the inefficient zig-zagging seen in the example above.

Second, looking back at just that example of minimizing a non-symmetric function ... Why, really, should we be constrained to using the same learning rate for all variables? When it's evident that all variables don't vary to the same degree, why don't we update them in individually appropriate ways?

Third -- and this is a fix for problems that only arise once you've taken actions to reduce the learning rate for overly-impactful features -- you also want to make sure that learning still progresses, that parameters still get updated.

These considerations are nicely illustrated by a few classics among the optimization algorithms.

#### Staying on track: Gradient descent with momentum\index{momentum (optimizer)}

In gradient descent with momentum, we don't *directly* use the gradient to update the weights. Instead, you can picture weight updates as particles moving on a trajectory: They want to keep going in whatever direction they're going -- keep their *momentum*, in physics speak -- but get continually deflected by collisions. These "collisions" are friendly nudges to, please, keep into account the gradient at the *now current* position. These dynamics result in a two-step update logic.

In the below formulas, the choice of symbols reflects the physical analogy. $\mathbf{x}$ is the position, "where we're at" in parameter space -- or more simply, the current values of the parameters. Time evolution is captured by superscripts, with $\mathbf{y}^{(k)}$ representing the state of variable $\mathbf{y}$ at the current time, $k$. The instantaneous velocity at time $k$ is just what is measured by the gradient, $\mathbf{g}^{(k)}$. But in updating position, we won't directly make use of it. Instead, at each iteration, the update velocity is a combination of old velocity -- weighted by *momentum* parameter $m$ -- and the freshly-computed gradient (weighted by the learning rate). Step one of the two-step logic captures this strategy:

$$
\mathbf{v}^{(k+1)} = m \ \mathbf{v}^{(k)} + lr \ \mathbf{g}^{(k)} 
$$ {#eq-optimizers-1}

The second step then is the update of $\mathbf{x}$ due to this "compromise" velocity $\mathbf{v}$.

$$
\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - \mathbf{v}^{(k+1)}
$$ {#eq-optimizers-2}

Besides the physics analogy, there is another one you may find useful, one that makes use of a concept prominent in time series analysis. If we choose $m$ and $lr$ such that they add up to 1, the result is an *exponentially weighted moving average*. (While this conceptualization, I think, helps understanding, in practice there is no necessity to have $m$ and $lr$ summing to 1, though).

Now, let's return to the non-isotropic paraboloid, and compare SGD with and without momentum. For the latter (bright curve), I'm using a combination of $lr = 0.5$ and $mu = 0.1$. For SGD -- dark curve -- the learning rate is the "good one" from the figure above.Definitely, SGD with momentum requires far fewer steps to reach the minimum (@fig-optimizers-momentum).

![SGD with momentum (white), compared with vanilla SGD (gray).](images/optimizers-momentum.png){#fig-optimizers-momentum fig-alt="A non-isotropic paraboloid, stretched-out widely along the x-axis, but with y-values centered sharply around y = 0. Displayed are two optimization paths, one using steepest descent, one using gradient descent with momentum. With steepest descent, many steps are needed to arrive at the minimum, while gradient descent with momentum needs far fewer steps."}

#### Adagrad\index{Adagrad (optimizer)}

Can we do better yet? Now, we know that in our running example, it is really the fact that one feature changes much faster than the other that slows down optimization. Having separate learning rates per parameter thus clearly seems like a thing we want. In fact, most of the optimizers popular in deep learning have per-parameter learning rates. But how would you actually determine those?

This is where different algorithms differ. Adagrad, for example, divides each parameter update by the cumulative sum of its partial derivatives (squared, to be precise), where "cumulative" means we're keeping track of them since the very first iteration. If we call that "accumulator variable" $s$, refer to the parameter in question by $i$, and count iterations using $k$, this gives us the following formula for keeping $s$ updated:

$$
s_i^{(k)} = \sum_{j=1}^k (g_i^{(j)})^2
$$ {#eq-optimizers-3}

(By the way, feel free to skip over the formulas if you don't like them. I'm doing my best to communicate what they do in words, so you shouldn't miss out on essential information.)

Now, the update rule for each parameter subtracts a portion of the gradient, as did vanilla steepest descent -- but this time, that portion is determined not just by the (global) learning rate, but also, by the aforementioned cumulative sum of squared partials. The bigger that sum -- that is, the bigger the gradients have been during training -- the smaller the adjustment:[^optimizers-1]

[^optimizers-1]: Here $\epsilon$ is just a tiny value added to avoid division by zero.

$$
x_i^{(k+1)} = x_i^{(k)} - \frac{lr}{\epsilon + \sqrt{s_i^{(k)}}}\ g_i^{(k)}\\
$$ {#eq-optimizers-4}

The net effect of this strategy is that, if a parameter has consistently high gradients, its influence is played down. Parameters with, habitually, tiny gradients, on the other hand, can be sure to receive a lot of attention once that changes.

With this algorithm, the global learning rate, $lr$, is of lesser importance. In our running example, it turns out that for best results, we can (and should) use a very high learning rate: 3.7! Here (@fig-optimizers-adagrad) is the result, again comparing with vanilla gradient descent (gray curve):

![Adagrad (white), compared with vanilla SGD (gray).](images/optimizers-adagrad.png){#fig-optimizers-adagrad fig-alt="A non-isotropic paraboloid, stretched-out widely along the x-axis, but with y-values centered sharply around y = 0. Displayed are two optimization paths, one using steepest descent, one using the Adagrad algorithm. With steepest descent, many steps are needed to arrive at the minimum, while Adagrad needs just four steps."}

In our example, thus, Adagrad performs excellently. But in training a neural network, we tend to run *a lot* of iterations. Then, with the way gradients are accumulated, the effective learning rate decreases more and more, and a dead end is reached.

Are there other ways to have individual, per-parameter learning rates?

#### RMSProp\index{RMSProp (optimizer)}

RMSProp replaces the cumulative-gradient strategy found in Adagrad with a weighted-average one. At each point, the "bookkeeping", per-parameter variable $s_i$ is a weighted average of its previous value and the previous (squared) gradient:

$$
s_i^{(k+1)} = \gamma \ s_i^{(k)} + (1-\gamma) \ (g_i^{(k)})^2
$$ {#eq-optimizers-5}

The update then looks as with Adagrad:

$$
x_i^{(k+1)} = x_i^{(k)} - \frac{lr}{\epsilon + \sqrt{s_i^{(k)}}}\ g_i^{(k)}\\
$$ {#eq-optimizers-6}

In this way, each parameter update gets weighted appropriately, without learning slowing down overall.

Here is the result, again compared against the SGD baseline (@fig-optimizers-rmsprop):

![RMSProp (white), compared with vanilla SGD (gray).](images/optimizers-rmsprop.png){#fig-optimizers-rmsprop fig-alt="A non-isotropic paraboloid, stretched-out widely along the x-axis, but with y-values centered sharply around y = 0. Displayed are two optimization paths, one using steepest descent, one using RMSProp. With steepest descent, many steps are needed to arrive at the minimum, while RMSProp needs just four steps."}

As of today, RMSProp is one of the most-often used optimizers in deep learning, with probably just Adam - to be introduced next -- being more popular.

#### Adam\index{Adam (optimizer)}

Adam combines two concepts we've already seen: momentum -- to keep "on track" -- and parameter-dependent updates, to avoid excessive dependence on fast-changing parameters. The logic is like this.[^optimizers-2]

[^optimizers-2]: Actual implementations usually contain an additional step, but there is no need to go into details here.

For one, just like in SGD with momentum, we keep an exponentially weighted average of gradients. Here the weighting coefficient, $\gamma_v$, is usually set to 0.9.

$$
v_i^{(k+1)} = \gamma_v \ v_i^{(k)} + (1-\gamma_v) \ g_i^{(k)}
$$ {#eq-optimizers-7}

Also, like in RMSProp, there is an exponentially weighted average of squared gradients, with weighting coefficient $\gamma_s$ usually set to 0.999.

$$
s_i^{(k+1)} = \gamma_s \ s_i^{(k)} + (1-\gamma_s) \ (g_i^{(k)})^2
$$ {#eq-optimizers-8}

The parameter updates now make use of that information in the following way. The velocity determines the direction of the update, while both velocity and magnitude of gradients (together with the learning rate, $lr$) determine its size:

$$
x_i^{(k+1)} = x_i^{(k)} - \frac{lr \ 
v_i^{(k+1)}}{\epsilon + \sqrt{s_i^{(k+1)}}}\ \\
$$ {#eq-optimizers-9}

Let's conclude this chapter by testing Adam on our running example (@fig-optimizers-adam).

![Adam (white), compared with vanilla SGD (gray).](images/optimizers-adam.png){#fig-optimizers-adam fig-alt="A non-isotropic paraboloid, stretched-out widely along the x-axis, but with y-values centered sharply around y = 0. Displayed are two optimization paths, one using steepest descent, one using Adam. With steepest descent, many steps are needed to arrive at the minimum, while Adam needs four steps only."}

Next, we head on to loss functions, the last building block to look at before we re-factor the regression network and function minimization examples to benefit from `torch` modules and optimizers.




## 9. Loss functions {#sec:loss-functions}

The concept of a loss function is essential to machine learning. At any iteration, the current loss value indicates how far the estimate is from the target. It is then used to update the parameters in a direction that will decrease the loss.

In our applied example, we already have made use of a loss function: mean squared error, computed manually as

```{r}
library(torch)

loss <- (y_pred - y)$pow(2)$sum()
```

As you might expect, here is another area where this kind of manual effort is not needed.

In this final conceptual chapter before we re-factor our running examples, we want to talk about two things: First, how to make use of `torch`'s built-in loss functions\index{loss functions (built into torch)}. And second, what function to choose.

### `torch` loss functions

In `torch`, loss functions start with `nn_` or `nnf_`.

Using `nnf_`, you directly *call a function*. Correspondingly, its arguments (estimate and target) both are tensors. For example, here is `nnf_mse_loss()`, the built-in analog to what we coded manually:

```{r}
nnf_mse_loss(torch_ones(2, 2), torch_zeros(2, 2) + 0.1)
```

    torch_tensor
    0.81
    [ CPUFloatType{} ]

With `nn_`, in contrast, you create an object:

```{r}
l <- nn_mse_loss()
```

This object can then be called on tensors to yield the desired loss:

```{r}
l(torch_ones(2, 2),torch_zeros(2, 2) + 0.1)
```

    torch_tensor
    0.81
    [ CPUFloatType{} ]

Whether to choose object or function is mainly a matter of preference and context. In larger models, you may end up combining several loss functions, and then, creating loss objects can result in more modular, and more maintainable code. In this book, I'll mainly use the first way, unless there are compelling reasons to do otherwise.

On to the second question.

### What loss function should I choose?

In deep learning, or machine learning overall, most applications aim to do one (or both) of two things: predict a numerical value, or estimate a probability. The regression task of our running example does the former; real-world applications might forecast temperatures, infer employee churn, or predict sales. In the second group, the prototypical task is *classification*. To categorize, say, an image according to its most salient content, we really compute the respective probabilities. Then, when the probability for "dog" is 0.7, while that for "cat" is 0.3, we say it's a dog.

#### Maximum likelihood

In both classification and regression, the mostly used loss functions are built on the *maximum likelihood* principle. Maximum likelihood means: We want to choose model parameters in a way that the *data*, the things we have observed or could have observed, are maximally likely. This principle is not "just" fundamental, it is also intuitively appealing. Imagine a simple example.

Say we have the values 7.1, 22.14, and 11.3, and we know that the underlying process follows a normal distribution. Then it is much more likely that these data have been generated by a distribution with mean 14 and standard deviation 7 than by one with mean 20 and standard deviation 1.

#### Regression

In regression (that implicitly assumes the target distribution to be normal[^loss_functions-1]), to maximize likelihood, we just keep using mean squared error -- the loss we've been computing all along. Maximum likelihood estimators have all kinds of desirable statistical properties. However, in concrete applications, there may be reasons to use different ones.

[^loss_functions-1]: For cases where that assumption seems unlikely, distribution-adequate loss functions are provided (e.g., Poisson negative log likelihood, available as `nnf_poisson_nll_loss()` .

For example, say a dataset has outliers where, for some reason, prediction and target are found to be deviating substantially. Mean squared error will allocate high importance to these outliers. In such cases, possible alternatives are mean absolute error (`nnf_l1_loss()`) and smooth L1 loss (`nn_smooth_l1_loss()`). The latter is a mixture type that, by default, computes the absolute (L1) error, but switches to squared (L2) error whenever the absolute errors get very small.

#### Classification

In classification, we are comparing two *distributions*. The estimate is a probability by design, and the target can be viewed as one, too. In that light, maximum likelihood estimation is equivalent to minimizing the Kullback-Leibler divergence (KL divergence).

KL divergence is a measure of how two distributions differ. It depends on two things: the likelihood of the data, as determined by some data-generating process, and the likelihood of the data under the model. In the machine learning scenario, however, we are concerned only with the latter. In that case, the criterion to be minimized reduces to the *cross-entropy*\index{cross entropy} between the two distributions. And cross-entropy loss is exactly what is commonly used in classification tasks.

In `torch`, there are several variants of loss functions that calculate cross-entropy. With this topic, it's nice to have a quick reference around; so here is a quick lookup table (@tbl-loss-funcs-features abbreviates the -- rather long-ish -- function names; see @tbl-loss-abbrevs for the mapping):

|        |          |             |            |               |           |
|--------|----------|-------------|------------|---------------|-----------|
|        | **Data** |             | **Input**  |               |           |
|        | binary   | multi-class | raw scores | probabilities | log probs |
| *BCeL* | Y        |             | Y          |               |           |
| *Ce*   |          | Y           | Y          |               |           |
| *BCe*  | Y        |             |            | Y             |           |
| *Nll*  |          | Y           |            |               | Y         |

: Loss functions, by type of data they work on (binary vs. multi-class) and expected input (raw scores, probabilities, or log probabilities). {#tbl-loss-funcs-features}

|        |                                          |
|--------|------------------------------------------|
| *BCeL* | `nnf_binary_cross_entropy_with_logits()` |
| *Ce*   | `nnf_cross_entropy()`                    |
| *BCe*  | `nnf_binary_cross_entropy()`             |
| *Nll*  | `nnf_nll_loss()`                         |

: Abbreviations used to refer to `torch` loss functions. {#tbl-loss-abbrevs}

To pick the function applicable to your use case, there are two things to consider.

First, are there just two possible classes ("dog vs. cat", "person present / person absent", etc.), or are there several?

And second, what is the type of the estimated values? Are they raw scores (in theory, any value between plus and minus infinity)? Are they probabilities (values between 0 and 1)? Or (finally) are they log probabilities, that is, probabilities to which a logarithm has been applied? (In the final case, all values should be either negative or equal to zero.)

##### Binary data\index{cross entropy!binary}

Starting with binary data, our example classification vector is a sequence of zeros and ones. When thinking in terms of probabilities, it is most intuitive to imagine the ones standing for presence, the zeros for absence of one of the classes in question -- cat or no cat, say.

```{r}
target <- torch_tensor(c(1, 0, 0, 1, 1))
```

The raw scores could be anything. For example:

```{r}
unnormalized_estimate <-
  torch_tensor(c(3, 2.7, -1.2, 7.7, 1.9))
```

To turn these into probabilities, all we need to do is pass them to `nnf_sigmoid()`. `nnf_sigmoid()` squishes its argument to values between zero and one:

```{r}
probability_estimate <- nnf_sigmoid(unnormalized_estimate)
probability_estimate
```

    torch_tensor
     0.9526
     0.9370
     0.2315
     0.9995
     0.8699
    [ CPUFloatType{5} ]

From the above table, we see that given `unnormalized_estimate` and `probability_estimate`, we can use both as inputs to a loss function -- but we have to choose the appropriate one. Provided we do that, the output has to be the same in both cases.

Let's see (raw scores first):

```{r}
nnf_binary_cross_entropy_with_logits(
  unnormalized_estimate, target
)
```

    torch_tensor
    0.643351
    [ CPUFloatType{} ]

And now, probabilities:\index{\texttt{nnf{\textunderscore}binary{\textunderscore}cross{\textunderscore}entropy()}}

```{r}
nnf_binary_cross_entropy(probability_estimate, target)
```

    torch_tensor
    0.643351
    [ CPUFloatType{} ]

That worked as expected. What does this mean in practice? It means that when we build a model for binary classification, and the final layer computes an un-normalized score, we don't need to attach a sigmoid layer to obtain probabilities. We can just call `nnf_binary_cross_entropy_with_logits()` when training the network. In fact, doing so is the preferred way, also due to reasons of numerical stability.

##### Multi-class data\index{cross entropy!multi-class}

Moving on to multi-class data, the most intuitive framing now really is in terms of (several) *classes*, not presence or absence of a single class. Think of classes as class indices (maybe indexing into some look-up table). Being indices, technically, classes start at 1:

```{r}
target <- torch_tensor(c(2, 1, 3, 1, 3), dtype = torch_long())
```

In the multi-class scenario, raw scores are a two-dimensional tensor. Each row contains the scores for one observation, and each column corresponds to one of the classes. Here's how the raw estimates could look:

```{r}
unnormalized_estimate <- torch_tensor(
  rbind(c(1.2, 7.7, -1),
    c(1.2, -2.1, -1),
    c(0.2, -0.7, 2.5),
    c(0, -0.3, -1),
    c(1.2, 0.1, 3.2)
  )
)
```

As per the above table, given this estimate, we should be calling `nnf_cross_entropy()` (and we will, when below we compare results).

So that's the first option, and it works exactly as with binary data. For the second, there is an additional step.

First, we again turn raw scores into probabilities, using `nnf_softmax()`. For most practical purposes, `nnf_softmax()` can be seen as the multi-class equivalent of `nnf_sigmoid()`. Strictly though, their effects are not the same. In a nutshell, `nnf_sigmoid()` treats low-score and high-score values equivalently, while `nnf_softmax()` exacerbates the distances between the top score and the remaining ones ("winner takes all").

```{r}
probability_estimate <- nnf_softmax(unnormalized_estimate,
  dim = 2
)
probability_estimate

```

    torch_tensor
     0.0015  0.9983  0.0002
     0.8713  0.0321  0.0965
     0.0879  0.0357  0.8764
     0.4742  0.3513  0.1745
     0.1147  0.0382  0.8472
    [ CPUFloatType{5,3} ]

The second step, the one that was not required in the binary case, consists in transforming the probabilities to log probabilities. In our example, this could be accomplished by calling `torch_log()` on the `probability_estimate` we just computed. Alternatively, both steps together are taken care of by `nnf_log_softmax()`:

```{r}
logprob_estimate <- nnf_log_softmax(unnormalized_estimate,
  dim = 2
)
logprob_estimate
```

    torch_tensor
    -6.5017 -0.0017 -8.7017
    -0.1377 -3.4377 -2.3377
    -2.4319 -3.3319 -0.1319
    -0.7461 -1.0461 -1.7461
    -2.1658 -3.2658 -0.1658
    [ CPUFloatType{5,3} ]

Now that we have estimates in both possible forms, we can again compare results from applicable loss functions. First, `nnf_cross_entropy()` on the raw scores:\index{\texttt{nnf{\textunderscore}cross{\textunderscore}entropy()}}

```{r}
nnf_cross_entropy(unnormalized_estimate, target)
```

    torch_tensor
    0.23665
    [ CPUFloatType{} ]

And second, `nnf_nll_loss()` on the log probabilities:\index{\texttt{nnf{\textunderscore}nll{\textunderscore}loss()}}

```{r}
nnf_nll_loss(logprob_estimate, target)
```

    torch_tensor
    0.23665
    [ CPUFloatType{} ]

Application-wise, what was said for the binary case applies here as well: In a multi-class classification network, there is no need to have a softmax layer at the end.

Before we end this chapter, let's address a question that might have come to mind. Is not binary classification a sub-type of the multi-class setup? Should we not, in that case, arrive at the same result, whatever the method chosen?

##### Check: Binary data, multi-class method

Let's see. We re-use the binary-classification scenario employed above. Here it is again:

```{r}
target <- torch_tensor(c(1, 0, 0, 1, 1))

unnormalized_estimate <- 
  torch_tensor(c(3, 2.7, -1.2, 7.7, 1.9))

probability_estimate <- nnf_sigmoid(unnormalized_estimate)

nnf_binary_cross_entropy(probability_estimate, target)

```

    torch_tensor
    0.64335
    [ CPUFloatType{} ]

We hope to get the same value doing things the multi-class way. We already have the probabilities (namely, `probability_estimate`); we just need to put them into the "observation by class" format expected by `nnf_nll_loss()`:

```{r}
# logits
multiclass_probability <- torch_tensor(rbind(
  c(1 - 0.9526, 0.9526),
  c(1 - 0.9370, 0.9370),
  c(1 - 0.2315, 0.2315),
  c(1 - 0.9995, 0.9995),
  c(1 - 0.8699, 0.8699)
))
```

Now, we still want to apply the logarithm. And there is one other thing to be taken care of: In the binary setup, classes were coded as probabilities (either 0 or 1); now, we're dealing with indices. This means we add 1 to the `target` tensor:

```{r}
target <- target + 1
```

Finally, we can call `nnf_nll_loss()`:

```{r}
nnf_nll_loss(
  torch_log(multiclass_probability),
  target$to(dtype = torch_long())
)
```

    torch_tensor
    0.643275
    [ CPUFloatType{} ]

There we go. The results are indeed the same.




## 10. Function minimization with L-BFGS {#sec:optim-2}

Now that we've become acquainted with `torch` modules and optimizers, we can go back to the two tasks we already approached without either: function minimization, and training a neural network. Again, we start with minimization, and leave the network to the next chapter.

Thinking back to what we did when minimizing the Rosenbrock function, in essence it was this:

1.  Define a tensor to hold the parameter to be optimized, namely, the $\mathbf{x}$-position where the function attains its minimum.

2.  Iteratively update the parameter, subtracting a fraction of the current gradient.

While as a strategy, this was straightforward, a problem remained: How big a fraction of the gradient should we subtract? It's exactly here that optimizers come in useful.

### Meet L-BFGS\index{L-BFGS (optimizer)}

So far, we've only talked about the kinds of optimizers often used in deep learning -- stochastic gradient descent (SGD), SGD with momentum, and a few classics from the *adaptive* *learning rate* family: RMSProp, Adadelta, Adagrad, Adam. All these have in common one thing: They only make use of the *gradient*, that is, the vector of first derivatives. Accordingly, they are all *first-order* algorithms. This means, however, that they are missing out on helpful information provided by the *Hessian*, the matrix of second derivatives.

#### Changing slopes

First derivatives tell us about the *slope* of the landscape: Does it go up? Does it go down? How much so? Going a step further, second derivatives encode how much that slope *changes*.

Why should that be important?

Assume we're at point $\mathbf{x}_n$, and have just decided on a suitable descent direction. We take a step, of length determined by some pre-chosen learning rate, all set to arrive at point $\mathbf{x}_{n+1}$. What we don't know is how the slope will have changed by the time we'll have gotten there. Maybe it's become much flatter in the meantime: In this case, we'll have gone way too far, overshooting and winding up in a far-off area where anything could have happened in-between (including the slope going *up* again!).

We can illustrate this on a function of a single variable. Take a parabola, such as

$$
y = 10x^2
$$

Its derivative is $\frac{dy}{dx} = 20x$. If our current $x$ is, say, $3$, and we work with a learning rate of $0.1$, we'll subtract $20 * 3 * 0.1= 6$, winding up at $-3$.

But say we had slowed down at $2$ and inspected the current slope. We'd have seen that there, the slope was less steep; in fact, when at that point, we should just have subtracted $20 * 2 * 0.1= 4$.

By sheer luck, this "close-your-eyes-and-jump" strategy can still work out -- *if* we happen to be using just the right learning rate for the function in question. (At the chosen learning rate, this would have been the case for a different parabola, $y = 5x^2$, for example.) But wouldn't it make sense to include second derivatives in the decision from the outset?

Algorithms that do this form the family of Newton methods. First, we look at their "purest" specimen, which best illustrates the principle but seldom is feasible in practice.

#### Exact Newton method

In higher dimensions, the exact Newton method multiplies the gradient by the inverse of the Hessian, thus scaling the descent direction coordinate-by-coordinate. Our current example has just a single independent variable; so this means for us: take the first derivative, and divide by the second.

We now have a scaled gradient -- but what portion of it should we subtract? In its original version, the exact Newton method does not make use of a learning rate, thus freeing us of the familiar trial-and-error game. Let's see, then: In our example, the second derivative is $20$, meaning that at $x=3$ we have to subtract $(20 * 3)/20=3$. Voilà, we end up at $0$, the location of the minimum, in a single step.

Seeing how that turned out just great, why don't we do it all the time? For one, it will work perfectly only with quadratic functions, like the one we chose for the demonstration. In other cases, it, too, will normally need some "tuning", for example, by using a learning rate here as well.

But the main reason is another one. In more realistic applications, and certainly in the areas of machine learning and deep learning, computing the inverse of the Hessian at every step is way too costly. (It may, in fact, not even be possible.) This is where *approximate*, a.k.a. *Quasi-Newton*, methods come in.

#### Approximate Newton: BFGS and L-BFGS

Among approximate Newton methods, probably the most-used is the *Broyden-Goldfarb-Fletcher-Shanno* algorithm, or *BFGS*. Instead of continually computing the exact inverse of the Hessian, it keeps an iteratively-updated approximation of that inverse. BFGS is often implemented in a more memory-friendly version, referred to as *Limited-Memory BFGS* (*L-BFGS*). This is the one provided as part of the core `torch` optimizers.

Before we get there, though, there is one last conceptual thing to discuss.

#### Line search

Like their exact counterpart, approximate Newton methods can work without a learning rate. In that case, they compute a descent direction and follow the scaled gradient as-is. We already talked about how, depending on the function in question, this can work more or less well. When it does not, there are two things one could do: Firstly, take small steps, or put differently, introduce a learning rate. And secondly, do a *line search*.

With line search, we spend some time evaluating how far to follow the descent direction. There are two principal ways of doing this.

The first, *exact* line search, involves yet another optimization problem: Take the current point, compute the descent direction, and hard-code them as givens in a *second* function that depends on the learning rate only. Then, differentiate this function to find *its* minimum. The solution will be the learning rate that optimizes the step length taken.

The alternative strategy is to do an approximate search. By now, you're probably not surprised: Just as approximate Newton is more realistically-feasible than exact Newton, approximate line search is more practicable than exact line search.

For line search, approximating the best solution means following a set of proven heuristics. Essentially, we look for something that is *just* *good enough*. Among the most established heuristics are the *Strong Wolfe conditions*, and this is the strategy implemented in `torch`'s `optim_lbfgs()`. In the next section, we'll see how to use `optim_lbfgs()` to minimize the Rosenbrock function, both with and without line search.

### Minimizing the Rosenbrock function with `optim_lbfgs()`

Here is the Rosenbrock function again:

```{r}
library(torch)

a <- 1
b <- 5

rosenbrock <- function(x) {
  x1 <- x[1]
  x2 <- x[2]
  (a - x1)^2 + b * (x2 - x1^2)^2
}
```

In our manual minimization efforts, the procedure was the following. A one-time action, we first defined the parameter tensor destined to hold the current $\mathbf{x}$:

```{r}
x <- torch_tensor(c(-1, 1), requires_grad = TRUE)
```

Then, we iteratively executed the following operations:

1.  Calculate the function value at the current $\mathbf{x}$.

2.  Compute the gradient of that value at the position in question.

3.  Subtract a fraction of the gradient from the current $\mathbf{x}$.

How, if so, does that blueprint change?

The first step remains unchanged. We still have

```{r}
value <- rosenbrock(x)
```

The second step stays the same, as well. We still call `backward()` directly on the output tensor:

```{r}
value$backward()
```

This is because an optimizer does not *compute* gradients; it *decides what to do with the gradient* once it's been computed.

What changes, thus, is the third step, the one that also was the most cumbersome. Now, it is the optimizer that applies the update. To be able to do that, there is a prerequisite: Prior to starting the loop, the optimizer will need to be told which parameter it is supposed to work on. In fact, this is so important that you can't even create an optimizer without passing it that parameter:

```{r}
opt <- optim_lbfgs(x)
```

In the loop, we now call the `step()` method on the optimizer object to update the parameter. There is just one part from our manual procedure that needs to get carried over to the new way: We still need to zero out the gradient on each iteration. Just this time, not on the parameter tensor, `x`, but the optimizer object itself. *In principle*, this then yields the following actions to be performed on each iteration:

```
value <- rosenbrock(x)

opt$zero_grad()
value$backward()

opt$step()
```

Why "in principle"? In fact, this is what we'd write for every optimizer *but* `optim_lbfgs()`.

For `optim_lbfgs()`, `step()` needs to be called passing in an anonymous function, a closure. Zeroing of previous gradients, function call, and gradient calculation, all these happen inside the closure:

```{r}
calc_loss <- function() {
  optimizer$zero_grad()
  value <- rosenbrock(x_star)
  value$backward()
  value
}


```

Having executed those actions, the closure returns the function value. Here is how it is called by `step()`:

```
for (i in 1:num_iterations) {
  optimizer$step(calc_loss)
}
```

Now we put it all together, add some logging output, and compare what happens with and without line search.

#### `optim_lbfgs()` default behavior

As a baseline, we first run without line search. Two iterations are enough. In the below output, you can see that in each iteration, the closure is evaluated several times. This is the technical reason we had to create it in the first place.

```{r}
num_iterations <- 2

x <- torch_tensor(c(-1, 1), requires_grad = TRUE)

optimizer <- optim_lbfgs(x)

calc_loss <- function() {
  optimizer$zero_grad()

  value <- rosenbrock(x)
  cat("Value is: ", as.numeric(value), "\n")

  value$backward()
  value
}

for (i in 1:num_iterations) {
  cat("\nIteration: ", i, "\n")
  optimizer$step(calc_loss)
}


```

    Iteration:  1 
    Value is:  4 
    Value is:  6 
    Value is:  318.0431 
    Value is:  5.146369 
    Value is:  4.443705 
    Value is:  0.8787204 
    Value is:  0.8543001 
    Value is:  2.001667 
    Value is:  0.5656172 
    Value is:  0.400589 
    Value is:  7.726219 
    Value is:  0.3388008 
    Value is:  0.2861604 
    Value is:  1.951176 
    Value is:  0.2071857 
    Value is:  0.150776 
    Value is:  0.411357 
    Value is:  0.08056168 
    Value is:  0.04880721 
    Value is:  0.0302862 

    Iteration:  2 
    Value is:  0.01697086 
    Value is:  0.01124081 
    Value is:  0.0006622815 
    Value is:  3.300996e-05 
    Value is:  1.35731e-07 
    Value is:  1.111701e-09 
    Value is:  4.547474e-12 

To make sure we really have found the minimum, we check `x`:

```{r}
x
```

    torch_tensor
     1.0000
     1.0000
    [ CPUFloatType{2} ]

Can this still be improved upon?

#### `optim_lbfgs()` with line search

Let's see. Below, the only line that's changed is the one where we construct the optimizer.

```{r}
num_iterations <- 2

x <- torch_tensor(c(-1, 1), requires_grad = TRUE)

optimizer <- optim_lbfgs(x, line_search_fn = "strong_wolfe")

calc_loss <- function() {
  optimizer$zero_grad()

  value <- rosenbrock(x)
  cat("Value is: ", as.numeric(value), "\n")

  value$backward()
  value
}

for (i in 1:num_iterations) {
  cat("\nIteration: ", i, "\n")
  optimizer$step(calc_loss)
}
```

    Iteration:  1 
    Value is:  4 
    Value is:  6 
    Value is:  3.802412 
    Value is:  3.680712 
    Value is:  2.883048 
    Value is:  2.5165 
    Value is:  2.064779 
    Value is:  1.38384 
    Value is:  1.073063 
    Value is:  0.8844351 
    Value is:  0.5554555 
    Value is:  0.2501077 
    Value is:  0.8948895 
    Value is:  0.1619074 
    Value is:  0.06823064 
    Value is:  0.01653575 
    Value is:  0.004060207 
    Value is:  0.00353789 
    Value is:  0.000391416 
    Value is:  4.303527e-06 
    Value is:  2.036851e-08 
    Value is:  6.870948e-12 

    Iteration:  2 
    Value is:  6.870948e-12 

With line search, a single iteration is sufficient to reach the minimum. Inspecting the individual losses, we also see that the algorithm reduces the loss nearly every time it probes the function, which without line search, had not been the case.



## 11. Modularizing the neural network {#sec:network-2}

Let's recall the network we built a few chapters ago. Its purpose was regression, but its method was not *linear*. Instead, an activation function (ReLU, for "rectified linear unit") introduced a nonlinearity, located between the single hidden layer and the output layer. The "layers", in this original implementation, were just tensors: weights and biases. You won't be surprised to hear that these will be replaced by *modules*.

How will the training process change? Conceptually, we can distinguish four phases: the forward pass, loss computation, backpropagation of gradients, and weight updating. Let's think about where our new tools will fit in:

-   The forward pass, instead of calling functions on tensors, will call the model.

-   In computing the loss, we now make use of `torch`'s `nnf_mse_loss()`.

-   Backpropagation of gradients is, in fact, the only operation that remains unchanged.

-   Weight updating is taken care of by the optimizer.

Once we've made those changes, the code will be more modular, and a lot more readable.

### Data

As a prerequisite, we generate the data, same as last time.

```{r}
library(torch)

# input dimensionality (number of input features)
d_in <- 3
# number of observations in training set
n <- 100

x <- torch_randn(n, d_in)
coefs <- c(0.2, -1.3, -0.5)
y <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)


```

### Network

With two linear layers connected via ReLU activation, the easiest choice is a sequential module, very similar to the one we saw in the introduction to modules:

```{r}
# dimensionality of hidden layer
d_hidden <- 32
# output dimensionality (number of predicted features)
d_out <- 1

net <- nn_sequential(
  nn_linear(d_in, d_hidden),
  nn_relu(),
  nn_linear(d_hidden, d_out)
)
```

### Training

Here is the updated training process. We use the Adam optimizer, a popular choice.

```{r}

opt <- optim_adam(net$parameters)

#### training loop --------------------------------------

for (t in 1:200) {
  
  ### -------- Forward pass --------
  y_pred <- net(x)
  
  ### -------- Compute loss -------- 
  loss <- nnf_mse_loss(y_pred, y)
  if (t %% 10 == 0)
    cat("Epoch: ", t, "   Loss: ", loss$item(), "\n")
  
  ### -------- Backpropagation --------
  opt$zero_grad()
  loss$backward()
  
  ### -------- Update weights -------- 
  opt$step()

}
```

    Epoch:  10    Loss:  2.549933 
    Epoch:  20    Loss:  2.422556 
    Epoch:  30    Loss:  2.298053 
    Epoch:  40    Loss:  2.173909 
    Epoch:  50    Loss:  2.0489 
    Epoch:  60    Loss:  1.924003 
    Epoch:  70    Loss:  1.800404 
    Epoch:  80    Loss:  1.678221 
    Epoch:  90    Loss:  1.56143 
    Epoch:  100    Loss:  1.453637 
    Epoch:  110    Loss:  1.355832 
    Epoch:  120    Loss:  1.269234 
    Epoch:  130    Loss:  1.195116 
    Epoch:  140    Loss:  1.134008 
    Epoch:  150    Loss:  1.085828 
    Epoch:  160    Loss:  1.048921 
    Epoch:  170    Loss:  1.021384 
    Epoch:  180    Loss:  1.0011 
    Epoch:  190    Loss:  0.9857832 
    Epoch:  200    Loss:  0.973796 

In addition to shortening and streamlining the code, our changes have made a big difference performance-wise.

### What's to come

You now know a lot about how `torch` works, and how to use it to minimize a cost function in various settings: for example, to train a neural network. But for real-world applications, there is a lot more `torch` has to offer. The next -- and most voluminous -- part of the book focuses on deep learning.




# Deep learning with torch

## 12. Overview

This part of the book is completely dedicated to applications of deep learning. There will be two categories of things to dive into: topics workflow-related, and topics related to domain adaptation.

Regarding workflow, we'll see how to:

-   prepare the input data in a form the model can work with;
-   effectively and efficiently train a model, monitoring progress and adjusting hyper-parameters on the fly;
-   save and load models;
-   making models generalize beyond the training data;
-   speed up training;
-   and more.

Secondly -- beyond an efficient workflow -- the task in question matters. Compositions of linear layers, of the type we used to learn `torch` in the first part, will not suffice when our goal is to model images or time series. Successful use of deep learning means tailoring model architecture to the domain in question. To that end, we start from concrete tasks, and present applicable architectures directly by example.

Concretely, the plan is the following. The upcoming two chapters will introduce you to workflow-related techniques that are indispensable in practice. You'll encounter another package, `luz`, that endows `torch` with an important layer of abstraction, and significantly streamlines the workflow. Once you know how to use it, we're all set to look at a first application: image classification. To improve on our initial results, we then back up and explore two more advanced workflow-related topics: how to improve generalization, and how to speed up training. Equipped with that knowledge, we first return to images, before extending our domain-related skills to tabular data, time series, and audio.



## 13. Loading data

Our toy example, which we'll see a third (and last) version of in the next chapter, had the model train on a tiny set of data -- small enough to pass all observations to the model in one go. What if that wasn't the case? Say we had 10,000 items instead, and every item was an RGB image of size 256 x 256 pixels. Even on very powerful hardware, we could not possibly train a model on the complete data all at once.

For that reason, deep-learning frameworks like `torch` include an input pipeline that lets you pass data to the model in *batches* -- that is, subsets of observations. Involved in this process are two classes: `dataset()` and `dataloader()`. Before we look at how to construct instances of these, let's characterize them by what they're *for*.

## Data vs. `dataset()`\index{\texttt{dataset()}} vs. `dataloader()`\index{\texttt{dataloader()}} -- what's the difference?

In this book, "dataset" (variable-width font, no parentheses), or just "the data", usually refers to things like R matrices, `data.frame`s, and what's contained therein. A `dataset()` (fixed-width font, parentheses), however, is a `torch` object that knows how to do one thing: *deliver to the caller a* *single item.* That item, usually, will be a list, consisting of one input and one target tensor. (It could be anything, though -- whatever makes sense for the task. For example, it could be a single tensor, if input and target are the same. Or more than two tensors, in case different inputs should be passed to different modules.)

As long as it fulfills the above-stated contract, a `dataset()` is free to do whatever needs to be done. It could, for example, download data from the internet, store them in some temporary location, do some pre-processing, and when asked, return bite-sized chunks of data in just the shape expected by a certain class of models. No matter what it does in the background, all its caller cares about is that it return a single item. Its caller, that's the `dataloader()`.

A `dataloader()`'s role is to feed input to the model in *batches*. One immediate reason is computer memory: Most `dataset()`s will be far too large to pass them to the model in one go. But there are additional benefits to batching. Since gradients are computed (and model weights updated) once per *batch*, there is an inherent stochasticity to the process, a stochasticity that helps with model training. We'll talk more about that in an upcoming chapter.

### Using `dataset()`s

`dataset()`s come in all flavors, from ready-to-use -- and brought to you by some package, `torchvision` or `torchdatasets`, say, or any package that chooses to provide access to data in `torch`-ready form -- to fully customized (made by you, that is). Creating `dataset()`s is straightforward, since they are R6 objects, and there's just three methods to be implemented. These methods are:

1.  `initialize(...)`. Parameters to `initialize()` are passed when a `dataset()` is instantiated. Possibilities include, but are not limited to, references to R `data.frame`s, filesystem paths, download URLs, and any configurations and parameterizations expected by the `dataset()`.
2.  `.getitem(i)`. This is the method responsible for fulfilling the contract. Whatever it returns counts as a single item. The parameter, `i`, is an index that, in many cases, will be used to determine the starting position in the underlying data structure (a `data.frame` of file system paths, for example). However, the `dataset()` is not *obliged* to actually make use of that parameter. With extremely huge `dataset()`s, for example, or given serious class imbalance, it could instead decide to return items based on *sampling*.
3.  `.length()`. This, usually, is a one-liner, its only purpose being to inform about the number of available items in a `dataset()`.

Here is a blueprint for creating a `dataset()`:

```
ds <- dataset()(
  initialize = function(...) {
    ...
  },
  .getitem = function(index) {
    ...
  },
  .length = function() {
    ...
  }
)
```

That said, let's compare three ways of obtaining a `dataset()` to work with, from tailor-made to maximally effortless.

#### A self-built `dataset()`

Let's say we wanted to build a classifier based on the popular `iris` alternative, `palmerpenguins`.

```{r}
library(torch)
library(palmerpenguins)
library(dplyr)

penguins %>% glimpse()
```

    $ species           <fct> Adelie, Adelie, Adelie, Adelie,...
    $ island            <fct> Torgersen, Torgersen, Torgersen,...
    $ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3,...
    $ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6,...
    $ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181,...
    $ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650,...
    $ sex               <fct> male, female, female, NA, female,...
    $ year              <int> 2007, 2007, 2007, 2007, 2007,...

In predicting `species`, we want to make use of just a subset of columns: `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`. We build a `dataset()` that returns exactly what is needed:

```{r}
penguins_dataset <- dataset(
  name = "penguins_dataset()",
  initialize = function(df) {
    df <- na.omit(df)
    self$x <- as.matrix(df[, 3:6]) %>% torch_tensor()
    self$y <- torch_tensor(
      as.numeric(df$species)
    )$to(torch_long())
  },
  .getitem = function(i) {
    list(x = self$x[i, ], y = self$y[i])
  },
  .length = function() {
    dim(self$x)[1]
  }
)
```

Once we've instantiated a `penguins_dataset()`, we should immediately perform some checks. First, does it have the expected length?

```{r}
ds <- penguins_dataset(penguins)
length(ds)
```

    [1] 333

And second, do individual elements have the expected shape and data type? Conveniently, we can access `dataset()` items like tensor values, through indexing:

```{r}
ds[1]

```

    $x
    torch_tensor
       39.1000
       18.7000
      181.0000
     3750.0000
    [ CPUFloatType{4} ]

    $y
    torch_tensor
    1
    [ CPULongType{} ]

This also works for items "further down" in the `dataset()` -- it has to: When indexing into a `dataset()`, what happens in the background is a call to `.getitem(i)`, passing along the desired position `i`.

Truth be told, in this case we didn't really have to build our own `dataset()`. With so little pre-processing to be done, there is an alternative: `tensor_dataset()`.

#### `tensor_dataset()`

When you already have a tensor around, or something that's readily converted to one, you can make use of a built-in `dataset()` generator: `tensor_dataset()`. This function can be passed any number of tensors; each batch item then is a list of tensor values:

```{r}
three <- tensor_dataset(
  torch_randn(10), torch_randn(10), torch_randn(10)
)
three[1]
```

    [[1]]
    torch_tensor
    0.522735
    [ CPUFloatType{} ]

    [[2]]
    torch_tensor
    -0.976477
    [ CPUFloatType{} ]

    [[3]]
    torch_tensor
    -1.14685
    [ CPUFloatType{} ]

In our `penguins` scenario, we end up with two lines of code:

```{r}
penguins <- na.omit(penguins)
ds <- tensor_dataset(
  torch_tensor(as.matrix(penguins[, 3:6])),
  torch_tensor(
    as.numeric(penguins$species)
  )$to(torch_long())
)

ds[1]
```

Admittedly though, we have not made use of all the dataset's columns. The more pre-processing you need a `dataset()` to do, the more likely you are to want to code your own.

Thirdly and finally, here is the most effortless possible way.

#### `torchvision::mnist_dataset()`

When you're working with packages in the `torch` ecosystem, chances are that they already include some `dataset()`s, be it for demonstration purposes or for the sake of the data themselves. `torchvision`, for example, packages a number of classic image datasets -- among those, that archetype of archetypes, MNIST.

Since we're going to talk about image processing in a later chapter, I won't comment on the arguments to `mnist_dataset()` here; we do, however, include a quick check that the data delivered conform to what we'd expect:

```{r}
library(torchvision)

ds <- mnist_dataset(
  root = ".",
  train = TRUE, # default
  download = TRUE,
  transform = function(x) {
    x %>% transform_to_tensor() 
  }
)

first <- ds[1]
cat("Image shape: ", first$x$shape, " Label: ", first$y, "\n")
```

    Image shape:  1 28 28  Label:  6 

At this point, that is all we need to know about `dataset()`s -- we'll encounter plenty of them in the course of this book. Now, we move on from the one to the many.

### Using `dataloader()`s

Continuing to work with the newly created MNIST `dataset()`, we instantiate a `dataloader()` for it. The `dataloader()` will deliver pairs of images and labels in batches: thirty-two at a time. In every epoch, it will return them in different order (`shuffle = TRUE`):

```{r}
dl <- dataloader(ds, batch_size = 32, shuffle = TRUE)
```

Just like `dataset()`s, `dataloader()`s can be queried about their length:

```{r}
length(dl)
```

    [1] 1875

This time, though, the returned value is not the number of items; it is the number of batches.

To loop over batches, we first obtain an iterator, an object that knows how to traverse the elements in this `dataloader()`. Calling `dataloader_next()`, we can then access successive batches, one by one:

```{r}
first_batch <- dl %>%
  # obtain an iterator for this dataloader
  dataloader_make_iter() %>% 
  dataloader_next()

dim(first_batch$x)
dim(first_batch$y)
```

    [1] 32  1 28 28
    [1] 32

If you compare the batch shape of `x` -- the image part -- with the shape of an individual image (as inspected above), you see that now, there is an additional dimension in front, reflecting the number of images in a batch.

The next step is passing the batches to a model. This -- in fact, this as well as the complete, end-to-end deep-learning workflow -- is what the next chapter is about.



## 14. Training with luz {#sec:luz}

At this point in the book, you know how to train a neural network. Truth be told, though, there's some cognitive effort involved in having to remember the right execution order of steps like `optimizer$zero_grad()`, `loss$backward()`, and `optimizer$step()`. Also, in more complex scenarios than our running example, the list of things to actively remember gets longer.

One thing we haven't talked about yet, for example, is how to handle the usual three stages of machine learning: training, validation, and testing. Another is the question of data flow between *devices* (CPU and GPU, if you have one). Both topics necessitate additional code to be introduced to the training loop. Writing this code can be tedious, and creates a potential for mistakes.

You can see exactly what I'm referring to in the appendix at the end of this chapter. But now, I want to focus on the remedy: a high-level, easy-to-use, concise way of organizing and instrumenting the training process, contributed by a package built on top of `torch`: `luz`.

### Que haya luz - Que haja luz - Let there be light

A *torch* already brings some light, but sometimes in life, there is no *too bright*. `luz` was designed to make deep learning with `torch` as effortless as possible, while at the same time allowing for easy customization. In this chapter, we focus on the overall process; examples of customization will appear in later chapters.

For ease of comparison, we take our running example, and add a third version, now using `luz`. First, we "just" directly port the example; then, we adapt it to a more realistic scenario. In that scenario, we

-   make use of separate training, validation, and test sets;

-   have `luz` compute *metrics* during training/validation;

-   illustrate the use of *callbacks* to perform custom actions or dynamically change hyper-parameters during training; and

-   explain what is going on with the aforementioned *devices*.

### Porting the toy example

#### Data

`luz` does not just substantially transform the code required to train a neural network; it also adds flexibility on the data side of things. In addition to a reference to a `dataloader()`, its `fit()` method accepts `dataset()`s, tensors, and even R objects, as we'll be able to verify soon.

We start by generating an R matrix and a vector, as before. This time though, we also wrap them in a `tensor_dataset()`, and instantiate a `dataloader()`. Instead of just 100, we now generate 1000 observations.

```{r}
library(torch)
library(luz)

# input dimensionality (number of input features)
d_in <- 3
# number of observations in training set
n <- 1000

x <- torch_randn(n, d_in)
coefs <- c(0.2, -1.3, -0.5)
y <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)

ds <- tensor_dataset(x, y)

dl <- dataloader(ds, batch_size = 100, shuffle = TRUE)
```

#### Model

To use `luz`, no changes are needed to the model definition. Note, though, that we just *define* the model architecture; we never actually *instantiate* a model object ourselves.

```{r}
# dimensionality of hidden layer
d_hidden <- 32
# output dimensionality (number of predicted features)
d_out <- 1

net <- nn_module(
  initialize = function(d_in, d_hidden, d_out) {
    self$net <- nn_sequential(
      nn_linear(d_in, d_hidden),
      nn_relu(),
      nn_linear(d_hidden, d_out)
    )
  },
  forward = function(x) {
    self$net(x)
  }
)
```

#### Training

To train the model, we don't write loops anymore. `luz` replaces the familiar *iterative* style by a *declarative* one: You tell `luz` what you want to happen, and like a docile sorcerer's apprentice, it sets in motion the machinery.

Concretely, instruction happens in two -- required -- calls.

1.  In `setup()`\index{\texttt{setup()} (luz)}, you specify the loss function and the optimizer to use.
2.  In `fit()`\index{\texttt{fit()} (luz)}, you pass reference(s) to the training (and optionally, validation) data, as well as the number of epochs to train for.

If the model is configurable -- meaning, it accepts arguments to `initialize()` -- a third method comes into play: `set_hparams()`\index{\texttt{set{\textunderscore}hparams()} (luz)}, to be called in-between the other two. (That's `hparams` for hyper-parameters.) Using this mechanism, you can easily experiment with, for example, different layer sizes, or other factors suspected to affect performance.

```{r}
fitted <- net %>%
  setup(loss = nn_mse_loss(), optimizer = optim_adam) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(dl, epochs = 20)
```

Running this code, you should see output approximately like this:

    Epoch 1/200
    Train metrics: Loss: 3.0343                                                                               
    Epoch 2/200
    Train metrics: Loss: 2.5387                                                                               
    Epoch 3/200
    Train metrics: Loss: 2.2758                                                                               
    ...
    ...
    Epoch 198/200
    Train metrics: Loss: 0.891                                                                                
    Epoch 199/200
    Train metrics: Loss: 0.8879                                                                               
    Epoch 200/200
    Train metrics: Loss: 0.9036 

Above, what we passed to `fit()` was the `dataloader()`. Let's check that referencing the `dataset()` would have been just as fine:

```{r}
fitted <- net %>%
  setup(loss = nn_mse_loss(), optimizer = optim_adam) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(ds, epochs = 20)
```

Or even, `torch` tensors:

```{r}
fitted <- net %>%
  setup(loss = nn_mse_loss(), optimizer = optim_adam) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(list(x, y), epochs = 20)
```

And finally, R objects, which can be convenient when we aren't already working with tensors.

```{r}
fitted <- net %>%
  setup(loss = nn_mse_loss(), optimizer = optim_adam) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(list(as.matrix(x), as.matrix(y)), epochs = 20)
```

In the following sections, we'll always be working with `dataloader()`s; but in some cases those "shortcuts" may come in handy.

Next, we extend the toy example, illustrating how to address more complex requirements.

### A more realistic scenario

#### Integrating training, validation, and test

In deep learning, training and validation phases are interleaved. Every epoch of training is followed by an epoch of validation. Importantly, the data used in both phases have to be strictly disjoint.

In each training phase, gradients are computed and weights are changed; during validation, none of that happens. Why have a validation set, then? If, for each epoch, we compute task-relevant metrics for both partitions, we can see if we are *overfitting* to the training data: that is, drawing conclusions based on training sample specifics not descriptive of the overall population we want to model. All we have to do is two things: instruct `luz` to compute a suitable metric, and pass it an additional `dataloader` pointing to the validation data.

The former is done in `setup()`, and for a regression task, common choices are mean squared or mean absolute error (MSE or MAE, resp.). As we're already using MSE as our loss, let's choose MAE for a metric:

```
fitted <- net %>%
  setup(
    loss = nn_mse_loss(),
    optimizer = optim_adam,
    metrics = list(luz_metric_mae())
  ) %>%
  fit(...)
```

The validation `dataloader` is passed in `fit()` -- but to be able to reference it, we need to construct it first! So now (anticipating we'll want to have a test set, too), we split up the original 1000 observations into three partitions, creating a `dataset` and a `dataloader` for each of them.

```{r}
train_ids <- sample(1:length(ds), size = 0.6 * length(ds))
valid_ids <- sample(
  setdiff(1:length(ds), train_ids),
  size = 0.2 * length(ds)
)
test_ids <- setdiff(
  1:length(ds),
  union(train_ids, valid_ids)
)

train_ds <- dataset_subset(ds, indices = train_ids)
valid_ds <- dataset_subset(ds, indices = valid_ids)
test_ds <- dataset_subset(ds, indices = test_ids)

train_dl <- dataloader(train_ds,
  batch_size = 100, shuffle = TRUE
)
valid_dl <- dataloader(valid_ds, batch_size = 100)
test_dl <- dataloader(test_ds, batch_size = 100)
```

Now, we are ready to start the enhanced workflow:

```{r}
fitted <- net %>%
  setup(
    loss = nn_mse_loss(),
    optimizer = optim_adam,
    metrics = list(luz_metric_mae())
  ) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(train_dl, epochs = 20, valid_data = valid_dl)
```

    Epoch 1/200
    Train metrics: Loss: 2.5863 - MAE: 1.2832                                       
    Valid metrics: Loss: 2.487 - MAE: 1.2365
    Epoch 2/200
    Train metrics: Loss: 2.4943 - MAE: 1.26                                          
    Valid metrics: Loss: 2.4049 - MAE: 1.2161
    Epoch 3/200
    Train metrics: Loss: 2.4036 - MAE: 1.236                                         
    Valid metrics: Loss: 2.3261 - MAE: 1.1962
    ...
    ...
    Epoch 198/200
    Train metrics: Loss: 0.8947 - MAE: 0.7504
    Valid metrics: Loss: 1.0572 - MAE: 0.8287
    Epoch 199/200
    Train metrics: Loss: 0.8948 - MAE: 0.7503
    Valid metrics: Loss: 1.0569 - MAE: 0.8286
    Epoch 200/200
    Train metrics: Loss: 0.8944 - MAE: 0.75
    Valid metrics: Loss: 1.0579 - MAE: 0.8292

Even though both training and validation sets come from the exact same distribution, we do see a bit of overfitting. This is a topic we'll talk about more in the next chapter.

Once training has finished, the `fitted` object above holds a history of epoch-wise metrics, as well as references to a number of important objects involved in the training process. Among the latter is the fitted model itself -- which enables an easy way to obtain predictions on the test set:\index{\texttt{predict()} (luz)}

```{r}
fitted %>% predict(test_dl)
```

    torch_tensor
     0.7799
     1.7839
    -1.1294
    -1.3002
    -1.8169
    -1.6762
    -0.7548
    -1.2041
     2.9613
    -0.9551
     0.7714
    -0.8265
     1.1334
    -2.8406
    -1.1679
     0.8350
     2.0134
     2.1083
     1.4093
     0.6962
    -0.3669
    -0.5292
     2.0310
    -0.5814
     2.7494
     0.7855
    -0.5263
    -1.1257
    -3.3117
     0.6157
    ... [the output was truncated (use n=-1 to disable)]
    [ CPUFloatType{200,1} ]

We also want to evaluate performance on the test set:\index{\texttt{evaluate()} (luz)}

```{r}
fitted %>% evaluate(test_dl)
```

    A `luz_module_evaluation`
    ── Results 
    loss: 0.9271
    mae: 0.7348

This workflow of: training and validation in lock-step, then checking and extracting predictions on the test set is something we'll encounter times and again in this book.

#### Using callbacks to "hook" into the training process\index{callbacks (luz)}

At this point, you may feel that what we've gained in code efficiency, we may have lost in flexibility. Coding the training loop yourself, you can arrange for all kinds of things to happen: save model weights, adjust the learning rate ... whatever you need.

In reality, no flexibility is lost. Instead, `luz` offers a standardized way to achieve the same goals: callbacks. Callbacks are objects that can execute arbitrary R code, at any of the following points in time:

-   when the overall training process starts or ends (`on_fit_begin()` / `on_fit_end()`);

-   when an epoch (comprising training and validation) starts or ends (`on_epoch_begin()` / `on_epoch_end()`);

-   when during an epoch, the training (validation, resp.) phase starts or ends (`on_train_begin()` / `on_train_end()`; `on_valid_begin()` / `on_valid_end()`);

-   when during training (validation, resp.), a new batch is either about to be or has been processed (`on_train_batch_begin()` / `on_train_batch_end()`; `on_valid_batch_begin()` / `on_valid_batch_end()`);

-   and even at specific landmarks inside the "innermost" training / validation logic, such as "after loss computation", "after `backward()`" or "after `step()`".

While you can implement any logic you wish using callbacks (and we'll see how to do this in a later chapter), `luz` already comes equipped with a very useful set. For example:

-   `luz_callback_model_checkpoint()` saves model weights after every epoch (or just in case of improvements, if so instructed).

-   `luz_callback_lr_scheduler()` activates one of `torch`'s *learning rate schedulers*. Different scheduler objects exist, each following their own logic in dynamically updating the learning rate.

-   `luz_callback_early_stopping()` terminates training once model performance stops to improve. What exactly "stops to improve" should mean is configurable by the user.

Callbacks are passed to the `fit()` method in a list. For example, augmenting our most recent workflow:

```{r}
fitted <- net %>%
  setup(
    loss = nn_mse_loss(),
    optimizer = optim_adam,
    metrics = list(luz_metric_mae())
  ) %>%
  set_hparams(d_in = d_in,
              d_hidden = d_hidden,
              d_out = d_out) %>%
  fit(
    train_dl,
    epochs = 20,
    valid_data = valid_dl,
    callbacks = list(
      luz_callback_model_checkpoint(path = "./models/",
                                    save_best_only = TRUE),
      luz_callback_early_stopping(patience = 10)
    )
  )

```

With this configuration, weights will be saved, but only if validation loss decreases. Training will halt if there is no improvement (again, in validation loss) for ten epochs. With both callbacks, you can pick any other metric to base the decision on, and the metric in question may also refer to the training set.

Here, we see early stopping happening after 111 epochs:

    Epoch 1/200
    Train metrics: Loss: 2.5803 - MAE: 1.2547
    Valid metrics: Loss: 3.3763 - MAE: 1.4232
    Epoch 2/200
    Train metrics: Loss: 2.4767 - MAE: 1.229
    Valid metrics: Loss: 3.2334 - MAE: 1.3909
    ...
    ...
    Epoch 110/200
    Train metrics: Loss: 1.011 - MAE: 0.8034
    Valid metrics: Loss: 1.1673 - MAE: 0.8578
    Epoch 111/200
    Train metrics: Loss: 1.0108 - MAE: 0.8032
    Valid metrics: Loss: 1.167 - MAE: 0.8578
    Early stopping at epoch 111 of 200

#### How `luz` helps with devices\index{device handling (luz)}

Finally, let's quickly mention how `luz` helps with device placement. Devices, in a usual environment, are the CPU and perhaps, if available, a GPU. For training, data and model weights need to be located on the same device. This can introduce complexities, and -- at the very least -- necessitates additional code to keep all pieces in sync.

With `luz`, related actions happen transparently to the user. Let's take the prediction step from above:

```{r}
fitted %>% predict(test_dl)
```

In case this code was executed on a machine that has a GPU, `luz` will have detected that, and the model's weight tensors will already have been moved there. Now, for the above call to `predict()`, what happened "under the hood" was the following:

-   `luz` put the model in evaluation mode, making sure that weights are not updated.
-   `luz` moved the test data to the GPU, batch by batch, and obtained model predictions.
-   These predictions were then moved back to the CPU, in anticipation of the caller wanting to process them further with R. (Conversion functions like `as.numeric()`, `as.matrix()` etc. can only act on CPU-resident tensors.)

In the below appendix, you find a complete walk-through of how to implement the train-validate-test workflow by hand. You'll likely find this a lot more complex than what we did above -- and it does not even bring into play metrics, or any of the functionality afforded by `luz` callbacks.

In the next chapter, we discuss essential ingredients of modern deep learning we haven't yet touched upon; and following that, we look at specific architectures destined to specifically handle different tasks and domains.

### Appendix: A train-validate-test workflow implemented by hand

For clarity, we repeat here the two things that do *not* depend on whether you're using `luz` or not: `dataloader()` preparation and model definition.

```

# input dimensionality (number of input features)
d_in <- 3
# number of observations in training set
n <- 1000

x <- torch_randn(n, d_in)
coefs <- c(0.2, -1.3, -0.5)
y <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)

ds <- tensor_dataset(x, y)

dl <- dataloader(ds, batch_size = 100, shuffle = TRUE)

train_ids <- sample(1:length(ds), size = 0.6 * length(ds))
valid_ids <- sample(setdiff(
  1:length(ds),
  train_ids
), size = 0.2 * length(ds))
test_ids <- setdiff(1:length(ds), union(train_ids, valid_ids))

train_ds <- dataset_subset(ds, indices = train_ids)
valid_ds <- dataset_subset(ds, indices = valid_ids)
test_ds <- dataset_subset(ds, indices = test_ids)

train_dl <- dataloader(train_ds,
  batch_size = 100,
  shuffle = TRUE
)
valid_dl <- dataloader(valid_ds, batch_size = 100)
test_dl <- dataloader(test_ds, batch_size = 100)

# dimensionality of hidden layer
d_hidden <- 32
# output dimensionality (number of predicted features)
d_out <- 1

net <- nn_module(
  initialize = function(d_in, d_hidden, d_out) {
    self$net <- nn_sequential(
      nn_linear(d_in, d_hidden),
      nn_relu(),
      nn_linear(d_hidden, d_out)
    )
  },
  forward = function(x) {
    self$net(x)
  }
)
```

Recall that with `luz`, now all that separates you from watching how training and validation losses evolve is a snippet like this:

```
fitted <- net %>%
  setup(
    loss = nn_mse_loss(),
    optimizer = optim_adam
  ) %>%
  set_hparams(
    d_in = d_in,
    d_hidden = d_hidden, d_out = d_out
  ) %>%
  fit(train_dl, epochs = 200, valid_data = valid_dl)
```

Without `luz`, however, things to be taken care of fall into three distinct categories.

First, instantiate the network, and, if CUDA is installed, move its weights to the GPU.

```
device <- torch_device(if
(cuda_is_available()) {
  "cuda"
} else {
  "cpu"
})

model <- net(d_in = d_in, d_hidden = d_hidden, d_out = d_out)
model <- model$to(device = device)

```

Second, create an optimizer.

```
optimizer <- optim_adam(model$parameters)
```

And third, the biggest chunk: In each epoch, iterate over training batches as well as validation batches, performing backpropagation when working on the former, while just passively reporting losses when processing the latter.

For clarity, we pack training logic and validation logic each into their own functions. `train_batch()` and `valid_batch()` will be called from inside loops over the respective batches. Those loops, in turn, will be executed for every epoch.

While `train_batch()` and `valid_batch()`, per se, trigger the usual actions in the usual order, note the device placement calls: For the model to be able to take in the data, they have to live on the same device. Then, for mean-squared-error computation to be possible, the target tensors need to live there as well.

```

train_batch <- function(b) {
  optimizer$zero_grad()
  output <- model(b[[1]]$to(device = device))
  target <- b[[2]]$to(device = device)
  loss <- nnf_mse_loss(output, target)
  loss$backward()
  optimizer$step()
  loss$item()
}
valid_batch <- function(b) {
  output <- model(b[[1]]$to(device = device))
  target <- b[[2]]$to(device = device)
  loss <- nnf_mse_loss(output, target)
  loss$item()
}

valid_batch <- function(b) {
  output <- model(b[[1]]$to(device = device))
  target <- b[[2]]$to(device = device)

  loss <- nn_mse_loss(output, target)
  loss$item()
}

```

The loop over epochs contains two lines that deserve special attention: `model$train()` and `model$eval()`. The former instructs `torch` to put the model in training mode; the latter does the opposite. With the simple model we're using here, it wouldn't be a problem if you forgot those calls; however, when later we'll be using regularization layers like `nn_dropout()` and `nn_batch_norm2d()`, calling these methods in the correct places is essential. This is because these layers behave differently during evaluation and training.

```
num_epochs <- 200

for (epoch in 1:num_epochs) {
  model$train()
  train_loss <- c()

  # use coro::loop() for stability and performance
  coro::loop(for (b in train_dl) {
    loss <- train_batch(b)
    train_loss <- c(train_loss, loss)
  })

  cat(sprintf(
    "\nEpoch %d, training: loss: %3.5f \n",
    epoch, mean(train_loss)
  ))

  model$eval()
  valid_loss <- c()

  # disable gradient tracking to reduce memory usage
  with_no_grad({ 
    coro::loop(for (b in valid_dl) {
      loss <- valid_batch(b)
      valid_loss <- c(valid_loss, loss)
    })  
  })
  
  cat(sprintf(
    "\nEpoch %d, validation: loss: %3.5f \n",
    epoch, mean(valid_loss)
  ))
}
```

This completes our walk-through of manual training, and should have made more concrete my assertion that using `luz` significantly reduces the potential for casual (e.g., copy-paste) errors.




## 15. A first go at image classification {#sec:image-classification-1}

### What does it take to classify an image?

Think about how we, as human beings, can say "that's a cat", or: "this is a dog". No conscious processing is required. (Usually, that is.)

Why? The neuroscience, and cognitive psychology, involved are definitely out of scope for this book; but on a high level, we can assume that there are at least two prerequisites: First, that our visual system be able to build up complex representations out of lower-level ones, and second, that we have a set of concepts available we can map those high-level representations to. Presumably, then, an algorithm expected to do the same thing needs to be endowed with these same capabilities.

In the context of this chapter, dedicated to image classification, the second prerequisite is satisfied gratuitously. Classification being a variant of supervised machine learning, the concepts are given by means of the targets. The first, however, is all-important. We can again distinguish two components: the capability to detect low-level features, and that to successively compose them into higher-level ones.

Take a simple example. What would be required to identify a rectangle? A rectangle consists of edges: straight-ish borders of sort where something in the visual impression (color, for example) changes. To start with, then, the algorithm would have to be able to identify a single edge. That "edge extractor", as we might call it, is going to mark all four edges in the image. In this case, no further composition of features is needed; we can directly infer the concept.

On the other hand, assume the image were showing a house built of bricks. Then, there would be many rectangles, together forming a wall of the house; another rectangle, the door; and a few further ones, the windows. Maybe there'd be a different arrangement of edges, triangle-shaped, the roof. Meaning, an edge detector is not enough: We also need a "rectangle detector", a "triangle detector", a "wall detector", a "roof detector" ... and so on. Evidently, these detectors can't all be programmed up front. They'll have to be emergent properties of the algorithm: the neural network, in our case.

### Neural networks for feature detection\index{feature detection} and feature emergence

The way we've spelled out the requirements, a neural network for image classification needs to (1) be able to detect features, and (2) build up a hierarchy of such. Networks being networks, we can safely assume that (1) will be taken care of by a specialized layer (module), while (2) will be made possible by chaining several layers.

#### Detecting low-level features with cross-correlation\index{cross-correlation}

This chapter is about "convolutional" neural networks; the specialized module in question is the "convolutional" one. Why, then, am I talking about cross-correlation? It's because what neural-network people refer to as *convolution*\index{convolution}, technically is *cross-correlation*. (Don't worry -- I'll be making the distinction just here, in the conceptual introduction; afterwards I'll be saying "convolution", just like everyone else.)

So why am I insisting? It is for two reasons. First, this book actually *has* a chapter on convolution -- the "real one"; it figures in part three right between matrix operations and the Discrete Fourier Transform. Second, while in a formal sense the difference may be small, semantically as well as in terms of mathematical status, convolution and cross-correlation are decidedly distinct. In broad strokes:

Convolution may well be the most fundamental operation in all of signal processing, fundamental in the way addition and multiplication are. It can act as a *filter*, a signal-space transformation intended to achieve a desired result. For example, a moving average filter can be programmed as a convolution. So can, however, something quite the opposite: a filter that emphasizes differences. (An edge enhancer would be an example of the latter.)

Cross-correlation, in contrast, is more specialized. It *finds* things, or put differently: It spots similarities. This is what is needed in image recognition. To demonstrate how it works, we start in a single dimension.

##### Cross-correlation in one dimension

Assume we have a signal -- a univariate time series -- that looks like this: `0,1,1,1,-1,0,-1,-1,1,1,1,-1`. We want to find locations where a *one* occurs three times in a row. To that end, we make use of a filter that, too, has three ones in a row: `1,1,1`.

That filter, also called a *kernel*, is going to slide over the input sequence, producing an output value at every location. To be precise: The output value in question will be mapped to the *input value co-located with the kernel's central value*. How, then, can we obtain an output for the very first input value, which has no way of being mapped to the center of the kernel? In order for this to work, the input sequence is padded with zeroes: one in front, and one at the end. The new signal looks like this: `0,0,1,1,1,-1,0,-1,-1,1,1,1,-1,0` .

Now, we have the kernel sliding over the signal. Like so:

```         
0,0,1,1,1,-1,0,-1,-1,1,1,1,-1,0
1,1,1

0,0,1,1,1,-1,0,-1,-1,1,1,1,-1,0
   1,1,1

0,0,1,1,1,-1,0,-1,-1,1,1,1,-1,0
      1,1, 1
```

And so on.

At every position, products are computed between the mapped input and kernel values, and then, those products are added up, to yield the output value at the central position. For example, this is what gets computed at the very first matching: `0*1 + 0*1 + 1*1 = 1`. Appending the outputs, we get a new sequence: `1,2,3,1,0,-2,-2,-1,1,3,1,0` .

How does this help in finding three consecutive ones? Well, a three can only result when the kernel has found such a location. Thus, with that choice of kernel, we take every occurrence of `3` in the output as the center of the target sequence we're looking for.

##### Cross-correlation in two dimensions

This chapter is about images; how does that logic carry over to two dimensions?

Everything works just the same; it's just that now, the input signal extends over two dimensions, and the kernel is two-dimensional, as well. Again, the input is padded; with a kernel of size 3 x 3, for example, one row is added on top and bottom each, and one column, on the left and the right. Again, the kernel slides over the image, row by row and column by column. At each point it computes an aggregate that is the sum of point-wise products. Mathematically, that's a *dot product*.

To get a feel for how this works, we look at a bare-bones example: a white square on black background (@fig-images-square).

![White square on black background.](images/images-square.png){#fig-images-square fig-alt="A white square on a black background."}

Nicely, the open-source graphics program Gimp has a feature that allows one to experiment with custom filters ("Filters" -\> "Custom" -\> "Convolution matrix"). We can construct kernels and directly examine their effects.

Say we want to find the left edge of the square. We are looking for locations where the color changes, horizontally, from black to white. This can be achieved with a 3x3 kernel that looks like this (@fig-images-square-left):

```         
 0 0 0
-1 1 0
 0 0 0
```

This kernel is *similar* to the edge type we're interested in in that it has, in the second row, a horizontal transition from -1 to 1.

Analogously, kernels can be constructed that extract the right (@fig-images-square-right), top (@fig-images-square-top), and bottom (@fig-images-square-bottom) edges.

![Gimp convolution matrix that detects the left edge.](images/images-square-left.png){#fig-images-square-left fig-alt="A gimp convolution matrix of size 5x5. Individual values read: Row 1: all zero. Row 2: all zero. Row 3: 0, -1, 1, 0, 0. Row 4: all zero. Row 5: all zero."}

![Gimp convolution matrix that detects the right edge.](images/images-square-right.png){#fig-images-square-right fig-alt="A gimp convolution matrix of size 5x5. Individual values read: Row 1: all zero. Row 2: all zero. Row 3: 0, 1, -1, 0, 0. Row 4: all zero. Row 5: all zero."}

![Gimp convolution matrix that detects the top edge.](images/images-square-top.png){#fig-images-square-top fig-alt="A gimp convolution matrix of size 5x5. Individual values read: Row 1: all zero. Row 2: 0, 0, -1, 0, 0. Row 3: 0, 0, 1, 0, 0. Row 4: all zero. Row 5: all zero."}

![Gimp convolution matrix that detects the bottom edge.](images/images-square-bottom.png){#fig-images-square-bottom fig-alt="A gimp convolution matrix of size 5x5. Individual values read: Row 1: all zero. Row 2: all zero. Row 3: 0, 0, 1, 0, 0. Row 4: 0, 0, -1, 0, 0. Row 5: all zero."}

To understand this numerically, we can simulate a tiny image (@fig-images-cross-correlation, left). The numbers represent a grayscale image with values ranging from 0 to 255. To its right, we have the kernel; this is the one we used to detect the left edge. As a result of having that kernel slide over the image, we obtain the "image" on the right. `0` being the lowest possible value, negative pixels end up black, and we obtain a white edge on black background, just like we saw with Gimp.

![Input image, filter, and result as pixel values. Negative pixel values being impossible, -255 will end up as 0.](images/images-cross-correlation.png){#fig-images-cross-correlation fig-alt="Convolution by example. On the left, the white square on black background, with black pixels mapped to value 0 and white pixels, to 255. In the middle, the 3 x 3 kernel that detects a left edge, with the central row holding values -1, 1, 0, and both other rows being all zero. On the right, the result. Rows 1 and 2 as well as 7 and 8 are all zero; same with columns 1, 2, 4, 5, 6, and 8. Column 3 reads: 0, 0, 1, 1, 1, 1, 0, 0. Column 7 reads: 0, 0, -255, -255, -255, -255, 0, 0."}

Now, we've talked a lot about constructing kernels. Neural networks are all about *learning* feature detectors, not having them programmed up-front. Naturally, then, learning a filter means having a layer type whose weights embody this logic.

##### Convolutional layers in `torch`

So far, the only layer type we've seen that learns weights is `nn_linear()`. `nn_linear()` performs an affine operation: It takes an input tensor, matrix-multiplies it by its weight matrix $\mathbf{W}$, and adds the bias vector $\mathbf{b}$. While there is just a single bias per layer, independently of the number of neurons it has, this is not the case for the weights: There is a unique connection between each feature in the input tensor and each of the layer's neurons.

This is not true for `nn_conv2d()`, `torch`'s (two-dimensional) convolution[^image_classification_1-1] layer.

[^image_classification_1-1]: Like I said above, I'll be using the established term "convolution" from now on. Actually -- given that weights are *learned --* it does not matter that much anyway.

Back to how convolutional layers differ from linear ones. We've already seen what the layer's effect is supposed to be: A *kernel* should slide over its input, generating an output value at each location. Now the kernel, for a convolutional layer, is exactly its weight matrix. The kernel sliding over an input image means that weights are re-used every time it shifts its position. Thus, the number of weights is determined by the size of the kernel, not the size of the input. As a consequence, a convolutional layer is way more economical than a linear one.

Another way to express this is the following.

Conceptually, we are looking for the same thing, wherever it appears in the image. Take the most standard of standard image classification benchmarks, MNIST. It is about classifying images of the Arabic numerals 0-9. Say we want to learn the shape for a 2. The 2 could be right in the middle of the image, or it could be shifted to the left (say). An algorithm should be able to recognize it no matter where. Additional requirements depend on the task. If all we need to be able to do is say "that's a 2", we're good to use an algorithm that is *translation-invariant*\index{invariance (translational)}: It outputs the same thing independently of any shifts that may have occurred. For classification, that's just fine: A 2 is a 2 is a 2.

Another important task, though, is image segmentation (something we'll look at in an upcoming chapter). In segmentation, we want to mark all pixels in an image according to whether they are part of some object or not. Think tumor cells, for example. The 2 is still a 2, but we do need the information where in the image it is located. The algorithm to use now has to be *translation-equivariant*\index{equivariance (translational)}: If a shift has occurred, the target is still detected, but at a new location. And thinking about the convolution algorithm, translation-equivariant is exactly what it is.

So now, we have an idea how `torch` lets us detect individual features in an image. This gives us the first in our list of desiderates. The second is about combining feature detectors, that is, building up a hierarchy, in order to discern more and more specialized types of objects. This means that from a single layer, we move on to a network of layers.

#### Build up feature hierarchies\index{feature hierarchies}

A prototypical convolutional neural network for image classification will chain blocks composed of three types of layers: convolutional ones (`nn_conv1d()`, `nn_conv2d()`, or `nn_conv3d()`, depending on the dimension we're in), activation layers (e.g., `nn_relu()`), and pooling layers (e.g., `nn_maxpool1d()`, `nn_maxpool2d()`, `nn_maxpool3d()`).

The only type we haven't talked about yet are the pooling\index{pooling} layers. Just like activation layers, these don't have any parameters; what they do is aggregate neighboring tensor values. The size of the region to summarize is specified in the layer constructor's parameters. Various types of aggregation are available: `nn_maxpool<n>d()` picks the highest value, while `nn_avg_pool<n>d()` computes the average.

Why would one want to perform these kinds of aggregation? Practically speaking, one *has to* if one wants to arrive at a per-image (as opposed to per-pixel) output. But we can't just choose *any* way of aggregating spatially-arranged values. Picture, for example, an average where the interior pixels of an image patch were weighted higher than the exterior ones. Then, it would make a difference where in the patch some object was located. But for classification, this should not be the case. For classification, as opposed to segmentation, we want translation *invariance* -- not just *equivariance*, the property we just said convolution has. And translation-invariant is just what layers like `nn_maxpool2d()`, `nn_avgpool2d()`, etc. are.

##### A prototypical convnet

A template for a convolutional network, called "convnet" from now on, could thus look as below. To preempt any possible confusion: Even though, above, I was talking about three types of *layers*, there really is just one type in the code: the convolutional one. For brevity, both ReLU activation and max pooling are realized as functions instead.

Here is a possible template. It is not intended as a recommendation (as to number of filters, kernel size, or other hyperparameters, for example) -- just to illustrate the mechanics. More detailed comments follow.

```{r}
library(torch)

convnet <- nn_module(
  "convnet",
  
  initialize = function() {
    
    # nn_conv2d(in_channels, out_channels, kernel_size)
    self$conv1 <- nn_conv2d(1, 16, 3)
    self$conv2 <- nn_conv2d(16, 32, 3)
    self$conv3 <- nn_conv2d(32, 64, 3)
    
    self$output <- nn_linear(2304, 3)

  },
  
  forward = function(x) {
    
    x %>% 
      self$conv1() %>% 
      nnf_relu() %>%
      nnf_max_pool2d(2) %>%
      self$conv2() %>% 
      nnf_relu() %>%
      nnf_max_pool2d(2) %>%
      self$conv3() %>% 
      nnf_relu() %>%
      nnf_max_pool2d(2) %>%
      torch_flatten(start_dim = 2) %>%
      self$output()
      
  }
)

model <- convnet()
```

To understand what is going on, we need to know how images are represented in `torch`. By itself, an image is represented as a three-dimensional tensor, with one dimension indexing into available channels\index{channels (image)} (package luz)} (one for gray-scale images, three for RGB, possibly more for different kinds of imaging outputs), and the other two, corresponding to the two spatial axes, height (rows) and width (columns). In deep learning, we work with batches; thus, there is an additional dimension\index{batch dimension} -- the very first one -- that refers to batch number.

Let's look at an example image that may be used with the above template:

```{r}
img <- torch_randn(1, 1, 64, 64)

```

What we have here is an image, or more precisely, a batch containing a single image, that has a single channel, and is of size 64 x 64.

That said, the above template assumes the following:

1.  The input image has one channel. That's why the first argument to `nn_conv2d()` is `1` when we construct the first of the conv layers. (No assumptions are made, on the other hand, about the size of the input image.)
2.  We want to distinguish between three different target classes. This means that the output layer, a linear module, needs to have three output channels.

To test the code, we can call the un-trained model on our example image:

```{r}
model(img)
```

```         
torch_tensor
0.01 *
 6.4821  3.4166 -5.6050
[ CPUFloatType{1,3} ][ grad_fn = <AddmmBackward0> ]
```

One final note about that template. When you were reading the code above, one line that might have stood out is the following:

```         
self$output <- nn_linear(2304, 3)
```

How did that 2304, the number of input connections to `nn_linear()`, come about? It is the result of (1) a number of operations that each reduce spatial resolution, plus (2) a flattening operation that removes all dimensional information besides the batch dimension. This will make more sense once we've discussed the arguments to the layers in question. But one thing needs to be said upfront: *If this sounds like magic, there is a simple means to make the magic go away.* Namely, a simple way to find out about tensor shapes at any stage in a network is to comment all subsequent actions in `forward()`, and call the modified model. Naturally, this should not replace understanding, but it's a great way not to lose one's nerves when encountering shape errors.

Now, about layer arguments.

##### Arguments to `nn_conv2d()`

Above, we passed three arguments to `nn_conv2d()`: `in_channels`, `out_channels`, and `kernel_size`. This is not an exhaustive list of parameters, though. The remaining ones all have default values, but it is important to know about their existence. We're going to elaborate on three of them, all of whom you're likely to play with applying the template to some concrete task. All of them affect output size. So do two of the three mandatory arguments, `out_channels` and `kernel_size`:

-   `out_channels` refers to the number of kernels\index{kernel} (often called *filters*\index{filter}, in this context) learned. Its value affects the second of the four dimensions of the output tensor; it does not affect spatial resolution, though. Learning more filters adds capacity to the network, as it increases the number of weights.

-   `kernel_size`, on the other hand, *does* alter spatial resolution -- unless its value is 1, in which case the kernel never exceeds image boundaries. Like `out_channels`, it is a candidate for experimentation. In general, though, it is advisable to keep kernel size rather small, and chain a larger number of convolutional layers, instead of enlarging kernel size in a "shallow" network.

Now for the three non-mandatory arguments to explore.

1.  `padding`\index{padding} is something we've encountered before. Any kernel that extends over more than a single pixel will move outside the valid region when sliding over an image; the more, the bigger the kernel. General options are to (1) either pad the image (with zeroes, for example), or (2) compute the dot product only where possible. In the latter case, spatial resolution will decrease. That need not in itself be a problem; like so many things, it's a matter of experimentation. By default, `torch` does not pad images; however by passing a value greater than `0` for `padding`, you can ensure that spatial resolution is preserved, whatever the kernel size. Compare @fig-images-conv-arithmetic-padding, reproduced from a nice compilation by @2016arXiv160307285D, to see the effect of padding.

2.  `stride`\index{stride} refers to the way a kernel moves over the image. With a `stride` greater than `1`, it takes "leaps" of sorts -- see @fig-images-conv-arithmetic-strides. This results in fewer "snapshots" being taken. As a result, spatial resolution decreases.

3.  A setting of `dilation`\index{dilation} greater than `1,` too, results in fewer snapshots, but for a different reason. Now, it's not that the kernel moves faster. Instead, the pixels it is applied to are not adjacent anymore. They're spread out -- how much, depends on the argument's value. See @fig-images-conv-arithmetic-dilation.

![Convolution, and the effect of padding. Copyright @2016arXiv160307285D, reproduced under [MIT license](https://github.com/vdumoulin/conv_arithmetic/blob/master/LICENSE).](images/images-conv-arithmetic-padding.png){#fig-images-conv-arithmetic-padding fig-alt="Comparing convolution with and without padding. A 3 x 3 filter slides over a 4 x 4 image. Top row: The resulting image is of size 2 x 2. Kernel never transcends the image. Bottom row: Padding by two on all sides. On the image's edges, kernel is aligned such that it stands out by two pixels."}

![Convolution, and the effect of `strides`. Copyright @2016arXiv160307285D, reproduced under [MIT license](https://github.com/vdumoulin/conv_arithmetic/blob/master/LICENSE).](images/images-conv-arithmetic-strides.png){#fig-images-conv-arithmetic-strides fig-alt="Convolution done with a strides setting of 2 x 2. A 3 x 3 filter slides over a 5 x 5 image. At each step, it jumps over one pixel, both when sliding horizontally and when sliding vertically."}

![Convolution, and the effect of `dilation`. Copyright @2016arXiv160307285D, reproduced under [MIT license](https://github.com/vdumoulin/conv_arithmetic/blob/master/LICENSE).](images/images-conv-arithmetic-dilation.png){#fig-images-conv-arithmetic-dilation fig-alt="Convolution done with a dilation factor of 2. A 3 x 3 filter slides over a 7 x 7 image. The filter is mapped in a way that there are 1-pixel holes in the image that are not covered by the filter."}

For non-mandatory arguments `padding`, `stride`, and `dilation`, @tbl-images-convargs has a summary of defaults and effects.

| Argument   | Default | Action (if non-default)                                             |
|------------|------------|-----------------------------------------------|
| `padding`  | 0       | virtual rows/columns added around the image                         |
| `stride`   | 1       | kernel moves across image at bigger step size ("jumps" over pixels) |
| `dilation` | 1       | kernel is applied to spread-out image pixels ("holes" in kernel)    |

: Arguments to `nn_conv_2d()` you may want to experiment with -- default values and non-default actions. {#tbl-images-convargs}

##### Arguments to pooling layers

Pooling layers compute aggregates over neighboring pixels. The number of pixels of aggregate over in every dimension is specified in the layer constructor's first argument (alternatively, the corresponding function's second argument). Slightly misleadingly, that argument is called `kernel_size`, although there are no weights involved: For example, in the above template, we were unconditionally taking the maximum pixel value over regions of size 2 x 2.

In analogy to convolution layers, pooling layers also accept arguments `padding` and `stride`. However, they are seldom used.

##### Zooming out

We've talked a lot about layers and their arguments. Let's zoom out and think back about the general template, and what it is supposed to achieve.

We are chaining blocks that, each, perform a convolution, apply a non-linearity, and spatially aggregate the result. Each block's weights act as feature detectors, and every block but the first receives as input something that already is the result of applying one or more feature detectors. The magical thing that happens, and the reason behind the success of convnets, is that by chaining layers, a *hierarchy* of features is built. Early layers detect edges and textures, later ones, patterns of various complexity, and the final ones, objects and parts of objects (see @fig-images-feature-visualization, a beautiful visualization reproduced from @olah2017feature).

![Feature visualization on a subset of layers of GoogleNet. Figure from @olah2017feature, reproduced under [Creative Commons Attribution CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) without modification.](images/images-feature-visualization.png){#fig-images-feature-visualization fig-alt="What layers at different levels of the hierarchy respond to, by example. From left to right: Various species of edges, textures, patterns, parts, objects."}

We now know enough about coding convnets and how they work to explore a real example.

### Classification on Tiny Imagenet

Before we start coding, let me anchor your expectations. In this chapter, we design and train a basic convnet from scratch. As to data pre-processing, we do what is needed, not more. In the next two chapters, we'll learn about common techniques used to improve model training, in terms of quality as well as speed. Once we've covered those, we'll pick up right where this chapter ended, and apply a few of those techniques to the present task. Therefore, what we're doing here is build a baseline, to be used in comparison with more sophisticated approaches. This is "just" a beginning.

#### Data pre-processing

In addition to `torch` and `luz`, we load a third package from the `torch` ecosystem: `torchvision`. `torchvision` provides operations on images, as well as a set of pre-trained models and common benchmark datasets.

```{r}
library(torch)
library(torchvision)
library(luz)
```

The dataset we use is "Tiny Imagenet". Tiny Imagenet is a subset of [ImageNet](https://image-net.org/index.php), a gigantic collection of more than fourteen million images, initially made popular through the "ImageNet Large Scale Visual Recognition Challenge" that was run between 2012 and 2017. In the challenge, the most popular task was multi-class classification, with one thousand different classes to choose from.

One thousand classes is a lot; and with images typically being processed at a resolution of 256 x 256, training a model takes a lot of time, even on luxurious hardware. For that reason, a more manageable version was created as part of a popular Stanford class on deep learning for images, [Convolutional Neural Networks for Visual Recognition (CS231n)](http://cs231n.stanford.edu/). The condensed dataset has two hundred classes, with five hundred training images per class. Two hundred classes, that's still a lot! (Most introductory examples will do "cats vs. dogs", or some other binary problem.) Thus, it's not an easy task.

We start by downloading the data.

Linux:

```
wget http://cs231n.stanford.edu/tiny-imagenet-200.zip
unzip tiny-imagenet-200.zip
```

Windows:

```
tiny_imagenet_dataset(".", download = TRUE)
```

```{r}
set.seed(777)
torch_manual_seed(777)

train_ds <- tiny_imagenet_dataset(
  root=".",
  download = FALSE,
  transform = function(x) {
    x %>%
      transform_to_tensor() 
  }
)

valid_ds <- tiny_imagenet_dataset(
  root=".",
  split = "val",
  transform = function(x) {
    x %>%
      transform_to_tensor()
  }
)

```

Notice how `tiny_imagenet_dataset()` takes an argument called `transform`\index{\texttt{transform} (torchvision)}. This is used to specify operations to be performed as part of the input pipeline. Here, not much is happening: We just convert images to something we can work with, tensors. However, very soon we'll see this argument used to specify sequences of transformations such as resizing, cropping, rotation, and more.

What remains to be done is create the data loaders.

```{r}
train_dl <- dataloader(train_ds,
  batch_size = 128,
  shuffle = TRUE
)
valid_dl <- dataloader(valid_ds, batch_size = 128)

```

Images are RGB, and of size 64 x 64:

```{r}
batch <- train_dl %>%
  dataloader_make_iter() %>%
  dataloader_next()

dim(batch$x)
```

```         
[1] 128   3  64  64
```

Classes are integers between 1 to 200:

```{r}
batch$y
```

```         
torch_tensor
 172
  17
  76
  78
 111
  57
   8
 166
 146
 114
  41
  28
 138
  98
  57
  98
  25
 148
 166
 135
  31
 182
  48
 184
 160
 166
  40
 115
 161
  21
... [the output was truncated (use n=-1 to disable)]
[ CPULongType{128} ]
```

Now we define a convnet, and train it with `luz`.

#### Image classification from scratch

Here is a prototypical convnet, modeled after our template, but more powerful.

In addition to what we've seen already, the code illustrates a way of modularizing the code, arranging layers into three groups:

-   a (large) feature detector that, as a whole, is shift-equivariant;

-   a shift-invariant pooling layer (`nn_adaptive_avg_pool2d()`) that allows us to specify a desired output resolution; and

-   a feed-forward neural network that takes the computed features and uses them to produce final scores: two hundred values, corresponding to two hundred classes, for each item in the batch.

```{r}
convnet <- nn_module(
  "convnet",
  initialize = function() {
    self$features <- nn_sequential(
      nn_conv2d(3, 64, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_conv2d(64, 128, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_conv2d(128, 256, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_conv2d(256, 512, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_max_pool2d(kernel_size = 2),
      nn_conv2d(512, 1024, kernel_size = 3, padding = 1),
      nn_relu(),
      nn_adaptive_avg_pool2d(c(1, 1))
    )
    self$classifier <- nn_sequential(
      nn_linear(1024, 1024),
      nn_relu(),
      nn_linear(1024, 1024),
      nn_relu(),
      nn_linear(1024, 200)
    )
  },
  forward = function(x) {
    x <- self$features(x)$squeeze()
    x <- self$classifier(x)
    x
  }
)
```

Now, we train the network. The classifier outputs raw logits, not probabilities; this means we need to make use of `nn_cross_entropy_loss()`. We train for fifty epochs:

```{r}
fitted <- luz_load("fitted.rds")
```

```         
fitted <- convnet %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_adam,
    metrics = list(
      luz_metric_accuracy()
    )
  ) %>%
  fit(train_dl,
      epochs = 50,
      valid_data = valid_dl,
      verbose = TRUE
      )
```

After fifty epochs, this resulted in accuracy values of 0.92 and 0.22, on the training and test sets, respectively. This is quite a difference! On the training set, this model is near-perfect; on the test set, it only gets up to every fourth image correct.

```         
Epoch 1/50
Train metrics: Loss: 5.0822 - Acc: 0.0146                                     
Valid metrics: Loss: 4.8564 - Acc: 0.0269
Epoch 2/50
Train metrics: Loss: 4.5545 - Acc: 0.0571                                     
Valid metrics: Loss: 4.2592 - Acc: 0.0904
Epoch 3/50
Train metrics: Loss: 4.0727 - Acc: 0.1122                                     
Valid metrics: Loss: 3.9097 - Acc: 0.1381
...
...
Epoch 48/50
Train metrics: Loss: 0.3033 - Acc: 0.9064                                     
Valid metrics: Loss: 10.2999 - Acc: 0.2188
Epoch 49/50
Train metrics: Loss: 0.2932 - Acc: 0.9098                                     
Valid metrics: Loss: 10.7348 - Acc: 0.222
Epoch 50/50
Train metrics: Loss: 0.2733 - Acc: 0.9152                                     
Valid metrics: Loss: 10.641 - Acc: 0.2204
```

With two hundred options to choose from, "every fourth" does not even seem so bad; however, looking at the enormous difference between both metrics, something is not quite right. The model has severely *overfitted* to the training set -- memorized the training samples, in other words. Overfitting is not specific to deep learning; it is the nemesis of all of machine learning. We'll consecrate the whole next chapter to this topic.

Before we end, though, let's see how we would use `luz` to obtain predictions:

```{r}
preds <- fitted %>% predict(valid_dl)
```

`predict()` directly returns what is output by the model: two hundred non-normalized scores for each item. That's because the model's last layer is a linear module, with no activation applied. (Remember how the loss function, `nn_cross_entropy_loss()`, applies a *softmax* operation before calculating cross-entropy.)

Now, we could certainly call `nnf_softmax()` ourselves, converting outputs from `predict()` to probabilities:

```{r}
preds <- nnf_softmax(preds, dim = 2)
```

However, if we're just interested in determining the most likely class, we can as well skip the normalization step, and directly pick the highest value for each batch item:

```{r}
torch_argmax(preds, dim = 2)
```

```         
torch_tensor
 144
   5
  22
  84
 190
 103
 186
  12
  39
  43
  61
 108
 140
  21
  43
  44
  24
  81
  79
 158
 169
 171
  50
 130
  67
  46
 103
  55
 180
  35
... [the output was truncated (use n=-1 to disable)]
[ CUDALongType{10000} ]
```

We could now go on to compare predictions with actual classes, looking for inspiration on what could be done better. But at this stage, there is still a *lot* that can be done better! We will return to this application in due time, but first, we need to learn about overfitting, and ways to speed up model training.


# Appendix

## Chapters left to the reader

* 19. Image segmentation
* 20. Tabular data
* 21. Time series
* 22. Audio classification
* 23. Overview
* 24. Matrix computations: Least-squares problems
* 25. Matrix computations: Convolution
* 26. Exploring the Discrete Fourier Transform (DFT)
* 27. The Fast Fourier Transform (FFT)
* 28. Wavelets

## Thank you

* Nathan Stephens
* nstephens@nvidia.com
* [torch-workshop](https://github.com/nwstephens/torch-workshop.git)